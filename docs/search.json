[
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Pukar Bhandari",
    "section": "",
    "text": "My research focuses on transportation planning, data science, and geospatial analysis. I apply statistical methods and data visualization techniques to understand transportation systems, mobility patterns, and urban planning challenges."
  },
  {
    "objectID": "research/index.html#published-articles",
    "href": "research/index.html#published-articles",
    "title": "Pukar Bhandari",
    "section": "Published Articles",
    "text": "Published Articles\n\n\n\n    \n        \n            \n                Faria Afrin Zinia, Pukar Bhandari, Justice Prosper Tuffour, and Andy Hong, \"Evaluating Social Equity of Transit Accessibility: A Case of Salt Lake County, U.S.,\" Transportation Research Record, Vol. 2677, No. 12 (2023), pp. 806-814, doi: 10.1177/03611981231170005\n            \n\n            \n            \n                \n                    \n                            Transportation\n                        \n                    \n                    \n                            Accessibility\n                        \n                    \n                    \n                            Equity\n                        \n                    \n                    \n                            Data Analysis\n                        \n                    \n                    \n                            R\n                        \n                    \n                    \n                            Geospatial\n                        \n                    \n            \n            \n\n            \n            \n                This study examines social equity dimensions of accessibility to light rail transit stations in Salt Lake County using the Two-Step Floating Catchment Area method and spatial regression models, finding generally equitable access with higher accessibility for households without vehicles.\n            \n            \n\n            \n                \n                    \n                        \n                             Full details Â»\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Pre-print\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code & Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Pukar Bhandari",
    "section": "Working Papers",
    "text": "Working Papers\n\n\n\n    \n\n\nNo matching items\n\nComing soon."
  },
  {
    "objectID": "research/index.html#conference-presentations",
    "href": "research/index.html#conference-presentations",
    "title": "Pukar Bhandari",
    "section": "Conference Presentations",
    "text": "Conference Presentations\n\n\n\n    \n\n\nNo matching items\n\nComing soon."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Pukar Bhandari",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBase Year SE Data Development\n\n\nLower Savannah Council of Governments (LSCOG) 2050 Long Range Transportation Plan\n\n\n\nR\n\nPython\n\nGIS\n\nData Science\n\nTransportation\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\nPukar Bhandari\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an Interactive Population Pyramid Explorer\n\n\nExploring demographic patterns across US counties through interactive visualization using {R-Shiny} and {mapgl}\n\n\n\nR\n\nShiny\n\nGIS\n\nCensus Data\n\nDashboard\n\n\n\nA deep dive into creating a story-driven Shiny application that combines interactive mapping with demographic visualizations using Census data.\n\n\n\n\n\nJun 25, 2025\n\n\nPukar Bhandari\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Robocopy GUI Tool\n\n\n\n\n\n\nTools\n\nUtilities\n\nWindows\n\n\n\nA web-based interface for generating Windows robocopy commands with ease\n\n\n\n\n\nJun 15, 2025\n\n\nPukar Bhandari\n\n\n\n\n\n\n\n\n\n\n\n\nOverture Maps Data Download\n\n\nApplying DuckDB in R and Python\n\n\n\nR\n\nPython\n\nGIS\n\nData Science\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nPukar Bhandari\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a U.S. Census Data Explorer\n\n\nA Shiny Dashboard for ACS, Decennial Census, and LEHD Data\n\n\n\nR\n\nShiny\n\nGIS\n\nCensus Data\n\nDashboard\n\n\n\nLearn how to build an interactive Shiny dashboard for exploring U.S. Census data including ACS, Decennial Census, and LEHD datasets with integrated mapping capabilities.\n\n\n\n\n\nMay 11, 2024\n\n\nPukar Bhandari\n\n\n\n\n\n\n\n\n\n\n\n\n3D Population Density Mapping of Nepal\n\n\nUsing Rayshader for Advanced Geospatial Visualization\n\n\n\nR\n\nGIS\n\nData Science\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\nPukar Bhandari\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html",
    "href": "posts/socioeconomic-demo/index.html",
    "title": "Base Year SE Data Development",
    "section": "",
    "text": "This section establishes the computational environment for processing socioeconomic data inputs for the Lower Savannah Council of Governments regional travel demand model using both R and Python platforms.\n\n\nThe package installation process incorporates essential libraries for comprehensive geospatial data analysis.\nThe R environment includes {tidyverse} for data manipulation, {sf} for spatial data handling, {tidycensus} for Census Bureau data access, and {lehdr} for Longitudinal Employer-Household Dynamics data retrieval.\nThe Python environment focuses on core data science libraries including {pandas} for data manipulation, {geopandas} for spatial analysis, and {pygris} for Census data queries. These packages form the analytical backbone for processing demographic, employment, and geographic data required for travel demand modeling.\n\n R Python\n\n\n\n# # Install required packages\n# install.packages(c(\n#   \"tidyverse\",\n#   \"vroom\",\n#   \"sf\",\n#   \"tidycensus\",\n#   \"lehdr\",\n#   \"arcgislayers\",\n#   \"mapview\",\n#   \"RColorBrewer\",\n#   \"janitor\"\n# ), dependencies = TRUE)\n\n# Load packages\nlibrary(tidyverse)    # Data manipulation and visualization\nlibrary(vroom)        # Read rectangular data\nlibrary(sf)           # Spatial analysis\nlibrary(tidycensus)   # Accessing US Census Data\nlibrary(lehdr)        # Access LODES data\nlibrary(arcgislayers) # ArcGIS REST API access\nlibrary(mapview)      # Interactive mapping\nlibrary(RColorBrewer) # Color palettes for maps\nlibrary(janitor)      # Data cleaning and preparation\n\n\n\n\n# Install required packages if not available\n# pip install numpy pandas geopandas shapely folium requests pygris\n\n# Load processing libraries & modules\nimport os\nfrom pathlib import Path\nimport zipfile\nimport requests\nimport urllib.parse\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load data and visualization libraries & modules\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport folium\nfrom shapely.geometry import Point\n\n# Census data query libraries & modules\nfrom pygris import blocks, block_groups\nfrom pygris.helpers import validate_state, validate_county\nfrom pygris.data import get_census, get_lodes\n\n\n\n\n\n\n\nConfiguration settings optimize performance and establish spatial consistency. The {tigris} cache prevents redundant TIGER/Line shapefile downloads. The South Carolina State Plane coordinate system (EPSG:3361) serves as the standard projection for accurate GIS operations\n\n R Python\n\n\n\n# Set options\noptions(tigris_use_cache = TRUE) # cache tiger/line shapefile for future use\n\n# set project CRS\nproject_crs &lt;- \"EPSG:3361\"\n\n\n\n\n# Set project CRS\nproject_crs = \"EPSG:3361\"\n\n\n\n\n\n\n\nAPI authentication enables access to detailed demographic and economic datasets from the Census Bureau. The key configuration supports both R and Python environments for automated data retrieval workflows.\n\nðŸ’¡ Need a Census API key? Get one for free at census.gov/developers\n\n\n R Python\n\n\n\n# Set your API key into environment\ntidycensus::census_api_key(\"your_api_key_here\", install = TRUE)\n\n\n\n\n# Set your API key into environment\nos.environ['CENSUS_API_KEY'] = 'your_api_key_here'\n\n\n\n\n\n\n\nThe centralized directory structure organizes input data, processing files, and model outputs. The standardized root folder path ensures consistent file management across computing environments and team members.\n\n R Python\n\n\n\n# Set your main data folder\nroot &lt;- \"M:/MA_Project/SC_LSCOG LRTP\"\n\n\n\n\n# Set your main data folder\nroot = \"M:/MA_Project/SC_LSCOG LRTP\""
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#install-and-load-packages",
    "href": "posts/socioeconomic-demo/index.html#install-and-load-packages",
    "title": "Base Year SE Data Development",
    "section": "",
    "text": "The package installation process incorporates essential libraries for comprehensive geospatial data analysis.\nThe R environment includes {tidyverse} for data manipulation, {sf} for spatial data handling, {tidycensus} for Census Bureau data access, and {lehdr} for Longitudinal Employer-Household Dynamics data retrieval.\nThe Python environment focuses on core data science libraries including {pandas} for data manipulation, {geopandas} for spatial analysis, and {pygris} for Census data queries. These packages form the analytical backbone for processing demographic, employment, and geographic data required for travel demand modeling.\n\n R Python\n\n\n\n# # Install required packages\n# install.packages(c(\n#   \"tidyverse\",\n#   \"vroom\",\n#   \"sf\",\n#   \"tidycensus\",\n#   \"lehdr\",\n#   \"arcgislayers\",\n#   \"mapview\",\n#   \"RColorBrewer\",\n#   \"janitor\"\n# ), dependencies = TRUE)\n\n# Load packages\nlibrary(tidyverse)    # Data manipulation and visualization\nlibrary(vroom)        # Read rectangular data\nlibrary(sf)           # Spatial analysis\nlibrary(tidycensus)   # Accessing US Census Data\nlibrary(lehdr)        # Access LODES data\nlibrary(arcgislayers) # ArcGIS REST API access\nlibrary(mapview)      # Interactive mapping\nlibrary(RColorBrewer) # Color palettes for maps\nlibrary(janitor)      # Data cleaning and preparation\n\n\n\n\n# Install required packages if not available\n# pip install numpy pandas geopandas shapely folium requests pygris\n\n# Load processing libraries & modules\nimport os\nfrom pathlib import Path\nimport zipfile\nimport requests\nimport urllib.parse\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load data and visualization libraries & modules\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport folium\nfrom shapely.geometry import Point\n\n# Census data query libraries & modules\nfrom pygris import blocks, block_groups\nfrom pygris.helpers import validate_state, validate_county\nfrom pygris.data import get_census, get_lodes"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#set-global-options-and-parameters",
    "href": "posts/socioeconomic-demo/index.html#set-global-options-and-parameters",
    "title": "Base Year SE Data Development",
    "section": "",
    "text": "Configuration settings optimize performance and establish spatial consistency. The {tigris} cache prevents redundant TIGER/Line shapefile downloads. The South Carolina State Plane coordinate system (EPSG:3361) serves as the standard projection for accurate GIS operations\n\n R Python\n\n\n\n# Set options\noptions(tigris_use_cache = TRUE) # cache tiger/line shapefile for future use\n\n# set project CRS\nproject_crs &lt;- \"EPSG:3361\"\n\n\n\n\n# Set project CRS\nproject_crs = \"EPSG:3361\""
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#set-census-api-key",
    "href": "posts/socioeconomic-demo/index.html#set-census-api-key",
    "title": "Base Year SE Data Development",
    "section": "",
    "text": "API authentication enables access to detailed demographic and economic datasets from the Census Bureau. The key configuration supports both R and Python environments for automated data retrieval workflows.\n\nðŸ’¡ Need a Census API key? Get one for free at census.gov/developers\n\n\n R Python\n\n\n\n# Set your API key into environment\ntidycensus::census_api_key(\"your_api_key_here\", install = TRUE)\n\n\n\n\n# Set your API key into environment\nos.environ['CENSUS_API_KEY'] = 'your_api_key_here'"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#project-folder",
    "href": "posts/socioeconomic-demo/index.html#project-folder",
    "title": "Base Year SE Data Development",
    "section": "",
    "text": "The centralized directory structure organizes input data, processing files, and model outputs. The standardized root folder path ensures consistent file management across computing environments and team members.\n\n R Python\n\n\n\n# Set your main data folder\nroot &lt;- \"M:/MA_Project/SC_LSCOG LRTP\"\n\n\n\n\n# Set your main data folder\nroot = \"M:/MA_Project/SC_LSCOG LRTP\""
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#define-state-and-counties",
    "href": "posts/socioeconomic-demo/index.html#define-state-and-counties",
    "title": "Base Year SE Data Development",
    "section": "2.1 Define state and counties",
    "text": "2.1 Define state and counties\nThe study area encompasses six counties within South Carolina: Aiken, Allendale, Bamberg, Barnwell, Calhoun, and Orangeburg. These counties constitute the LSCOG planning region for travel demand modeling purposes.\n\n R Python\n\n\n\n# Define state abbreviation and county names\nstate_abb &lt;- \"SC\"\ncounty_names &lt;- c(\n  \"Aiken\",\n  \"Allendale\",\n  \"Bamberg\",\n  \"Barnwell\",\n  \"Calhoun\",\n  \"Orangeburg\"\n)\n\n\n\n\n# Define state abbreviation and county names\nstate_abb = \"SC\"\ncounty_names = [\n    \"Aiken\",\n    \"Allendale\",\n    \"Bamberg\",\n    \"Barnwell\",\n    \"Calhoun\",\n    \"Orangeburg\"\n]\n\n\n\n\nFIPS code conversion translates state abbreviations and county names into standardized Federal Information Processing Standard codes. These codes enable consistent data retrieval across census datasets and ensure proper geographic matching with demographic and economic data sources.\n\n R Python\n\n\n\n# Converting state abbreviation code to FIPS code\nstate_fips &lt;- tidycensus:::validate_state(state = state_abb)\ncounty_fips &lt;- vapply(\n  county_names,\n  function(x) tidycensus:::validate_county(state = state_abb, county = x),\n  character(1)\n)\n\n# Converting County Names to FIPS code\nfips_codes &lt;- paste(state_fips, county_fips, sep = \"\")\nfips_codes\n\n[1] \"45003\" \"45005\" \"45009\" \"45011\" \"45017\" \"45075\"\n\n\n\n\n\n# Converting state abbreviation code to FIPS code\nstate_fips = validate_state(state_abb)\n\nUsing FIPS code '45' for input 'SC'\n\n# Converting County Names to FIPS code\ncounty_fips = [\n    validate_county(state_fips, county)\n    for county in county_names\n]\n\nUsing FIPS code '003' for input 'Aiken'\nUsing FIPS code '005' for input 'Allendale'\nUsing FIPS code '009' for input 'Bamberg'\nUsing FIPS code '011' for input 'Barnwell'\nUsing FIPS code '017' for input 'Calhoun'\nUsing FIPS code '075' for input 'Orangeburg'\n\n# Converting County Names to FIPS code\nfips_codes = [f\"{state_fips}{county}\" for county in county_fips]\nfips_codes\n\n['45003', '45005', '45009', '45011', '45017', '45075']"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#load-taz-geometry",
    "href": "posts/socioeconomic-demo/index.html#load-taz-geometry",
    "title": "Base Year SE Data Development",
    "section": "2.2 Load TAZ geometry",
    "text": "2.2 Load TAZ geometry\nThe TAZ shapefile provides the fundamental spatial framework for travel demand modeling. The geometry is loaded from the TDM exports geodatabase and filtered to include only zones within the six-county study area using FIPS code matching.\nCoordinate transformation converts the TAZ geometry to the projectâ€™s standard coordinate reference system (EPSG:3361) for accurate spatial calculations. The attribute selection retains essential fields including TAZ identifiers, area measurements, area type classifications, and county assignments.\n\n R Python\n\n\n\n# Load TAZ Shapefile\nlscog_taz &lt;- sf::read_sf(\n  \"data/SE_2019_AD_10_30_2023.gpkg\",\n  query = paste0(\n    \"SELECT * FROM \\\"SE_2019_AD_10_30_2023\\\" WHERE countyID IN (\",\n    paste0(\"'\", fips_codes, \"'\", collapse = \", \"),\n    \")\"\n  )\n) |&gt;\n  sf::st_transform(project_crs) |&gt;\n  dplyr::select(\n    ID,\n    Area,\n    Acres,\n    TAZ_ID = TAZ_IDs,\n    AREA_TYPE,\n    COUNTY,\n    COUNTYID = countyID\n  )\n\nlscog_taz\n\nSimple feature collection with 585 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1691490 ymin: 331894 xmax: 2237471 ymax: 744679.9\nProjected CRS: NAD83(HARN) / South Carolina (ft)\n# A tibble: 585 Ã— 8\n   ID     Area  Acres TAZ_ID AREA_TYPE COUNTY COUNTYID                      geom\n   &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;        &lt;MULTIPOLYGON [foot]&gt;\n 1 9050â€¦ 13.3   8518. 90501â€¦ RURAL     Bambeâ€¦ 45009    (((2046194 478862.1, 204â€¦\n 2 9050â€¦ 39.2  25084. 90501â€¦ RURAL     Bambeâ€¦ 45009    (((2012593 500179.5, 201â€¦\n 3 7505â€¦  9.03  5778. 75050â€¦ RURAL     Orangâ€¦ 45075    (((2056266 515976, 20561â€¦\n 4 7505â€¦ 15.5   9934. 75050â€¦ RURAL     Orangâ€¦ 45075    (((2061827 488917.9, 206â€¦\n 5 7505â€¦ 17.5  11216. 75050â€¦ SUBURBAN  Orangâ€¦ 45075    (((2154222 534929.2, 215â€¦\n 6 7505â€¦ 12.8   8191. 75050â€¦ RURAL     Orangâ€¦ 45075    (((2195053 518745.9, 219â€¦\n 7 7505â€¦  8.10  5181. 75050â€¦ SUBURBAN  Orangâ€¦ 45075    (((2179620 542034.9, 217â€¦\n 8 7505â€¦ 13.4   8568. 75050â€¦ SUBURBAN  Orangâ€¦ 45075    (((2179131 542424, 21770â€¦\n 9 7505â€¦  6.00  3843. 75050â€¦ SUBURBAN  Orangâ€¦ 45075    (((2184440 568297.6, 218â€¦\n10 7505â€¦  5.06  3239. 75050â€¦ SUBURBAN  Orangâ€¦ 45075    (((2204464 570787.3, 220â€¦\n# â„¹ 575 more rows\n\n\n\n\n\n# Load TAZ Shapefile\nlscog_taz = gpd.read_file(\n    \"data/SE_2019_AD_10_30_2023.gpkg\",\n    layer=\"SE_2019_AD_10_30_2023\",\n    where=f\"countyID IN ({', '.join([f\"'{fips}'\" for fips in fips_codes])})\"\n)\n\nlscog_taz = lscog_taz.to_crs(project_crs)\n\nlscog_taz = lscog_taz.rename(\n  columns={\n    'TAZ_IDs': 'TAZ_ID',\n    'countyID': 'COUNTYID'\n})[['ID', 'Area', 'Acres', 'TAZ_ID', 'AREA_TYPE', 'COUNTY', 'COUNTYID', 'geometry']]\n\nlscog_taz\n\n           ID  ...                                           geometry\n0     9050130  ...  MULTIPOLYGON (((2046194.08 478862.054, 2046134...\n1     9050132  ...  MULTIPOLYGON (((2012593.472 500179.47, 2013190...\n2    75050131  ...  MULTIPOLYGON (((2056266.071 515975.986, 205617...\n3    75050045  ...  MULTIPOLYGON (((2061826.693 488917.873, 206173...\n4    75050182  ...  MULTIPOLYGON (((2154221.857 534929.221, 215441...\n..        ...  ...                                                ...\n580   5050049  ...  MULTIPOLYGON (((1924248.136 433664.04, 1924004...\n581   5050055  ...  MULTIPOLYGON (((1905461.23 427627.626, 1905456...\n582   5050066  ...  MULTIPOLYGON (((1880812.362 414251.546, 188090...\n583   5050054  ...  MULTIPOLYGON (((1871871.454 413403.458, 187167...\n584   5050065  ...  MULTIPOLYGON (((1849074.015 398566.33, 1849218...\n\n[585 rows x 8 columns]\n\n\n\n\n\nThe interactive map visualization displays the TAZ structure colored by county, providing spatial context for the analysis area and enabling quality assurance of the geometric data loading process.\n\n R Python\n\n\n\n# Create interactive map\nmapview::mapview(lscog_taz, zcol = \"COUNTY\", lwd = 1.6, map.types = \"CartoDB.Voyager\", col.regions = RColorBrewer::brewer.pal(6, \"Dark2\"))\n\n\n\n\n\n\n\n\n# Create interactive map\nlscog_taz.explore(column=\"COUNTY\", categorical=True, legend=True, tiles=\"CartoDB.Voyager\", zoom_start=8)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#decennial-census",
    "href": "posts/socioeconomic-demo/index.html#decennial-census",
    "title": "Base Year SE Data Development",
    "section": "3.1 2020 Decennial census",
    "text": "3.1 2020 Decennial census\nThe 2020 Decennial Census provides population and housing data at the census block level, offering the finest spatial resolution for demographic analysis. Population variables include total population, group quarters population, and household population derived by subtraction. Housing variables encompass total dwelling units and household counts by size categories.\nHousehold size distributions are consolidated into four categories: 1-person, 2-person, 3-person, and 4-or-more-person households. The 4-or-more category aggregates larger household sizes to simplify model implementation while maintaining essential demographic stratification for trip generation analysis.\n\nFor more information on the Decennial Census data, refer to the Decennial Census Technical Documentation.\n\n\n R Python\n\n\n\n# Define variables to download\ndec_variables &lt;- c(\n    TOTPOP = \"P1_001N\", # Total Population\n    GQPOP = \"P18_001N\", # Population living in Group Quarters\n    DU = \"H1_001N\", # Dwelling Units\n    HH_1 = \"H9_002N\", # 1-person household\n    HH_2 = \"H9_003N\", # 2-person household\n    HH_3 = \"H9_004N\", # 3-person household\n    # HH_4 = \"H9_005N\", # 4-person household\n    # HH_5 = \"H9_006N\", # 5-person household\n    # HH_6 = \"H9_007N\", # 6-person household\n    # HH_7 = \"H9_008N\", # 7-or-more-person household\n    HH = \"H9_001N\" # Total Number of Households\n  )\n\n# Load Population and Household Data\nlscog_dec &lt;- tidycensus::get_decennial(\n  year = 2020,\n  sumfile = \"dhc\",\n  geography = \"block\",\n  state = state_fips,\n  county = county_fips,\n  output = \"wide\",\n  cb = FALSE,\n  geometry = TRUE,\n  keep_geo_vars = TRUE,\n  # key = Sys.getenv('CENSUS_API_KEY'),\n  variables = dec_variables\n) |&gt;\n  sf::st_transform(project_crs) |&gt;\n  dplyr::mutate(\n    HHPOP = TOTPOP - GQPOP,\n    HH_4 = HH - (HH_1 + HH_2 + HH_3)\n  ) |&gt;\n  dplyr::select(GEOID, TOTPOP, GQPOP, HHPOP, HH, HH_1, HH_2, HH_3, HH_4, DU)\n\nlscog_dec\n\nSimple feature collection with 13961 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1691517 ymin: 331891.7 xmax: 2237472 ymax: 744669.5\nProjected CRS: NAD83(HARN) / South Carolina (ft)\n# A tibble: 13,961 Ã— 11\n   GEOID           TOTPOP GQPOP HHPOP    HH  HH_1  HH_2  HH_3  HH_4    DU\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 450179504004051      0     0     0     0     0     0     0     0     0\n 2 450179504003011      0     0     0     0     0     0     0     0     0\n 3 450179502011045     54     4    50    16     3     1     3     9    18\n 4 450179504001020      0     0     0     0     0     0     0     0     0\n 5 450750105003029     13     0    13     4     0     3     0     1     4\n 6 450750117021009     10     0    10     3     3     0     0     0     8\n 7 450750117011051      0     0     0     0     0     0     0     0     0\n 8 450750118021087      6     0     6     4     0     1     2     1     4\n 9 450750120003020      0     0     0     0     0     0     0     0     0\n10 450179501003046     18     0    18     0     0     0     0     0     1\n# â„¹ 13,951 more rows\n# â„¹ 1 more variable: geometry &lt;MULTIPOLYGON [foot]&gt;\n\n\n\n\n\n# Define variables to download\ndec_variables = {\n    'P1_001N': 'TOTPOP',     # Total Population\n    'P18_001N': 'GQPOP',      # Population living in Group Quarters\n    'H1_001N': 'DU',          # Dwelling Units\n    'H9_002N': 'HH_1',        # 1-person household\n    'H9_003N': 'HH_2',        # 2-person household\n    'H9_004N': 'HH_3',        # 3-person household\n    # 'H9_005N': 'HH_4',        # 4-person household\n    # 'H9_006N': 'HH_5',        # 5-person household\n    # 'H9_007N': 'HH_6',        # 6-person household\n    # 'H9_008N': 'HH_7',        # 7-or-more-person household\n    'H9_001N': 'HH'           # Total Number of Households\n}\n\n# get census block geometries\nlscog_cb = blocks(\n    state=state_fips,\n    county=county_fips,\n    year=2020,\n    cache=True\n)\n\n# Download decennial census data at block level\nlscog_dec = get_census(\n    dataset=\"dec/dhc\",\n    year=2020,\n    variables=list(dec_variables.keys()),\n    params={\n        \"for\": f\"block:*\",\n        # \"key\": f\"{os.getenv('CENSUS_API_KEY')}\",\n        \"in\": f\"state:{state_fips} county:{','.join(county_fips)}\"\n    },\n    return_geoid=True,\n    guess_dtypes=True,\n)\n\n# join data to geometry\nlscog_dec = lscog_cb[['GEOID20', 'geometry']].merge(lscog_dec, left_on = \"GEOID20\", right_on = \"GEOID\")\n\n# Rename columns\nlscog_dec = lscog_dec.rename(columns=dec_variables)\n\n# Transform CRS\nlscog_dec = lscog_dec.to_crs(project_crs)\n\n# Calculate derived variables\nlscog_dec['HHPOP'] = lscog_dec['TOTPOP'] - lscog_dec['GQPOP']\nlscog_dec['HH_4'] = lscog_dec['HH'] - (\n    lscog_dec['HH_1'] + lscog_dec['HH_2'] + lscog_dec['HH_3']\n)\n\n# Select final columns\nlscog_dec = lscog_dec[['GEOID', 'TOTPOP', 'GQPOP', 'HHPOP',\n                      'HH', 'HH_1', 'HH_2', 'HH_3', 'HH_4', 'DU', 'geometry']]\n\nlscog_dec\n\n                 GEOID  ...                                           geometry\n0      450179504004051  ...  POLYGON ((2084300.974 622308.526, 2084460.74 6...\n1      450179504003011  ...  POLYGON ((2112518.54 626782.208, 2112608.778 6...\n2      450179502011045  ...  POLYGON ((2067775.167 662339.483, 2068091.491 ...\n3      450179504001020  ...  POLYGON ((2087947.053 684345.612, 2087992.284 ...\n4      450750105003029  ...  POLYGON ((2089072.743 552687.443, 2089248.831 ...\n...                ...  ...                                                ...\n13956  450059705003050  ...  POLYGON ((1926607.333 409005.466, 1926609.861 ...\n13957  450030203042000  ...  POLYGON ((1778751.197 641782.585, 1778797.329 ...\n13958  450030218002115  ...  POLYGON ((1919787.314 648288.439, 1919913.531 ...\n13959  450030206012008  ...  POLYGON ((1723756.559 630234.507, 1723784.384 ...\n13960  450059705002005  ...  POLYGON ((1926817.622 432583.896, 1930423.825 ...\n\n[13961 rows x 11 columns]"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#acs-estimates",
    "href": "posts/socioeconomic-demo/index.html#acs-estimates",
    "title": "Base Year SE Data Development",
    "section": "3.2 2020 ACS estimates",
    "text": "3.2 2020 ACS estimates\nThe American Community Survey 5-year estimates provide household income data at the block group level. Income categories are aggregated into three broad ranges: under $15,000, $15,000-$49,999, and $50,000 and above. This stratification aligns with travel behavior research indicating distinct mobility patterns across income levels.\nThe block group geography represents the finest spatial resolution available for ACS income data, providing sufficient detail for socioeconomic modeling while maintaining statistical reliability through the 5-year aggregation period.\n\nFor more information on the ACS data, refer to the ACS Technical Documentation.\n\n\n R Python\n\n\n\n# Define variables to download\nacs_variables &lt;- c(\n    INC_CAT_02 = \"B19001_002\", # Less than $10,000\n    INC_CAT_03 = \"B19001_003\", # $10,000 to $14,999\n    INC_CAT_04 = \"B19001_004\", # $15,000 to $19,999\n    INC_CAT_05 = \"B19001_005\", # $20,000 to $24,999\n    INC_CAT_06 = \"B19001_006\", # $25,000 to $29,999\n    INC_CAT_07 = \"B19001_007\", # $30,000 to $34,999\n    INC_CAT_08 = \"B19001_008\", # $35,000 to $39,999\n    INC_CAT_09 = \"B19001_009\", # $40,000 to $44,999\n    INC_CAT_10 = \"B19001_010\", # $45,000 to $49,999\n    # INC_CAT_11 = \"B19001_011\", # $50,000 to $59,999\n    # INC_CAT_12 = \"B19001_012\", # $60,000 to $74,999\n    # INC_CAT_13 = \"B19001_013\", # $75,000 to $99,999\n    # INC_CAT_14 = \"B19001_014\", # $100,000 to $124,999\n    # INC_CAT_15 = \"B19001_015\", # $125,000 to $149,999\n    # INC_CAT_16 = \"B19001_016\", # $150,000 to $199,999\n    # INC_CAT_17 = \"B19001_017\", # $200,000 or more\n    INC_CAT_01 = \"B19001_001\" # Total\n  )\n\n# Load Household Income Data\nlscog_acs &lt;- tidycensus::get_acs(\n  year = 2020,\n  survey = \"acs5\",\n  geography = \"block group\",\n  state = state_fips,\n  county = county_fips,\n  output = \"wide\",\n  cb = FALSE,\n  geometry = TRUE,\n  # key = Sys.getenv('CENSUS_API_KEY'),\n  variables = acs_variables\n) |&gt;\n  sf::st_transform(project_crs) |&gt;\n  dplyr::mutate(\n    INC_14999 = INC_CAT_02E + INC_CAT_03E,\n    INC_49999 = INC_CAT_04E +\n      INC_CAT_05E +\n      INC_CAT_06E +\n      INC_CAT_07E +\n      INC_CAT_08E +\n      INC_CAT_09E +\n      INC_CAT_10E,\n    INC_50000 = INC_CAT_01E - (INC_14999 + INC_49999)\n  ) |&gt;\n  dplyr::select(GEOID, INC_TOTAL = INC_CAT_01E, INC_14999, INC_49999, INC_50000)\n\nlscog_acs\n\nSimple feature collection with 262 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1691517 ymin: 331891.7 xmax: 2237472 ymax: 744669.5\nProjected CRS: NAD83(HARN) / South Carolina (ft)\nFirst 10 features:\n          GEOID INC_TOTAL INC_14999 INC_49999 INC_50000\n1  450030207011       467        82       171       214\n2  450030212052       949         0       256       693\n3  450030212051       857        21       250       586\n4  450030213001       612       103       129       380\n5  450030216031      1191       176       303       712\n6  450030215003       773        95       240       438\n7  450030205004      1081        56       302       723\n8  450030205003       937        37       158       742\n9  450030212041       758        37       344       377\n10 450030215004       973        77       247       649\n                         geometry\n1  MULTIPOLYGON (((1701402 614...\n2  MULTIPOLYGON (((1776367 591...\n3  MULTIPOLYGON (((1768340 598...\n4  MULTIPOLYGON (((1768476 630...\n5  MULTIPOLYGON (((1785900 618...\n6  MULTIPOLYGON (((1782074 620...\n7  MULTIPOLYGON (((1707972 630...\n8  MULTIPOLYGON (((1708123 634...\n9  MULTIPOLYGON (((1775528 610...\n10 MULTIPOLYGON (((1779272 619...\n\n\n\n\n\n# Define variables to download\nacs_variables = {\n    'B19001_002E': 'INC_CAT_02',  # Less than $10,000\n    'B19001_003E': 'INC_CAT_03',  # $10,000 to $14,999\n    'B19001_004E': 'INC_CAT_04',  # $15,000 to $19,999\n    'B19001_005E': 'INC_CAT_05',  # $20,000 to $24,999\n    'B19001_006E': 'INC_CAT_06',  # $25,000 to $29,999\n    'B19001_007E': 'INC_CAT_07',  # $30,000 to $34,999\n    'B19001_008E': 'INC_CAT_08',  # $35,000 to $39,999\n    'B19001_009E': 'INC_CAT_09',  # $40,000 to $44,999\n    'B19001_010E': 'INC_CAT_10',  # $45,000 to $49,999\n    # 'B19001_011E': 'INC_CAT_11',  # $50,000 to $59,999\n    # 'B19001_012E': 'INC_CAT_12',  # $60,000 to $74,999\n    # 'B19001_013E': 'INC_CAT_13',  # $75,000 to $99,999\n    # 'B19001_014E': 'INC_CAT_14',  # $100,000 to $124,999\n    # 'B19001_015E': 'INC_CAT_15',  # $125,000 to $149,999\n    # 'B19001_016E': 'INC_CAT_16',  # $150,000 to $199,999\n    # 'B19001_017E': 'INC_CAT_17',  # $200,000 or more\n    'B19001_001E': 'INC_CAT_01'   # Total\n}\n\n# get blockgroup geometries\nlscog_bg = block_groups(\n    state=state_fips,\n    county=county_fips,\n    year=2020,\n    cache=True\n)\n\n# Download household income data at block group level\nlscog_acs = get_census(\n    dataset=\"acs/acs5\",\n    year=2020,\n    variables=list(acs_variables.keys()),\n    params={\n        \"for\": f\"block group:*\",\n        # \"key\": f\"{os.getenv('CENSUS_API_KEY')}\",\n        \"in\": f\"state:{state_fips} county:{','.join(county_fips)}\"\n    },\n    return_geoid=True,\n    guess_dtypes=True\n)\n\n# join data to geometry\nlscog_acs = lscog_bg[['GEOID', 'geometry']].merge(lscog_acs, on = \"GEOID\")\n\n# Rename columns\nlscog_acs = lscog_acs.rename(columns=acs_variables)\n\n# Transform CRS\nlscog_acs = lscog_acs.to_crs(project_crs)\n\n# Calculate derived variables\nlscog_acs['INC_14999'] = lscog_acs['INC_CAT_02'] + lscog_acs['INC_CAT_03']\nlscog_acs['INC_49999'] = (\n    lscog_acs['INC_CAT_04'] +\n    lscog_acs['INC_CAT_05'] +\n    lscog_acs['INC_CAT_06'] +\n    lscog_acs['INC_CAT_07'] +\n    lscog_acs['INC_CAT_08'] +\n    lscog_acs['INC_CAT_09'] +\n    lscog_acs['INC_CAT_10']\n)\nlscog_acs['INC_50000'] = lscog_acs['INC_CAT_01'] - (\n    lscog_acs['INC_14999'] + lscog_acs['INC_49999']\n)\n\n# Select final columns\nlscog_acs = lscog_acs.rename(columns={'INC_CAT_01': 'INC_TOTAL'})\nlscog_acs = lscog_acs[['GEOID', 'INC_TOTAL', 'INC_14999', 'INC_49999', 'INC_50000', 'geometry'\n]]\n\nlscog_acs\n\n            GEOID  ...                                           geometry\n0    450030207011  ...  POLYGON ((1701402.37 614608.958, 1701441.07 61...\n1    450030212052  ...  POLYGON ((1776366.684 591083.944, 1776445.37 5...\n2    450030212051  ...  POLYGON ((1768340.248 598546.468, 1768482.141 ...\n3    450030213001  ...  POLYGON ((1768475.931 630588.733, 1768610.076 ...\n4    450030216031  ...  POLYGON ((1785900.107 618251.028, 1786047.488 ...\n..            ...  ...                                                ...\n257  450750119004  ...  POLYGON ((1974249.527 620700.361, 1975105.197 ...\n258  450750103011  ...  POLYGON ((2142864.631 581695.327, 2142874.033 ...\n259  450750109011  ...  POLYGON ((2033500.16 608479.774, 2033509.304 6...\n260  450750109022  ...  POLYGON ((2013478.807 622487.426, 2013488.515 ...\n261  450750109023  ...  POLYGON ((2022268.105 617836.647, 2022656.655 ...\n\n[262 rows x 6 columns]"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#lehd-data",
    "href": "posts/socioeconomic-demo/index.html#lehd-data",
    "title": "Base Year SE Data Development",
    "section": "3.3 2019 LEHD data",
    "text": "3.3 2019 LEHD data\nThe Longitudinal Employer-Household Dynamics Workplace Area Characteristics data provides employment counts by industry sector at the census block level. Employment categories follow the North American Industry Classification System and are aggregated into transportation-relevant sectors including retail, services, manufacturing, and public administration.\nThe 2019 reference year represents pre-pandemic employment patterns, providing a stable baseline for long-term transportation planning. Employment data at the block level enables precise spatial allocation of work destinations within the travel demand model framework.\n\nFor more information on the LEHD data, refer to the LODES Technical Documentation.\n\n\n R Python\n\n\n\n# Download LEHD WAC data at block level\nlscog_emp &lt;- lehdr::grab_lodes(\n  version = \"LODES8\",\n  state = tolower(state_abb),\n  lodes_type = \"wac\",\n  segment = \"S000\",\n  job_type = \"JT00\",\n  year = 2019,\n  state_part = \"\",\n  agg_geo = \"block\",\n  use_cache = TRUE\n) |&gt;\n  dplyr::filter(grepl(\n    paste(\"^(\", paste(fips_codes, collapse = \"|\"), \")\", sep = \"\"),\n    w_geocode\n  )) |&gt;\n  # check the documentation at: https://lehd.ces.census.gov/data/lodes/LODES8/LODESTechDoc8.0.pdf\n  dplyr::mutate(\n    GEOID = as.character(w_geocode),\n    TOTAL_EMP = C000, # Total Employment\n    AGR_FOR_FI = CNS01, # Agricultural, forestry, and fishing employment\n    MINING = CNS02, # Mining employment\n    CONSTRUCTI = CNS04, # Construction employment\n    MANUFACTUR = CNS05, # Manufacturing employment\n    TRANSP_COM = CNS08 + CNS09, # Transportation, communication employment\n    WHOLESALE = CNS06, # Wholesale employment\n    RETAIL = CNS07, # Retail employment\n    FIRE = CNS10 + CNS11, # Finance / Insurance / Real Estate employment\n    SERVICES = CNS03 +\n      CNS12 +\n      CNS13 +\n      CNS14 + # Service employment\n      CNS15 +\n      CNS16 +\n      CNS17 +\n      CNS18 +\n      CNS19,\n    PUBLIC_ADM = CNS20 # Public Administration employment\n  ) |&gt;\n  dplyr::select(\n    GEOID,\n    TOTAL_EMP,\n    AGR_FOR_FI,\n    MINING,\n    CONSTRUCTI,\n    MANUFACTUR,\n    TRANSP_COM,\n    WHOLESALE,\n    RETAIL,\n    FIRE,\n    SERVICES,\n    PUBLIC_ADM\n  )\n\nlscog_emp\n\n# A tibble: 2,450 Ã— 12\n   GEOID  TOTAL_EMP AGR_FOR_FI MINING CONSTRUCTI MANUFACTUR TRANSP_COM WHOLESALE\n   &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 45003â€¦         6          6      0          0          0          0         0\n 2 45003â€¦         4          0      0          0          0          4         0\n 3 45003â€¦        12          0      0         12          0          0         0\n 4 45003â€¦         1          0      0          0          0          0         0\n 5 45003â€¦        11         11      0          0          0          0         0\n 6 45003â€¦         1          0      0          0          0          0         0\n 7 45003â€¦         9          0      0          0          0          0         0\n 8 45003â€¦        18          0      0         18          0          0         0\n 9 45003â€¦         1          0      0          1          0          0         0\n10 45003â€¦         4          4      0          0          0          0         0\n# â„¹ 2,440 more rows\n# â„¹ 4 more variables: RETAIL &lt;dbl&gt;, FIRE &lt;dbl&gt;, SERVICES &lt;dbl&gt;,\n#   PUBLIC_ADM &lt;dbl&gt;\n\n\n\n\n\n# Download LEHD WAC data at block level\nlscog_emp = get_lodes(\n    state=state_abb,\n    year=2019,\n    version=\"LODES8\",\n    lodes_type=\"wac\",\n    part=\"main\",\n    segment=\"S000\",\n    job_type=\"JT00\",\n    agg_level=\"block\",\n    cache=True,\n    return_geometry=False\n)\n\n# Filter for specific FIPS codes\nlscog_emp = lscog_emp[lscog_emp['w_geocode'].str.match(f\"^({'|'.join(fips_codes)})\")]\n\n# Create new columns with employment categories\n# Check documentation at: https://lehd.ces.census.gov/data/lodes/LODES8/LODESTechDoc8.0.pdf\nlscog_emp = lscog_emp.assign(\n    GEOID=lscog_emp['w_geocode'].astype(str),\n    TOTAL_EMP=lscog_emp['C000'],  # Total Employment\n    AGR_FOR_FI=lscog_emp['CNS01'],  # Agricultural, forestry, and fishing employment\n    MINING=lscog_emp['CNS02'],  # Mining employment\n    CONSTRUCTI=lscog_emp['CNS04'],  # Construction employment\n    MANUFACTUR=lscog_emp['CNS05'],  # Manufacturing employment\n    TRANSP_COM=lscog_emp['CNS08'] + lscog_emp['CNS09'],  # Transportation, communication employment\n    WHOLESALE=lscog_emp['CNS06'],  # Wholesale employment\n    RETAIL=lscog_emp['CNS07'],  # Retail employment\n    FIRE=lscog_emp['CNS10'] + lscog_emp['CNS11'],  # Finance / Insurance / Real Estate employment\n    SERVICES=(lscog_emp['CNS03'] +\n              lscog_emp['CNS12'] +\n              lscog_emp['CNS13'] +\n              lscog_emp['CNS14'] +\n              lscog_emp['CNS15'] +\n              lscog_emp['CNS16'] +\n              lscog_emp['CNS17'] +\n              lscog_emp['CNS18'] +\n              lscog_emp['CNS19']),  # Service employment\n    PUBLIC_ADM=lscog_emp['CNS20']  # Public Administration employment\n)\n\n# Select only the desired columns\nlscog_emp = lscog_emp[['GEOID', 'TOTAL_EMP', 'AGR_FOR_FI', 'MINING', 'CONSTRUCTI',\n                       'MANUFACTUR', 'TRANSP_COM', 'WHOLESALE', 'RETAIL', 'FIRE',\n                       'SERVICES', 'PUBLIC_ADM']]\n\n# Display structure/info about the dataframe\nlscog_emp\n\n                 GEOID  TOTAL_EMP  AGR_FOR_FI  ...  FIRE  SERVICES  PUBLIC_ADM\n191    450030201001001          6           6  ...     0         0           0\n192    450030201001017          4           0  ...     0         0           0\n193    450030201001037         12           0  ...     0         0           0\n194    450030201001049          1           0  ...     0         0           0\n195    450030201002006         11          11  ...     0         0           0\n...                ...        ...         ...  ...   ...       ...         ...\n28115  450750120002099         25           0  ...     0         0           0\n28116  450750120002108          8           0  ...     0         0           0\n28117  450750120002118          3           0  ...     0         0           0\n28118  450750120004038          3           0  ...     0         0           0\n28119  450750120004071          6           0  ...     0         5           0\n\n[2450 rows x 12 columns]"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#nces-school-and-college-enrollment-data",
    "href": "posts/socioeconomic-demo/index.html#nces-school-and-college-enrollment-data",
    "title": "Base Year SE Data Development",
    "section": "3.4 2020 NCES school and college enrollment data",
    "text": "3.4 2020 NCES school and college enrollment data\nThe National Center for Education Statistics provides comprehensive educational institution data including enrollment and staffing information for transportation planning analysis.\n\nPublic schools\nPublic school data is made available by National Center for Education Statistics through their Common Core of Data (CCD) program. For ease of processing, we retrieve the CCD from the ArcGIS REST service for the 2019-2020 academic year. The dataset includes total student enrollment and full-time equivalent teacher counts for each institution within the six-county region. Public schools represent major trip generation sources for both student and employee travel, requiring precise spatial location data for accurate modeling.\n\nFor more information on the NCES Public School data, refer to CCD Online Documentation.\n\nTo retrieve the data from ArcGIS REST service, we use the {arcgislayers} package in R and create a custom function in Python to read the ArcGIS FeatureLayer or Table. This has been implemented to function similarly to the {arcgislayers} package in R, allowing us to query the service with a SQL WHERE clause and return the data as a GeoDataFrame.\n\n R Python\n\n\n\n# Public School Location data 2019-2020\nlscog_pub_sch_enroll &lt;- arcgislayers::arc_read(\n  url = \"https://nces.ed.gov/opengis/rest/services/K12_School_Locations/EDGE_ADMINDATA_PUBLICSCH_1920/MapServer/0\",\n  where = paste0(\n    \"LSTATE = '\",\n    state_abb,\n    \"' AND NMCNTY IN (\",\n    paste0(\"'\", paste0(county_names, \" County\"), \"'\", collapse = \", \"),\n    \")\"\n  ),\n  alias = \"label\",\n  crs = project_crs\n) |&gt;\n  dplyr::select(\n    INSTITUTION_ID = NCESSCH,\n    NAME = SCH_NAME,\n    STATE = LSTATE,\n    STUDENT_COUNT_PUB = TOTAL,\n    TEACHER_COUNT_PUB = FTE\n  )\n\nlscog_pub_sch_enroll\n\nSimple feature collection with 98 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1700952 ymin: 406810.6 xmax: 2203406 ymax: 724699.4\nProjected CRS: NAD83(HARN) / South Carolina (ft)\nFirst 10 features:\n   INSTITUTION_ID                          NAME STATE STUDENT_COUNT_PUB\n1    450108001163           Barnwell Elementary    SC               462\n2    450075000064        Allendale Fairfax High    SC               283\n3    450075001184          Allendale Elementary    SC               245\n4    450075001415      Allendale-Fairfax Middle    SC               268\n5    450093000119         Bamberg-Ehrhardt High    SC               381\n6    450093000120       Bamberg-Ehrhardt Middle    SC               188\n7    450096000122             Denmark Olar High    SC               162\n8    450096000123           Denmark-Olar Middle    SC               149\n9    450096001426       Denmark-Olar Elementary    SC               353\n10   450098000127 Barnwell County Career Center    SC                 0\n   TEACHER_COUNT_PUB                 geometry\n1               32.0 POINT (1891531 517378.5)\n2               28.9 POINT (1913416 420526.6)\n3               18.0 POINT (1916344 418212.2)\n4               18.0 POINT (1913416 420526.6)\n5               28.5 POINT (1991502 535259.1)\n6               15.0 POINT (1990622 534442.6)\n7               20.0   POINT (1962410 543779)\n8               10.0 POINT (1956236 534420.3)\n9               25.0 POINT (1960237 537023.1)\n10              12.0   POINT (1894891 540093)\n\n\n\n\n\n\nCode\n# Create function to read ArcGIS FeatureLayer or Table\ndef arc_read(url, where=\"1=1\", outFields=\"*\", outSR=4326, **kwargs):\n    \"\"\"\n    Read an ArcGIS FeatureLayer or Table to a GeoDataFrame.\n\n    Parameters:\n    url (str): The ArcGIS REST service URL ending with /MapServer/0 or /FeatureServer/0\n    where (str): SQL WHERE clause for filtering. Default: \"1=1\" (all records)\n    outFields (str): Comma-separated field names or \"*\" for all fields. Default: \"*\"\n    outSR (int): Output spatial reference EPSG code. Default: 4326\n    **kwargs: Additional query parameters passed to the ArcGIS REST API\n\n    Returns:\n    geopandas.GeoDataFrame: Spatial data from the service\n    \"\"\"\n\n    # Ensure URL ends with /query\n    if not url.endswith('/query'):\n        url = url.rstrip('/') + '/query'\n\n    # Build query parameters\n    params = {\n        'where': where,\n        'outFields': outFields,\n        'returnGeometry': 'true',\n        # 'geometryType': 'esriGeometryPoint',\n        'outSR': outSR,\n        'f': 'geojson'\n    }\n\n    # Add any additional parameters\n    params.update(kwargs)\n\n    # Make request\n    response = requests.get(url, params=params)\n\n    # Read as GeoDataFrame\n    return gpd.read_file(response.text)\n\n\n\n# Public School Enrollment data 2019-2020\nlscog_pub_sch_enroll = arc_read(\n    url=\"https://nces.ed.gov/opengis/rest/services/K12_School_Locations/EDGE_ADMINDATA_PUBLICSCH_1920/MapServer/0\",\n    where=f\"LSTATE = '{state_abb}' AND NMCNTY IN ('{\"', '\".join([f\"{name} County\" for name in county_names])}')\",\n    outFields='NCESSCH,SCH_NAME,LSTATE,TOTAL,FTE'\n)\n\n# Transform CRS\nlscog_pub_sch_enroll = lscog_pub_sch_enroll.to_crs(project_crs)\n\n# Select and rename columns\nlscog_pub_sch_enroll = lscog_pub_sch_enroll.rename(columns={\n    'NCESSCH': 'INSTITUTION_ID',\n    'SCH_NAME': 'NAME',\n    'LSTATE': 'STATE',\n    'TOTAL': 'STUDENT_COUNT_PUB',\n    'FTE': 'TEACHER_COUNT_PUB'\n})\n\nlscog_pub_sch_enroll\n\n   INSTITUTION_ID  ...                        geometry\n0    450108001163  ...  POINT (1891532.112 517376.183)\n1    450075000064  ...  POINT (1913417.281 420524.266)\n2    450075001184  ...   POINT (1916345.361 418209.84)\n3    450075001415  ...  POINT (1913417.281 420524.266)\n4    450093000119  ...  POINT (1991503.106 535256.782)\n..            ...  ...                             ...\n93   450391001291  ...  POINT (2048660.583 606357.033)\n94   450391001370  ...    POINT (2040865.5 608975.982)\n95   450391001604  ...  POINT (2050404.085 617575.512)\n96   450391001693  ...    POINT (2030130.8 597632.129)\n97   450391001694  ...  POINT (2043557.083 596562.932)\n\n[98 rows x 6 columns]\n\n\n\n\n\n\n\nPrivate schools\nPrivate school enrollment data is accessed from the NCES Private School Universe Survey (PSS) archived dataset. The data is spatially enabled using latitude and longitude coordinates and filtered to include only institutions within the study area TAZ boundaries. Private schools contribute to the regional education trip matrix and must be incorporated alongside public institutions for comprehensive coverage.\n\nFor more information on the NCES Private Schools data, refer to PSS Online Documentation.\n\n\n R Python\n\n\n\n# Private School Enrollment data 2019-2020\nlscog_pvt_sch_enroll &lt;- vroom::vroom(\n  unz(\n    file.path(\n      root,\n      \"GIS/data_external/20250315 NCES/PSS - Private/2019-20/pss1920_pu_csv.zip\"\n    ),\n    \"pss1920_pu.csv\"\n  ),\n  col_types = vroom::cols_only(\n    PPIN        = vroom::col_character(),\n    PINST       = vroom::col_character(),\n    PL_STABB    = vroom::col_character(),\n    PCNTNM      = vroom::col_character(),\n    SIZE        = vroom::col_double(),\n    NUMTEACH    = vroom::col_double(),\n    LATITUDE20  = vroom::col_double(),\n    LONGITUDE20 = vroom::col_double()\n  )\n) |&gt;\n  sf::st_as_sf(coords = c(\"LONGITUDE20\", \"LATITUDE20\"), crs = \"EPSG:4326\") |&gt;\n  sf::st_transform(project_crs) |&gt;\n  sf::st_filter(lscog_taz, .predicate = st_intersects) |&gt;\n  dplyr::select(\n    INSTITUTION_ID = PPIN,\n    NAME = PINST,\n    STATE = PL_STABB,\n    STUDENT_COUNT_PVT = SIZE,\n    TEACHER_COUNT_PVT = NUMTEACH\n  )\n\nlscog_pvt_sch_enroll\n\nSimple feature collection with 24 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1704062 ymin: 492304.2 xmax: 2179510 ymax: 720184.2\nProjected CRS: NAD83(HARN) / South Carolina (ft)\n# A tibble: 24 Ã— 6\n   INSTITUTION_ID NAME                 STATE STUDENT_COUNT_PVT TEACHER_COUNT_PVT\n   &lt;chr&gt;          &lt;chr&gt;                &lt;chr&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 K9305823       AIKENS FBC PRESCHOOL SC                    1               1.3\n 2 01264947       ANDREW JACKSON ACADâ€¦ SC                    3              13.3\n 3 A9106158       BARNWELL CHRISTIAN â€¦ &lt;NA&gt;                  2               5.8\n 4 01263568       CALHOUN ACADEMY      SC                    3              26.6\n 5 A1771477       FIRST BAPTIST CHURCâ€¦ &lt;NA&gt;                  1               3.1\n 6 A9703151       FIRST PRESBYTERIAN â€¦ &lt;NA&gt;                  1               2.8\n 7 BB170334       FIRST SOUTHERN METHâ€¦ SC                    1               6  \n 8 A1102039       FOUNDATION CHRISTIAâ€¦ &lt;NA&gt;                  1               5  \n 9 A0307976       GRACE CHILD DEVELOPâ€¦ &lt;NA&gt;                  1               2.9\n10 A0307978       GREATER FAITH BAPTIâ€¦ &lt;NA&gt;                  1               1  \n# â„¹ 14 more rows\n# â„¹ 1 more variable: geometry &lt;POINT [foot]&gt;\n\n\n\n\n\n# Private School Enrollment data 2019-2020\nzip_path = Path(root) / \"GIS/data_external/20250315 NCES/PSS - Private/2019-20/pss1920_pu_csv.zip\"\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    with zip_ref.open('pss1920_pu.csv') as csv_file:\n        lscog_pvt_sch_enroll = pd.read_csv(\n            csv_file,\n            usecols=['PPIN', 'PINST', 'PL_STABB', 'PCNTNM', 'SIZE', 'NUMTEACH', 'LATITUDE20', 'LONGITUDE20'],\n            dtype={'PPIN': 'str', 'PINST': 'str', 'PL_STABB': 'str', 'PCNTNM': 'str', 'SIZE': 'float64', 'NUMTEACH': 'float64'}\n        )\n\nlscog_pvt_sch_enroll = gpd.GeoDataFrame(\n    lscog_pvt_sch_enroll,\n    geometry=gpd.points_from_xy(lscog_pvt_sch_enroll['LONGITUDE20'], lscog_pvt_sch_enroll['LATITUDE20']),\n    crs='EPSG:4326'\n).to_crs(project_crs)\n\nlscog_pvt_sch_enroll = gpd.sjoin(lscog_pvt_sch_enroll, lscog_taz, how='inner', predicate='intersects')[\n    ['PPIN', 'PINST', 'PL_STABB', 'SIZE', 'NUMTEACH', 'geometry']\n].rename(columns={\n    'PPIN': 'INSTITUTION_ID',\n    'PINST': 'NAME',\n    'PL_STABB': 'STATE',\n    'SIZE': 'STUDENT_COUNT_PVT',\n    'NUMTEACH': 'TEACHER_COUNT_PVT'\n})\n\nlscog_pvt_sch_enroll\n\n      INSTITUTION_ID  ...                        geometry\n17469       K9305823  ...  POINT (1781275.702 629440.997)\n17474       01264947  ...   POINT (1993756.78 492304.247)\n17476       A9106158  ...   POINT (1914982.59 524548.558)\n17484       01263568  ...  POINT (2072197.427 660303.544)\n17533       A1771477  ...  POINT (1704061.773 606321.234)\n17537       A9703151  ...  POINT (1780623.642 630282.337)\n17538       BB170334  ...   POINT (2037420.57 608741.769)\n17543       A1102039  ...  POINT (2006658.982 720184.199)\n17547       A0307976  ...   POINT (1704601.274 606316.17)\n17550       A0307978  ...  POINT (2047302.618 604573.723)\n17566       A1904026  ...  POINT (2179509.785 547981.083)\n17571       01263692  ...  POINT (1918041.289 549840.623)\n17599       01264754  ...  POINT (1779301.654 629488.248)\n17601       A0308015  ...  POINT (1733987.726 611499.727)\n17623       A1303185  ...   POINT (1853302.82 681159.161)\n17637       A9903938  ...   POINT (2047246.79 597082.682)\n17651       A0109147  ...  POINT (1780574.399 631607.375)\n17657       01264288  ...  POINT (1778262.542 613244.891)\n17658       K9305825  ...  POINT (1779510.666 617490.702)\n17663       K9305640  ...   POINT (2041004.71 611841.216)\n17672       A9703170  ...   POINT (1780823.53 629681.358)\n17676       01262826  ...   POINT (1781344.96 628712.135)\n17722       01932407  ...   POINT (2037497.91 608547.947)\n17733       A9903957  ...  POINT (1875514.037 565892.953)\n\n[24 rows x 6 columns]\n\n\n\n\n\n\n\nPost-secondary institutions\nPost-secondary institution locations are obtained from the NCES Integrated Postsecondary Education Data System (IPEDS), filtered by state and county FIPS codes. These institutions generate significant travel demand through student commuting, employee travel, and visitor trips, making them essential components of the regional transportation network analysis.\n\nFor more information on the NCES Post-Secondary Education data, refer to IPEDS Documentation.\n\n\n R Python\n\n\n\n# Post-Secondary Location data 2019-2020\nlscog_college_loc &lt;- arcgislayers::arc_read(\n  url = \"https://nces.ed.gov/opengis/rest/services/Postsecondary_School_Locations/EDGE_GEOCODE_POSTSECONDARYSCH_1920/MapServer/0\",\n  where = paste0(\n    \"STATE = '\",\n    state_abb,\n    \"' AND CNTY IN (\",\n    paste0(\"'\", fips_codes, \"'\", collapse = \", \"),\n    \")\"\n  ),\n  alias = \"label\",\n  crs = project_crs\n)\n\nlscog_college_loc\n\nSimple feature collection with 11 features and 24 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1708029 ymin: 429827.9 xmax: 2052011 ymax: 633710.3\nProjected CRS: NAD83(HARN) / South Carolina (ft)\nFirst 10 features:\n   OBJECTID UNITID                                               NAME\n1      3411 217615                            Aiken Technical College\n2      3424 217873                                 Claflin University\n3      3431 217989                          Denmark Technical College\n4      3439 218159 Kenneth Shuler School of Cosmetology-North Augusta\n5      3448 218487               Orangeburg Calhoun Technical College\n6      3451 218645                 University of South Carolina Aiken\n7      3455 218681          University of South Carolina-Salkehatchie\n8      3459 218733                    South Carolina State University\n9      3468 218919                                   Voorhees College\n10     5829 457998          Aiken School of Cosmetology and Barbering\n                         STREET          CITY STATE        ZIP STFIP  CNTY\n1  2276 Jefferson Davis Highway  Graniteville    SC      29829    45 45003\n2           400 Magnolia Street    Orangeburg    SC 29115-4498    45 45075\n3       1126 Solomon Blatt Blvd       Denmark    SC      29042    45 45009\n4              1113 Knox Avenue North Augusta    SC      29841    45 45003\n5        3250 Saint Matthews Rd    Orangeburg    SC 29118-8299    45 45075\n6           471 University Pkwy         Aiken    SC      29801    45 45003\n7         465 James Brandt Blvd     Allendale    SC 29810-0617    45 45005\n8             300 College St NE    Orangeburg    SC 29117-0001    45 45075\n9              481 Porter Drive       Denmark    SC      29042    45 45009\n10        225 Richland Ave East         Aiken    SC      29801    45 45003\n              NMCNTY LOCALE      LAT       LON  CBSA\n1       Aiken County     41 33.53383 -81.84167 12260\n2  Orangeburg County     32 33.49844 -80.85432 36700\n3     Bamberg County     41 33.31336 -81.12363     N\n4       Aiken County     21 33.49759 -81.95789 12260\n5  Orangeburg County     41 33.54485 -80.82927 36700\n6       Aiken County     21 33.57270 -81.76761 12260\n7   Allendale County     41 33.01431 -81.30184     N\n8  Orangeburg County     32 33.49797 -80.84872 36700\n9     Bamberg County     32 33.30720 -81.12786     N\n10      Aiken County     21 33.55954 -81.71728 12260\n                           NMCBSA CBSATYPE CSA                            NMCSA\n1  Augusta-Richmond County, GA-SC        1   N                                N\n2                  Orangeburg, SC        2 192 Columbia-Orangeburg-Newberry, SC\n3                               N        0   N                                N\n4  Augusta-Richmond County, GA-SC        1   N                                N\n5                  Orangeburg, SC        2 192 Columbia-Orangeburg-Newberry, SC\n6  Augusta-Richmond County, GA-SC        1   N                                N\n7                               N        0   N                                N\n8                  Orangeburg, SC        2 192 Columbia-Orangeburg-Newberry, SC\n9                               N        0   N                                N\n10 Augusta-Richmond County, GA-SC        1   N                                N\n   NECTA NMNECTA   CD  SLDL  SLDU SCHOOLYEAR                 geometry\n1      N       N 4502 45084 45025  2019-2020 POINT (1743560 619744.3)\n2      N       N 4506 45095 45039  2019-2020 POINT (2044404 605856.3)\n3      N       N 4506 45090 45040  2019-2020 POINT (1962235 538509.9)\n4      N       N 4502 45083 45024  2019-2020 POINT (1708029 606866.3)\n5      N       N 4506 45095 45040  2019-2020 POINT (2052011 622751.3)\n6      N       N 4502 45081 45025  2019-2020 POINT (1766228 633710.3)\n7      N       N 4506 45091 45045  2019-2020 POINT (1907482 429827.9)\n8      N       N 4506 45095 45039  2019-2020 POINT (2046110 605685.6)\n9      N       N 4506 45090 45040  2019-2020 POINT (1960939 536271.9)\n10     N       N 4502 45081 45026  2019-2020 POINT (1781522 628812.8)\n\n\n\n\n\n# Post-Secondary Location data 2019-2020\nlscog_college_loc = arc_read(\n    url=\"https://nces.ed.gov/opengis/rest/services/Postsecondary_School_Locations/EDGE_GEOCODE_POSTSECONDARYSCH_1920/MapServer/0\",\n    where=f\"STATE = '{state_abb}' AND CNTY IN ('{\"', '\".join([f\"{fip}\" for fip in fips_codes])}')\",\n    outFields='*',\n    outSR=project_crs\n)\n\nlscog_college_loc = lscog_college_loc.to_crs(project_crs)\n\nlscog_college_loc\n\n    OBJECTID  UNITID  ... SCHOOLYEAR                        geometry\n0       3411  217615  ...  2019-2020  POINT (1743561.221 619741.983)\n1       3424  217873  ...  2019-2020  POINT (2044404.666 605853.958)\n2       3431  217989  ...  2019-2020  POINT (1962236.326 538507.569)\n3       3439  218159  ...  2019-2020  POINT (1708030.354 606863.953)\n4       3448  218487  ...  2019-2020   POINT (2052011.899 622748.89)\n5       3451  218645  ...  2019-2020  POINT (1766228.593 633707.888)\n6       3455  218681  ...  2019-2020  POINT (1907483.079 429825.628)\n7       3459  218733  ...  2019-2020  POINT (2046110.928 605683.233)\n8       3468  218919  ...  2019-2020   POINT (1960940.241 536269.53)\n9       5829  457998  ...  2019-2020  POINT (1781523.402 628810.398)\n10      6683  488022  ...  2019-2020  POINT (2043606.983 603651.758)\n\n[11 rows x 25 columns]"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#household-weighted-interpolation",
    "href": "posts/socioeconomic-demo/index.html#household-weighted-interpolation",
    "title": "Base Year SE Data Development",
    "section": "4.1 Household-weighted interpolation",
    "text": "4.1 Household-weighted interpolation\nThe interpolation process transfers ACS block group data to individual census blocks using household counts as weights. This method ensures that socioeconomic characteristics are distributed proportionally based on residential density rather than simple geometric overlay. The {tidycensus} package provides robust interpolation functionality that preserves the extensive nature of count variables while maintaining spatial relationships. For Python, the interpolate_pw() function is implemented to achieve similar functionality using population-weighted interpolation.\n\n R Python\n\n\n\n# Interpolate ACS data to Decennial Census blocks\nlscog_acs_cb &lt;- tidycensus::interpolate_pw(\n  from = lscog_acs,\n  to = lscog_dec,\n  to_id = \"GEOID\",\n  extensive = TRUE,\n  weights = lscog_dec,\n  crs = project_crs,\n  weight_column = \"HH\"\n)\n\nlscog_acs_cb\n\nSimple feature collection with 13961 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1691517 ymin: 331891.7 xmax: 2237472 ymax: 744669.5\nProjected CRS: NAD83(HARN) / South Carolina (ft)\n# A tibble: 13,961 Ã— 6\n   GEOID                        geometry INC_TOTAL INC_14999 INC_49999 INC_50000\n   &lt;chr&gt;           &lt;MULTIPOLYGON [foot]&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 4501795040â€¦ (((2084301 622308.5, 208â€¦      0        0         0         0    \n 2 4501795040â€¦ (((2112519 626782.2, 211â€¦      0        0         0         0    \n 3 4501795020â€¦ (((2067775 662339.5, 206â€¦     20.6      4.27      7.56      8.72 \n 4 4501795040â€¦ (((2087947 684345.6, 208â€¦      0        0         0         0    \n 5 4507501050â€¦ (((2089073 552687.4, 208â€¦      5.78     1.94      0.924     2.92 \n 6 4507501170â€¦ (((2046694 542273.9, 204â€¦      2.48     0.393     1.30      0.780\n 7 4507501170â€¦ (((2056704 510850.5, 205â€¦      0        0         0         0    \n 8 4507501180â€¦ (((1960896 587381.7, 196â€¦      3.19     0.694     1.42      1.08 \n 9 4507501200â€¦ (((2000969 657869.3, 200â€¦      0        0         0         0    \n10 4501795010â€¦ (((2016178 708106.6, 201â€¦      0        0         0         0    \n# â„¹ 13,951 more rows\n\n\n\n\n\n\nCode\ndef interpolate_pw(from_gdf, to_gdf, weights_gdf, to_id=None, extensive=True,\n                   weight_column=None, weight_placement='surface', crs=None):\n    \"\"\"\n    Population-weighted areal interpolation between geometries.\n\n    Transfers numeric data from source geometries to target geometries using\n    population-weighted interpolation based on point weights (e.g., census blocks).\n\n    Parameters:\n    -----------\n    from_gdf : GeoDataFrame\n        Source geometries with numeric data to interpolate\n    to_gdf : GeoDataFrame\n        Target geometries to interpolate data to\n    weights_gdf : GeoDataFrame\n        Weight geometries (e.g., census blocks) used for interpolation.\n        If polygons, will be converted to points. Can be the same as to_gdf.\n    to_id : str, optional\n        Column name for unique identifier in target geometries.\n        If None, creates an 'id' column.\n    extensive : bool, default True\n        If True, return weighted sums (for counts).\n        If False, return weighted means (for rates/percentages).\n    weight_column : str, optional\n        Column name in weights_gdf for weighting (e.g., 'POP', 'HH').\n        If None, all weights are equal.\n    weight_placement : str, default 'surface'\n        How to convert polygons to points: 'surface' or 'centroid'\n    crs : str or CRS object, optional\n        Coordinate reference system to project all datasets to\n\n    Returns:\n    --------\n    GeoDataFrame\n        Target geometries with interpolated numeric values\n    \"\"\"\n\n    # Input validation\n    if not all(isinstance(gdf, gpd.GeoDataFrame) for gdf in [from_gdf, to_gdf, weights_gdf]):\n        raise ValueError(\"All inputs must be GeoDataFrames\")\n\n    # Make copies to avoid modifying originals\n    from_gdf = from_gdf.copy()\n    to_gdf = to_gdf.copy()\n    weights_gdf = weights_gdf.copy()\n\n    # Set CRS if provided\n    if crs:\n        from_gdf = from_gdf.to_crs(crs)\n        to_gdf = to_gdf.to_crs(crs)\n        weights_gdf = weights_gdf.to_crs(crs)\n\n    # Check CRS consistency\n    if not (from_gdf.crs == to_gdf.crs == weights_gdf.crs):\n        raise ValueError(\"All inputs must have the same CRS\")\n\n    # Handle to_id\n    if to_id is None:\n        to_id = 'id'\n        to_gdf[to_id] = to_gdf.index.astype(str)\n\n    # Remove conflicting columns\n    if to_id in from_gdf.columns:\n        from_gdf = from_gdf.drop(columns=[to_id])\n\n    # Create unique from_id\n    from_id = 'from_id'\n    from_gdf[from_id] = from_gdf.index.astype(str)\n\n    # Handle weight column\n    if weight_column is None:\n        weight_column = 'interpolation_weight'\n        weights_gdf[weight_column] = 1.0\n    else:\n        # Rename to avoid conflicts\n        weights_gdf['interpolation_weight'] = weights_gdf[weight_column]\n        weight_column = 'interpolation_weight'\n\n    # Convert weights to points if needed\n    if weights_gdf.geometry.geom_type.iloc[0] in ['Polygon', 'MultiPolygon']:\n        if weight_placement == 'surface':\n            weights_gdf = weights_gdf.copy()\n            weights_gdf.geometry = weights_gdf.geometry.representative_point()\n        elif weight_placement == 'centroid':\n            weights_gdf = weights_gdf.copy()\n            weights_gdf.geometry = weights_gdf.geometry.centroid\n        else:\n            raise ValueError(\"weight_placement must be 'surface' or 'centroid'\")\n\n    # Keep only weight column and geometry\n    weight_points = weights_gdf[[weight_column, 'geometry']].copy()\n\n    # Calculate denominators (total weights per source geometry)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=UserWarning)\n        source_weights = gpd.sjoin(from_gdf, weight_points, how='left', predicate='contains')\n\n    denominators = (source_weights.groupby(from_id)[weight_column]\n                   .sum()\n                   .reset_index()\n                   .rename(columns={weight_column: 'weight_total'}))\n\n    # Calculate intersections between from and to\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=UserWarning)\n        intersections = gpd.overlay(from_gdf, to_gdf, how='intersection')\n\n    # Filter to keep only polygon intersections\n    intersections = intersections[intersections.geometry.geom_type.isin(['Polygon', 'MultiPolygon', 'GeometryCollection'])]\n\n    if len(intersections) == 0:\n        raise ValueError(\"No valid polygon intersections found between source and target geometries\")\n\n    # Add intersection ID\n    intersections['intersection_id'] = range(len(intersections))\n\n    # Spatial join intersections with weight points to get weights within each intersection\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=UserWarning)\n        intersection_weights = gpd.sjoin(intersections, weight_points, how='left', predicate='contains')\n\n    # Calculate intersection values (sum of weights per intersection)\n    intersection_values = (intersection_weights.groupby('intersection_id')[weight_column]\n                         .sum()\n                         .reset_index()\n                         .rename(columns={weight_column: 'intersection_value'}))\n\n    # Merge back to intersections and keep only unique intersections\n    intersections = intersections.merge(intersection_values, on='intersection_id', how='left')\n    intersections['intersection_value'] = intersections['intersection_value'].fillna(0)\n\n    # Remove duplicates created by the spatial join\n    intersections = intersections.drop_duplicates(subset='intersection_id')\n\n    # Merge with denominators to calculate weight coefficients\n    intersections = intersections.merge(denominators, on=from_id, how='left')\n    intersections['weight_total'] = intersections['weight_total'].fillna(1)\n\n    # Calculate weight coefficients (intersection weight / total weight in source)\n    intersections.loc[intersections['weight_total'] &gt; 0, 'weight_coef'] = (\n        intersections['intersection_value'] / intersections['weight_total']\n    )\n    intersections['weight_coef'] = intersections['weight_coef'].fillna(0)\n\n    # Get numeric columns from source data\n    numeric_cols = from_gdf.select_dtypes(include=[np.number]).columns\n    # Remove ID columns\n    numeric_cols = [col for col in numeric_cols if col not in [from_id]]\n\n    # Prepare intersection data for interpolation\n    intersection_data = intersections[[from_id, to_id, 'weight_coef'] + numeric_cols].copy()\n\n    if extensive:\n        # For extensive variables: multiply by weight coefficient, then sum by target\n        for col in numeric_cols:\n            intersection_data[col] = intersection_data[col] * intersection_data['weight_coef']\n\n        interpolated = (intersection_data.groupby(to_id)[numeric_cols]\n                       .sum()\n                       .reset_index())\n    else:\n        # For intensive variables: weighted average\n        interpolated_data = []\n        for target_id in intersection_data[to_id].unique():\n            target_data = intersection_data[intersection_data[to_id] == target_id]\n            if len(target_data) &gt; 0 and target_data['weight_coef'].sum() &gt; 0:\n                weighted_vals = {}\n                for col in numeric_cols:\n                    weighted_vals[col] = (target_data[col] * target_data['weight_coef']).sum() / target_data['weight_coef'].sum()\n                weighted_vals[to_id] = target_id\n                interpolated_data.append(weighted_vals)\n\n        interpolated = pd.DataFrame(interpolated_data)\n\n    # Merge with target geometries\n    result = to_gdf[[to_id, 'geometry']].merge(interpolated, on=to_id, how='left')\n\n    # Fill NaN values with 0 for missing interpolations\n    for col in numeric_cols:\n        if col in result.columns:\n            result[col] = result[col].fillna(0)\n\n    return result\n\n\n\n# Interpolate ACS data to Decennial Census blocks\nlscog_acs_cb = interpolate_pw(\n    from_gdf=lscog_acs,\n    to_gdf=lscog_dec,\n    weights_gdf=lscog_dec,\n    to_id='GEOID',\n    extensive=True,\n    weight_column='HH',\n    crs=project_crs\n)\n\nlscog_acs_cb.head()\n\n             GEOID  ... INC_50000\n0  450179504004051  ...  0.000000\n1  450179504003011  ...  0.000000\n2  450179502011045  ...  8.723288\n3  450179504001020  ...  0.000000\n4  450750105003029  ...  2.916335\n\n[5 rows x 6 columns]\n\n\n\n\n\nThe comparative visualization reveals the increased spatial resolution achieved through interpolation. Block-level data provides more granular detail for transportation modeling applications, enabling better representation of local variations in income distribution across the study area.\n\n R Python\n\n\n\n# Compare before and after interpolation\nmapview::mapview(lscog_acs_cb, zcol = \"INC_49999\", color = NA) |\n  mapview::mapview(lscog_acs, zcol = \"INC_49999\", color = NA)\n\n\n\n\n# Compare before and after interpolation\nlscog_acs_cb.explore(column=\"INC_49999\", color=\"blue\", legend=True, tiles=\"CartoDB positron\") |\\\n    lscog_acs.explore(column=\"INC_49999\", color=\"red\", legend=True, tiles=\"CartoDB positron\")"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#combine-population-and-households",
    "href": "posts/socioeconomic-demo/index.html#combine-population-and-households",
    "title": "Base Year SE Data Development",
    "section": "4.2 Combine population and households",
    "text": "4.2 Combine population and households\nThe integration step merges the interpolated ACS socioeconomic data with the Decennial Census population and household counts. This join operation creates a unified dataset containing both demographic totals and detailed characteristics at the census block level. The left join ensures that all census blocks retain their geographic boundaries while incorporating available socioeconomic attributes.\n\n R Python\n\n\n\n## Combine ACS Data to Decennial data\nlscog_pop_hh &lt;- lscog_dec |&gt;\n  dplyr::left_join(\n    sf::st_drop_geometry(lscog_acs_cb),\n    by = dplyr::join_by(GEOID)\n  )\n\nlscog_pop_hh\n\nSimple feature collection with 13961 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1691517 ymin: 331891.7 xmax: 2237472 ymax: 744669.5\nProjected CRS: NAD83(HARN) / South Carolina (ft)\n# A tibble: 13,961 Ã— 15\n   GEOID           TOTPOP GQPOP HHPOP    HH  HH_1  HH_2  HH_3  HH_4    DU\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 450179504004051      0     0     0     0     0     0     0     0     0\n 2 450179504003011      0     0     0     0     0     0     0     0     0\n 3 450179502011045     54     4    50    16     3     1     3     9    18\n 4 450179504001020      0     0     0     0     0     0     0     0     0\n 5 450750105003029     13     0    13     4     0     3     0     1     4\n 6 450750117021009     10     0    10     3     3     0     0     0     8\n 7 450750117011051      0     0     0     0     0     0     0     0     0\n 8 450750118021087      6     0     6     4     0     1     2     1     4\n 9 450750120003020      0     0     0     0     0     0     0     0     0\n10 450179501003046     18     0    18     0     0     0     0     0     1\n# â„¹ 13,951 more rows\n# â„¹ 5 more variables: geometry &lt;MULTIPOLYGON [foot]&gt;, INC_TOTAL &lt;dbl&gt;,\n#   INC_14999 &lt;dbl&gt;, INC_49999 &lt;dbl&gt;, INC_50000 &lt;dbl&gt;\n\n\n\n\n\n## Combine ACS Data to Decennial data\nlscog_pop_hh = lscog_dec.merge(\n    lscog_acs_cb.drop(columns=['geometry']),\n    on='GEOID',\n    how='left'\n)\n\nlscog_pop_hh\n\n                 GEOID  TOTPOP  GQPOP  ...  INC_14999  INC_49999  INC_50000\n0      450179504004051       0      0  ...   0.000000   0.000000   0.000000\n1      450179504003011       0      0  ...   0.000000   0.000000   0.000000\n2      450179502011045      54      4  ...   4.273973   7.561644   8.723288\n3      450179504001020       0      0  ...   0.000000   0.000000   0.000000\n4      450750105003029      13      0  ...   1.944223   0.924303   2.916335\n...                ...     ...    ...  ...        ...        ...        ...\n13956  450059705003050       5      0  ...   0.000000   0.000000   0.000000\n13957  450030203042000       0      0  ...   0.000000   0.000000   0.000000\n13958  450030218002115       8      0  ...   0.101167   0.470817   0.295720\n13959  450030206012008       0      0  ...   0.000000   0.000000   0.000000\n13960  450059705002005       0      0  ...   0.000000   0.000000   0.000000\n\n[13961 rows x 15 columns]\n\n\n\n\n\nIncome category adjustments reconcile the interpolated ACS estimates with actual household counts from the Decennial Census. The proportional allocation method redistributes income categories based on the ratio of interpolated totals to observed household counts, maintaining consistency between data sources. The three-tier income classification (under $15,000, $15,000-$49,999, and $50,000 and above) provides sufficient granularity for travel demand modeling while ensuring statistical reliability.\n\n R Python\n\n\n\n## Combine adjusted HH income level to Decennial census instead of ACS\nlscog_pop_hh &lt;- lscog_pop_hh |&gt;\n  dplyr::mutate(\n    INC_49999 = tidyr::replace_na(round(INC_49999 / INC_TOTAL * HH, 0), 0),\n    INC_50000 = tidyr::replace_na(round(INC_50000 / INC_TOTAL * HH, 0), 0),\n    INC_14999 = HH - (INC_49999 + INC_50000)\n  ) |&gt;\n  dplyr::select(-INC_TOTAL)\n\nlscog_pop_hh\n\nSimple feature collection with 13961 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1691517 ymin: 331891.7 xmax: 2237472 ymax: 744669.5\nProjected CRS: NAD83(HARN) / South Carolina (ft)\n# A tibble: 13,961 Ã— 14\n   GEOID           TOTPOP GQPOP HHPOP    HH  HH_1  HH_2  HH_3  HH_4    DU\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 450179504004051      0     0     0     0     0     0     0     0     0\n 2 450179504003011      0     0     0     0     0     0     0     0     0\n 3 450179502011045     54     4    50    16     3     1     3     9    18\n 4 450179504001020      0     0     0     0     0     0     0     0     0\n 5 450750105003029     13     0    13     4     0     3     0     1     4\n 6 450750117021009     10     0    10     3     3     0     0     0     8\n 7 450750117011051      0     0     0     0     0     0     0     0     0\n 8 450750118021087      6     0     6     4     0     1     2     1     4\n 9 450750120003020      0     0     0     0     0     0     0     0     0\n10 450179501003046     18     0    18     0     0     0     0     0     1\n# â„¹ 13,951 more rows\n# â„¹ 4 more variables: geometry &lt;MULTIPOLYGON [foot]&gt;, INC_14999 &lt;dbl&gt;,\n#   INC_49999 &lt;dbl&gt;, INC_50000 &lt;dbl&gt;\n\n\n\n\n\n## Combine adjusted HH income level to Decennial census instead of ACS\nlscog_pop_hh[\"INC_49999\"] = ((lscog_pop_hh[\"INC_49999\"] / lscog_pop_hh[\"INC_TOTAL\"]) * lscog_pop_hh[\"HH\"]).round().fillna(0)\nlscog_pop_hh[\"INC_50000\"] = ((lscog_pop_hh[\"INC_50000\"] / lscog_pop_hh[\"INC_TOTAL\"]) * lscog_pop_hh[\"HH\"]).round().fillna(0)\nlscog_pop_hh[\"INC_14999\"] = lscog_pop_hh[\"HH\"] - (lscog_pop_hh[\"INC_49999\"] + lscog_pop_hh[\"INC_50000\"])\n\nlscog_pop_hh = lscog_pop_hh.drop(columns=\"INC_TOTAL\")\nlscog_pop_hh\n\n                 GEOID  TOTPOP  GQPOP  ...  INC_14999  INC_49999  INC_50000\n0      450179504004051       0      0  ...        0.0        0.0        0.0\n1      450179504003011       0      0  ...        0.0        0.0        0.0\n2      450179502011045      54      4  ...        3.0        6.0        7.0\n3      450179504001020       0      0  ...        0.0        0.0        0.0\n4      450750105003029      13      0  ...        1.0        1.0        2.0\n...                ...     ...    ...  ...        ...        ...        ...\n13956  450059705003050       5      0  ...        0.0        0.0        0.0\n13957  450030203042000       0      0  ...        0.0        0.0        0.0\n13958  450030218002115       8      0  ...        0.0        1.0        0.0\n13959  450030206012008       0      0  ...        0.0        0.0        0.0\n13960  450059705002005       0      0  ...        0.0        0.0        0.0\n\n[13961 rows x 14 columns]"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#employment-data",
    "href": "posts/socioeconomic-demo/index.html#employment-data",
    "title": "Base Year SE Data Development",
    "section": "4.3 Employment data",
    "text": "4.3 Employment data\nThe employment integration incorporates LEHD (Longitudinal Employer-Household Dynamics) workplace area characteristics into the combined dataset. This addition provides employment counts by census block, enabling the development of trip attraction models and work-based travel pattern analysis. The merge operation maintains the geographic integrity of census blocks while adding employment variables essential for comprehensive transportation planning.\n\n R Python\n\n\n\n# Join LEHD Data to the Decennial data\nlscog_pop_hh_emp &lt;- lscog_pop_hh |&gt;\n  dplyr::left_join(lscog_emp, by = dplyr::join_by(GEOID))\n\nlscog_pop_hh_emp\n\nSimple feature collection with 13961 features and 24 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1691517 ymin: 331891.7 xmax: 2237472 ymax: 744669.5\nProjected CRS: NAD83(HARN) / South Carolina (ft)\n# A tibble: 13,961 Ã— 25\n   GEOID           TOTPOP GQPOP HHPOP    HH  HH_1  HH_2  HH_3  HH_4    DU\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 450179504004051      0     0     0     0     0     0     0     0     0\n 2 450179504003011      0     0     0     0     0     0     0     0     0\n 3 450179502011045     54     4    50    16     3     1     3     9    18\n 4 450179504001020      0     0     0     0     0     0     0     0     0\n 5 450750105003029     13     0    13     4     0     3     0     1     4\n 6 450750117021009     10     0    10     3     3     0     0     0     8\n 7 450750117011051      0     0     0     0     0     0     0     0     0\n 8 450750118021087      6     0     6     4     0     1     2     1     4\n 9 450750120003020      0     0     0     0     0     0     0     0     0\n10 450179501003046     18     0    18     0     0     0     0     0     1\n# â„¹ 13,951 more rows\n# â„¹ 15 more variables: geometry &lt;MULTIPOLYGON [foot]&gt;, INC_14999 &lt;dbl&gt;,\n#   INC_49999 &lt;dbl&gt;, INC_50000 &lt;dbl&gt;, TOTAL_EMP &lt;dbl&gt;, AGR_FOR_FI &lt;dbl&gt;,\n#   MINING &lt;dbl&gt;, CONSTRUCTI &lt;dbl&gt;, MANUFACTUR &lt;dbl&gt;, TRANSP_COM &lt;dbl&gt;,\n#   WHOLESALE &lt;dbl&gt;, RETAIL &lt;dbl&gt;, FIRE &lt;dbl&gt;, SERVICES &lt;dbl&gt;, PUBLIC_ADM &lt;dbl&gt;\n\n\n\n\n\n# Join LEHD Data to the Decennial data\nlscog_pop_hh_emp = lscog_pop_hh.merge(\n    lscog_emp,\n    on='GEOID',\n    how='left'\n)\n\nlscog_pop_hh_emp\n\n                 GEOID  TOTPOP  GQPOP  ...  FIRE  SERVICES  PUBLIC_ADM\n0      450179504004051       0      0  ...   NaN       NaN         NaN\n1      450179504003011       0      0  ...   NaN       NaN         NaN\n2      450179502011045      54      4  ...   NaN       NaN         NaN\n3      450179504001020       0      0  ...   NaN       NaN         NaN\n4      450750105003029      13      0  ...   NaN       NaN         NaN\n...                ...     ...    ...  ...   ...       ...         ...\n13956  450059705003050       5      0  ...   NaN       NaN         NaN\n13957  450030203042000       0      0  ...   NaN       NaN         NaN\n13958  450030218002115       8      0  ...   NaN       NaN         NaN\n13959  450030206012008       0      0  ...   NaN       NaN         NaN\n13960  450059705002005       0      0  ...   NaN       NaN         NaN\n\n[13961 rows x 25 columns]"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#taz-data",
    "href": "posts/socioeconomic-demo/index.html#taz-data",
    "title": "Base Year SE Data Development",
    "section": "5.1 TAZ data",
    "text": "5.1 TAZ data\nThe Traffic Analysis Zone (TAZ) boundary export provides the fundamental geographic framework for the regional travel demand model. The blank TAZ file serves as a template for subsequent socioeconomic data allocation, containing only zone identification fields and geometric boundaries without attribute data.\nExport as CSV flat file\n\n R Python\n\n\n\n# Export as CSV\nlscog_taz |&gt;\n  sf::st_drop_geometry() |&gt;\n  readr::write_csv(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_raw/lscog_taz_blank.csv\"\n    ),\n    append = FALSE\n  )\n\n\n\n\n# Export as CSV\nlscog_taz.to_csv(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_raw\" / \"lscog_taz_blank.csv\",\n    index=False\n)\n\n\n\n\nExport as Geodatabase layer\n\n R Python\n\n\n\n# Export as GDB\nlscog_taz |&gt;\n  sf::write_sf(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_gis/LSCOG_2020Base_SE.gdb\"\n    ),\n    layer = \"lscog_taz_blank\",\n    append = FALSE\n  )\n\n\n\n\n# Export as GDB\nlscog_taz.to_file(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_gis\" / \"LSCOG_2020Base_SE.gdb\",\n    layer='lscog_taz_blank',\n    driver='FileGDB'\n)"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#census-blocks-to-taz-conversion",
    "href": "posts/socioeconomic-demo/index.html#census-blocks-to-taz-conversion",
    "title": "Base Year SE Data Development",
    "section": "5.2 Census blocks to TAZ conversion",
    "text": "5.2 Census blocks to TAZ conversion\nThe block-to-TAZ conversion table establishes the critical linkage between fine-scale Census geography and the modeling zone system. This crosswalk file enables the aggregation of block-level socioeconomic data to TAZ boundaries while maintaining traceability to source geographies.\nExport as CSV flat file\n\n R Python\n\n\n\n# Export as CSV\nlscog_cb |&gt;\n  sf::st_join(lscog_taz) |&gt;\n  dplyr::select(GEOID20, ID, TAZ_ID) |&gt;\n  sf::st_drop_geometry() |&gt;\n  readr::write_csv(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_raw/lscog_block_taz.csv\"\n    ),\n    append = FALSE\n  )\n\n\n\n\n# Export as CSV\nlscog_cb.merge(lscog_taz[['ID', 'TAZ_ID']], left_on='GEOID20', right_on='ID')[['GEOID20', 'ID', 'TAZ_ID']].to_csv(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_raw\" / \"lscog_block_taz.csv\",\n    index=False\n)"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#decennial-census-data-at-census-block-level",
    "href": "posts/socioeconomic-demo/index.html#decennial-census-data-at-census-block-level",
    "title": "Base Year SE Data Development",
    "section": "5.3 Decennial census data at census block level",
    "text": "5.3 Decennial census data at census block level\nThe decennial census block export captures the foundational demographic counts used throughout the modeling process. This dataset provides the most reliable population and household totals at the finest geographic resolution, serving as the base for all subsequent data integration and validation steps.\nExport as CSV flat file\n\n R Python\n\n\n\n# Export as CSV\nlscog_dec |&gt;\n  sf::st_drop_geometry() |&gt;\n  readr::write_csv(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_raw/lscog_dec_block.csv\"\n    ),\n    append = FALSE\n  )\n\n\n\n\n# Export as CSV\nlscog_dec.to_csv(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_raw\" / \"lscog_dec_block.csv\",\n    index=False\n)\n\n\n\n\nExport as Geodatabase layer\n\n R Python\n\n\n\n# Export as GDB\nlscog_dec |&gt;\n  sf::write_sf(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_gis/LSCOG_2020Base_SE.gdb\"\n    ),\n    layer = \"lscog_dec_block\",\n    append = FALSE\n  )\n\n\n\n\n# Export as GDB\nlscog_dec.to_file(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_gis\" / \"LSCOG_2020Base_SE.gdb\",\n    layer='lscog_dec_block',\n    driver='FileGDB'\n)"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#acs-estimates-at-census-block-level",
    "href": "posts/socioeconomic-demo/index.html#acs-estimates-at-census-block-level",
    "title": "Base Year SE Data Development",
    "section": "5.4 ACS estimates at census block level",
    "text": "5.4 ACS estimates at census block level\nThe interpolated ACS data export delivers income distribution estimates at the census block level, providing the socioeconomic stratification necessary for trip generation modeling. This processed dataset represents the final product of the household-weighted interpolation methodology, ready for direct integration into the travel demand model framework.\nExport as CSV flat file\n\n R Python\n\n\n\n# Export as CSV\nlscog_acs_cb |&gt;\n  dplyr::select(GEOID, INC_14999, INC_49999, INC_50000) |&gt;\n  sf::st_drop_geometry() |&gt;\n  readr::write_csv(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_raw/lscog_acs_block.csv\"\n    ),\n    append = FALSE\n  )\n\n\n\n\n# Export as CSV\nlscog_acs_cb[['GEOID', 'INC_14999', 'INC_49999', 'INC_50000']].to_csv(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_raw\" / \"lscog_acs_block.csv\",\n    index=False\n)\n\n\n\n\nExport as Geodatabase layer\n\n R Python\n\n\n\n# Export as GDB\nlscog_acs_cb |&gt;\n  dplyr::select(GEOID, INC_14999, INC_49999, INC_50000) |&gt;\n  sf::write_sf(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_gis/LSCOG_2020Base_SE.gdb\"\n    ),\n    layer = \"lscog_acs_block\",\n    append = FALSE\n  )\n\n\n\n\n# Export as GDB\nlscog_acs_cb[['GEOID', 'INC_14999', 'INC_49999', 'INC_50000']].to_file(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_gis\" / \"LSCOG_2020Base_SE.gdb\",\n    layer='lscog_acs_block',\n    driver='FileGDB'\n)"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#lehd-data-at-census-block-level",
    "href": "posts/socioeconomic-demo/index.html#lehd-data-at-census-block-level",
    "title": "Base Year SE Data Development",
    "section": "5.5 LEHD data at census block level",
    "text": "5.5 LEHD data at census block level\nThe employment data export provides comprehensive workplace characteristics by industry sector at the census block level. This dataset captures the spatial distribution of employment opportunities across the study region, supporting both trip attraction modeling and economic impact analysis.\nExport as CSV flat file\n\n R Python\n\n\n\n# Export as CSV\nlscog_pop_hh_emp |&gt;\n  dplyr::select(GEOID, TOTAL_EMP:PUBLIC_ADM) |&gt;\n  sf::st_drop_geometry() |&gt;\n  readr::write_csv(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_raw/lscog_emp_block.csv\"\n    ),\n    append = FALSE\n  )\n\n\n\n\n# Export as CSV\nlscog_pop_hh_emp[['GEOID', 'TOTAL_EMP', 'AGR_FOR_FI', 'MINING', 'CONSTRUCTI',\n                   'MANUFACTUR', 'TRANSP_COM', 'WHOLESALE', 'RETAIL', 'FIRE',\n                   'SERVICES', 'PUBLIC_ADM']].to_csv(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_raw\" / \"lscog_emp_block.csv\",\n    index=False\n)\n\n\n\n\nExport as Geodatabase layer\n\n R Python\n\n\n\n# Export as GDB\nlscog_pop_hh_emp |&gt;\n  dplyr::select(GEOID, TOTAL_EMP:PUBLIC_ADM) |&gt;\n  sf::write_sf(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_gis/LSCOG_2020Base_SE.gdb\"\n    ),\n    layer = \"lscog_emp_block\",\n    append = FALSE\n  )\n\n\n\n\n# Export as GDB\nlscog_pop_hh_emp[['GEOID', 'TOTAL_EMP', 'AGR_FOR_FI', 'MINING', 'CONSTRUCTI',\n                   'MANUFACTUR', 'TRANSP_COM', 'WHOLESALE', 'RETAIL', 'FIRE',\n                   'SERVICES', 'PUBLIC_ADM']].to_file(\n        Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_gis\" / \"LSCOG_2020Base_SE.gdb\",\n        layer='lscog_emp_block',\n        driver='FileGDB'\n    )"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#public-schools-to-taz",
    "href": "posts/socioeconomic-demo/index.html#public-schools-to-taz",
    "title": "Base Year SE Data Development",
    "section": "5.6 Public schools to TAZ",
    "text": "5.6 Public schools to TAZ\nThe public school location export integrates educational facility data with the TAZ system, providing essential inputs for school-related trip modeling. Student and teacher counts by facility support the development of specialized trip generation rates for educational purposes.\nExport as CSV flat file\n\n R Python\n\n\n\n# Export as CSV\nlscog_pub_sch_enroll |&gt;\n  sf::st_join(lscog_taz |&gt; dplyr::select(ID, TAZ_ID)) |&gt;\n  sf::st_drop_geometry() |&gt;\n  readr::write_csv(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_raw/lscog_pubsch_loc.csv\"\n    ),\n    append = FALSE\n  )\n\n\n\n\n# Export as CSV\nlscog_pub_sch_enroll.sjoin(\n    lscog_taz[['ID', 'TAZ_ID']],\n    how='left'\n)[['INSTITUTION_ID', 'NAME', 'STATE', 'STUDENT_COUNT_PUB', 'TEACHER_COUNT_PUB', 'geometry']] \\\n    .to_csv(\n        Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_raw\" / \"lscog_pubsch_loc.csv\",\n        index=False\n    )\n\n\n\n\nExport as Geodatabase layer\n\n R Python\n\n\n\n# Export as GDB\nlscog_pub_sch_enroll |&gt;\n  sf::st_join(lscog_taz |&gt; dplyr::select(ID, TAZ_ID)) |&gt;\n  sf::write_sf(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_gis/LSCOG_2020Base_SE.gdb\"\n    ),\n    layer = \"lscog_pubsch_loc\",\n    append = FALSE\n  )\n\n\n\n\n# Export as GDB\nlscog_pub_sch_enroll.sjoin(\n    lscog_taz[['ID', 'TAZ_ID']],\n    how='left'\n).to_file(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_gis\" / \"LSCOG_2020Base_SE.gdb\",\n    layer='lscog_pubsch_loc',\n    driver='FileGDB'\n)"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#private-schools-to-taz",
    "href": "posts/socioeconomic-demo/index.html#private-schools-to-taz",
    "title": "Base Year SE Data Development",
    "section": "5.7 Private schools to TAZ",
    "text": "5.7 Private schools to TAZ\nThe private school dataset complements the public education data by capturing enrollment patterns in private educational institutions. This comprehensive coverage of educational facilities ensures that all school-related travel demand is properly represented in the regional model.\nExport as CSV flat file\n\n R Python\n\n\n\n# Export as CSV\nlscog_pvt_sch_enroll |&gt;\n  sf::st_join(lscog_taz |&gt; dplyr::select(ID, TAZ_ID)) |&gt;\n  sf::st_drop_geometry() |&gt;\n  readr::write_csv(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_raw/lscog_pvtsch_loc.csv\"\n    ),\n    append = FALSE\n  )\n\n\n\n\n# Export as CSV\nlscog_pvt_sch_enroll.sjoin(\n    lscog_taz[['ID', 'TAZ_ID']],\n    how='left'\n)[['INSTITUTION_ID', 'NAME', 'STATE', 'STUDENT_COUNT_PVT', 'TEACHER_COUNT_PVT', 'geometry']] \\\n    .to_csv(\n        Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_raw\" / \"lscog_pvtsch_loc.csv\",\n        index=False\n    )\n\n\n\n\nExport as Geodatabase layer\n\n R Python\n\n\n\n# Export as GDB\nlscog_pvt_sch_enroll |&gt;\n  sf::st_join(lscog_taz |&gt; dplyr::select(ID, TAZ_ID)) |&gt;\n  sf::write_sf(\n    file.path(\n      root,\n      \"Task 1 TDM Development/Base Year/_gis/LSCOG_2020Base_SE.gdb\"\n    ),\n    layer = \"lscog_pvtsch_loc\",\n    append = FALSE\n  )\n\n\n\n\n# Export as GDB\nlscog_pvt_sch_enroll.sjoin(\n    lscog_taz[['ID', 'TAZ_ID']],\n    how='left'\n).to_file(\n    Path(root) / \"Task 1 TDM Development\" / \"Base Year\" / \"_gis\" / \"LSCOG_2020Base_SE.gdb\",\n    layer='lscog_pvtsch_loc',\n    driver='FileGDB'\n)"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#population-households-and-employment",
    "href": "posts/socioeconomic-demo/index.html#population-households-and-employment",
    "title": "Base Year SE Data Development",
    "section": "6.1 Population, households, and employment",
    "text": "6.1 Population, households, and employment\nThe spatial join operation aggregates all demographic, housing, and employment variables from census blocks to their corresponding TAZs using centroid-based assignment. This process ensures that each blockâ€™s socioeconomic characteristics are properly allocated to the appropriate modeling zone while maintaining data integrity through comprehensive summation of all relevant variables.\n\n R Python\n\n\n\n# Aggregate population, households, and employment to TAZ\nlscog_taz_pop &lt;- lscog_taz |&gt;\n  sf::st_join(\n    lscog_pop_hh_emp |&gt; sf::st_centroid(of_largest_polygon = TRUE)\n  ) |&gt;\n  dplyr::group_by(\n    ID,\n    Area,\n    TAZ_ID,\n    COUNTY,\n    AREA_TYPE,\n    COUNTYID,\n    .drop = FALSE\n  ) |&gt;\n  dplyr::summarize(\n    .groups = \"drop\",\n    # Population and Household Size\n    TOTPOP = sum(TOTPOP, na.rm = TRUE),\n    GQPOP = sum(GQPOP, na.rm = TRUE),\n    HHPOP = sum(HHPOP, na.rm = TRUE),\n    HH = sum(HH, na.rm = TRUE),\n    HH_1 = sum(HH_1, na.rm = TRUE),\n    HH_2 = sum(HH_2, na.rm = TRUE),\n    HH_3 = sum(HH_3, na.rm = TRUE),\n    HH_4 = sum(HH_4, na.rm = TRUE),\n    DU = sum(DU, na.rm = TRUE),\n    # Household Income\n    INC_14999 = sum(INC_14999, na.rm = TRUE),\n    INC_49999 = sum(INC_49999, na.rm = TRUE),\n    INC_50000 = sum(INC_50000, na.rm = TRUE),\n    # Employment\n    TOTAL_EMP = sum(TOTAL_EMP, na.rm = TRUE),\n    AGR_FOR_FI = sum(AGR_FOR_FI, na.rm = TRUE),\n    MINING = sum(MINING, na.rm = TRUE),\n    CONSTRUCTI = sum(CONSTRUCTI, na.rm = TRUE),\n    MANUFACTUR = sum(MANUFACTUR, na.rm = TRUE),\n    TRANSP_COM = sum(TRANSP_COM, na.rm = TRUE),\n    WHOLESALE = sum(WHOLESALE, na.rm = TRUE),\n    RETAIL = sum(RETAIL, na.rm = TRUE),\n    FIRE = sum(FIRE, na.rm = TRUE),\n    SERVICES = sum(SERVICES, na.rm = TRUE),\n    PUBLIC_ADM = sum(PUBLIC_ADM, na.rm = TRUE)\n  )\n\nlscog_taz_pop\n\nSimple feature collection with 585 features and 29 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1691490 ymin: 331894 xmax: 2237471 ymax: 744679.9\nProjected CRS: NAD83(HARN) / South Carolina (ft)\n# A tibble: 585 Ã— 30\n   ID       Area TAZ_ID COUNTY AREA_TYPE COUNTYID TOTPOP GQPOP HHPOP    HH  HH_1\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 11050â€¦  37.7  11050â€¦ Barnwâ€¦ RURAL     45011       802     0   802   329    95\n 2 11050â€¦  21.5  11050â€¦ Barnwâ€¦ RURAL     45011       715     0   715   320   118\n 3 11050â€¦  24.4  11050â€¦ Barnwâ€¦ RURAL     45011      1406    79  1327   555   194\n 4 11050â€¦  17.4  11050â€¦ Barnwâ€¦ RURAL     45011       712     0   712   309   109\n 5 11050â€¦  12.6  11050â€¦ Barnwâ€¦ RURAL     45011       941     0   941   395    99\n 6 11050â€¦ 191.   11050â€¦ Barnwâ€¦ RURAL     45011         7     0     7     8     2\n 7 11050â€¦   9.69 11050â€¦ Barnwâ€¦ SUBURBAN  45011       185     0   185    69    15\n 8 11050â€¦  16.5  11050â€¦ Barnwâ€¦ RURAL     45011       680     0   680   274    87\n 9 11050â€¦  11.4  11050â€¦ Barnwâ€¦ SUBURBAN  45011       490     0   490   221    69\n10 11050â€¦   9.02 11050â€¦ Barnwâ€¦ SUBURBAN  45011       711     0   711   276    76\n# â„¹ 575 more rows\n# â„¹ 19 more variables: HH_2 &lt;dbl&gt;, HH_3 &lt;dbl&gt;, HH_4 &lt;dbl&gt;, DU &lt;dbl&gt;,\n#   INC_14999 &lt;dbl&gt;, INC_49999 &lt;dbl&gt;, INC_50000 &lt;dbl&gt;, TOTAL_EMP &lt;dbl&gt;,\n#   AGR_FOR_FI &lt;dbl&gt;, MINING &lt;dbl&gt;, CONSTRUCTI &lt;dbl&gt;, MANUFACTUR &lt;dbl&gt;,\n#   TRANSP_COM &lt;dbl&gt;, WHOLESALE &lt;dbl&gt;, RETAIL &lt;dbl&gt;, FIRE &lt;dbl&gt;,\n#   SERVICES &lt;dbl&gt;, PUBLIC_ADM &lt;dbl&gt;, geom &lt;MULTIPOLYGON [foot]&gt;\n\n\n\n\n\n# Aggregate population, households, and employment to TAZ\nlscog_taz_pop = (lscog_taz\n    .sjoin(\n        lscog_pop_hh_emp.assign(geometry=lscog_pop_hh_emp.geometry.centroid).to_crs(lscog_taz.crs),\n        how='left'\n    )\n    .groupby(['ID', 'Area', 'TAZ_ID', 'COUNTY', 'AREA_TYPE', 'COUNTYID'],\n             as_index=False, dropna=False)\n    .agg({\n        # Population and Household Size\n        'TOTPOP': lambda x: x.sum(skipna=True),\n        'GQPOP': lambda x: x.sum(skipna=True),\n        'HHPOP': lambda x: x.sum(skipna=True),\n        'HH': lambda x: x.sum(skipna=True),\n        'HH_1': lambda x: x.sum(skipna=True),\n        'HH_2': lambda x: x.sum(skipna=True),\n        'HH_3': lambda x: x.sum(skipna=True),\n        'HH_4': lambda x: x.sum(skipna=True),\n        'DU': lambda x: x.sum(skipna=True),\n        # Household Income\n        'INC_14999': lambda x: x.sum(skipna=True),\n        'INC_49999': lambda x: x.sum(skipna=True),\n        'INC_50000': lambda x: x.sum(skipna=True),\n        # Employment\n        'TOTAL_EMP': lambda x: x.sum(skipna=True),\n        'AGR_FOR_FI': lambda x: x.sum(skipna=True),\n        'MINING': lambda x: x.sum(skipna=True),\n        'CONSTRUCTI': lambda x: x.sum(skipna=True),\n        'MANUFACTUR': lambda x: x.sum(skipna=True),\n        'TRANSP_COM': lambda x: x.sum(skipna=True),\n        'WHOLESALE': lambda x: x.sum(skipna=True),\n        'RETAIL': lambda x: x.sum(skipna=True),\n        'FIRE': lambda x: x.sum(skipna=True),\n        'SERVICES': lambda x: x.sum(skipna=True),\n        'PUBLIC_ADM': lambda x: x.sum(skipna=True),\n        'geometry': 'first'\n    })\n)\n\nlscog_taz_pop = gpd.GeoDataFrame(lscog_taz_pop, crs=lscog_taz.crs)\nlscog_taz_pop\n\n           ID  ...                                           geometry\n0    11050058  ...  MULTIPOLYGON (((1940883.78 467711.359, 1940683...\n1    11050059  ...  MULTIPOLYGON (((1909744.245 514791.296, 190990...\n2    11050060  ...  MULTIPOLYGON (((1933978.652 551400.913, 193349...\n3    11050061  ...  MULTIPOLYGON (((1901544.254 537296.043, 190161...\n4    11050062  ...  MULTIPOLYGON (((1932677.078 512746.246, 193263...\n..        ...  ...                                                ...\n580   9050125  ...  MULTIPOLYGON (((1968427.616 539621.663, 196834...\n581   9050126  ...  MULTIPOLYGON (((1961524.044 543029.131, 196150...\n582   9050128  ...  MULTIPOLYGON (((1994408.254 547077.578, 199440...\n583   9050130  ...  MULTIPOLYGON (((2046194.08 478862.054, 2046134...\n584   9050132  ...  MULTIPOLYGON (((2012593.472 500179.47, 2013190...\n\n[585 rows x 30 columns]"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#school-and-college-enrollment",
    "href": "posts/socioeconomic-demo/index.html#school-and-college-enrollment",
    "title": "Base Year SE Data Development",
    "section": "6.2 School and college enrollment",
    "text": "6.2 School and college enrollment\nThe school enrollment combination merges public and private educational institution data into a unified dataset for comprehensive coverage of student populations.\n\n R Python\n\n\n\n# Combine school enrollment data\nlscog_sch_enroll &lt;- dplyr::bind_rows(\n  lscog_pub_sch_enroll,\n  lscog_pvt_sch_enroll\n) |&gt;\n  dplyr::mutate(\n    STUDENT_COUNT = dplyr::coalesce(STUDENT_COUNT_PUB, 0) +\n      dplyr::coalesce(STUDENT_COUNT_PVT, 0),\n    TEACHER_COUNT = dplyr::coalesce(TEACHER_COUNT_PUB, 0) +\n      dplyr::coalesce(TEACHER_COUNT_PVT, 0)\n  )\n\nlscog_sch_enroll\n\nSimple feature collection with 122 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1700952 ymin: 406810.6 xmax: 2203406 ymax: 724699.4\nProjected CRS: NAD83(HARN) / South Carolina (ft)\nFirst 10 features:\n   INSTITUTION_ID                          NAME STATE STUDENT_COUNT_PUB\n1    450108001163           Barnwell Elementary    SC               462\n2    450075000064        Allendale Fairfax High    SC               283\n3    450075001184          Allendale Elementary    SC               245\n4    450075001415      Allendale-Fairfax Middle    SC               268\n5    450093000119         Bamberg-Ehrhardt High    SC               381\n6    450093000120       Bamberg-Ehrhardt Middle    SC               188\n7    450096000122             Denmark Olar High    SC               162\n8    450096000123           Denmark-Olar Middle    SC               149\n9    450096001426       Denmark-Olar Elementary    SC               353\n10   450098000127 Barnwell County Career Center    SC                 0\n   TEACHER_COUNT_PUB STUDENT_COUNT_PVT TEACHER_COUNT_PVT\n1               32.0                NA                NA\n2               28.9                NA                NA\n3               18.0                NA                NA\n4               18.0                NA                NA\n5               28.5                NA                NA\n6               15.0                NA                NA\n7               20.0                NA                NA\n8               10.0                NA                NA\n9               25.0                NA                NA\n10              12.0                NA                NA\n                   geometry STUDENT_COUNT TEACHER_COUNT\n1  POINT (1891531 517378.5)           462          32.0\n2  POINT (1913416 420526.6)           283          28.9\n3  POINT (1916344 418212.2)           245          18.0\n4  POINT (1913416 420526.6)           268          18.0\n5  POINT (1991502 535259.1)           381          28.5\n6  POINT (1990622 534442.6)           188          15.0\n7    POINT (1962410 543779)           162          20.0\n8  POINT (1956236 534420.3)           149          10.0\n9  POINT (1960237 537023.1)           353          25.0\n10   POINT (1894891 540093)             0          12.0\n\n\n\n\n\n# Combine school enrollment data\nlscog_sch_enroll = pd.concat([\n    lscog_pub_sch_enroll.assign(\n        STUDENT_COUNT=lscog_pub_sch_enroll['STUDENT_COUNT_PUB'],\n        TEACHER_COUNT=lscog_pub_sch_enroll['TEACHER_COUNT_PUB']\n    ),\n    lscog_pvt_sch_enroll.assign(\n        STUDENT_COUNT=lscog_pvt_sch_enroll['STUDENT_COUNT_PVT'],\n        TEACHER_COUNT=lscog_pvt_sch_enroll['TEACHER_COUNT_PVT']\n    )\n])\n\nlscog_sch_enroll\n\n      INSTITUTION_ID  ... TEACHER_COUNT_PVT\n0       450108001163  ...               NaN\n1       450075000064  ...               NaN\n2       450075001184  ...               NaN\n3       450075001415  ...               NaN\n4       450093000119  ...               NaN\n...              ...  ...               ...\n17663       K9305640  ...               3.0\n17672       A9703170  ...               4.8\n17676       01262826  ...              23.0\n17722       01932407  ...               6.0\n17733       A9903957  ...               1.0\n\n[122 rows x 10 columns]\n\n\n\n\n\nThe subsequent TAZ aggregation counts total student enrollment within each zone, providing essential data for modeling education-related trip patterns and supporting specialized trip generation rates for school-based travel.\n\n R Python\n\n\n\n# count the number of school enrollment within each TAZ\nlscog_taz_enroll &lt;- lscog_taz |&gt;\n  sf::st_join(lscog_sch_enroll) |&gt;\n  dplyr::group_by(\n    ID,\n    Area,\n    TAZ_ID,\n    COUNTY,\n    AREA_TYPE,\n    COUNTYID,\n    .drop = FALSE\n  ) |&gt;\n  dplyr::summarize(\n    .groups = \"drop\",\n    STUDENT_COUNT = sum(STUDENT_COUNT, na.rm = TRUE)\n  )\n\nlscog_taz_enroll\n\nSimple feature collection with 585 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1691490 ymin: 331894 xmax: 2237471 ymax: 744679.9\nProjected CRS: NAD83(HARN) / South Carolina (ft)\n# A tibble: 585 Ã— 8\n   ID         Area TAZ_ID   COUNTY      AREA_TYPE COUNTYID STUDENT_COUNT\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;\n 1 11050058  37.7  11050058 Barnwell SC RURAL     45011                0\n 2 11050059  21.5  11050059 Barnwell SC RURAL     45011                0\n 3 11050060  24.4  11050060 Barnwell SC RURAL     45011              598\n 4 11050061  17.4  11050061 Barnwell SC RURAL     45011                3\n 5 11050062  12.6  11050062 Barnwell SC RURAL     45011                2\n 6 11050072 191.   11050072 Barnwell SC RURAL     45011                0\n 7 11050073   9.69 11050073 Barnwell SC SUBURBAN  45011                0\n 8 11050074  16.5  11050074 Barnwell SC RURAL     45011                0\n 9 11050075  11.4  11050075 Barnwell SC SUBURBAN  45011                0\n10 11050076   9.02 11050076 Barnwell SC SUBURBAN  45011                0\n# â„¹ 575 more rows\n# â„¹ 1 more variable: geom &lt;MULTIPOLYGON [foot]&gt;\n\n\n\n\n\n# count the number of school enrollment within each TAZ\nlscog_taz_enroll = lscog_taz.sjoin(lscog_sch_enroll, how='left') \\\n    .groupby(['ID', 'Area', 'TAZ_ID', 'COUNTY', 'AREA_TYPE', 'COUNTYID'], as_index=False) \\\n    .agg({'STUDENT_COUNT': 'sum'})\n\nlscog_taz_enroll\n\n           ID       Area    TAZ_ID  ... AREA_TYPE COUNTYID STUDENT_COUNT\n0    11050058  37.707611  11050058  ...     RURAL    45011           0.0\n1    11050059  21.450575  11050059  ...     RURAL    45011           0.0\n2    11050060  24.411972  11050060  ...     RURAL    45011         598.0\n3    11050061  17.351381  11050061  ...     RURAL    45011           3.0\n4    11050062  12.587147  11050062  ...     RURAL    45011           2.0\n..        ...        ...       ...  ...       ...      ...           ...\n580   9050125  12.789308   9050125  ...     RURAL    45009         162.0\n581   9050126   0.633810   9050126  ...  SUBURBAN    45009           0.0\n582   9050128  12.890918   9050128  ...     RURAL    45009         693.0\n583   9050130  13.308991   9050130  ...     RURAL    45009           0.0\n584   9050132  39.194309   9050132  ...     RURAL    45009           0.0\n\n[585 rows x 7 columns]"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#household-size-total",
    "href": "posts/socioeconomic-demo/index.html#household-size-total",
    "title": "Base Year SE Data Development",
    "section": "8.1 Household size total",
    "text": "8.1 Household size total\nThis validation confirms that the total household count matches the sum of all household size categories for each TAZ. Any discrepancies indicate potential issues in the household size distribution that require investigation and correction before model implementation.\n\n R Python\n\n\n\n# check the sum of household by household size\nlscog_se_base |&gt;\n  dplyr::filter(HH != (HH_1 + HH_2 + HH_3 + HH_4)) |&gt;\n  nrow()\n\n[1] 0\n\n\n\n\n\n# check the sum of household by household size\nlscog_se_base[\n    lscog_se_base['HH'] != (lscog_se_base['HH_1'] + lscog_se_base['HH_2'] +\n                            lscog_se_base['HH_3'] + lscog_se_base['HH_4'])\n].shape[0]\n\n0"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#household-income-total",
    "href": "posts/socioeconomic-demo/index.html#household-income-total",
    "title": "Base Year SE Data Development",
    "section": "8.2 Household income total",
    "text": "8.2 Household income total\nThe income category validation verifies that household totals equal the sum of all three income brackets across all zones. This check ensures the integrity of the income distribution data following the proportional allocation methodology applied during the ACS interpolation process.\n\n R Python\n\n\n\n# check the sum of household by income level\nlscog_se_base |&gt;\n  dplyr::filter(HH != (INC_14999 + INC_49999 + INC_50000)) |&gt;\n  nrow()\n\n[1] 0\n\n\n\n\n\n# check the sum of household by income level\nlscog_se_base[\n    lscog_se_base['HH'] != (lscog_se_base['INC_14999'] + lscog_se_base['INC_49999'] +\n                            lscog_se_base['INC_50000'])\n].shape[0]\n\n0"
  },
  {
    "objectID": "posts/socioeconomic-demo/index.html#employment-categories",
    "href": "posts/socioeconomic-demo/index.html#employment-categories",
    "title": "Base Year SE Data Development",
    "section": "8.3 Employment categories",
    "text": "8.3 Employment categories\nThe employment validation confirms that total employment equals the sum of all industry sector categories for each TAZ. This comprehensive check validates the LEHD data integration and ensures that no employment is lost or double-counted during the sectoral disaggregation process.RetryClaude can make mistakes. Please double-check responses.\n\n R Python\n\n\n\n# check the sum of employment by categories\nlscog_se_base |&gt;\n  dplyr::filter(\n    TOTAL_EMP !=\n      (AGR_FOR_FI +\n        MINING +\n        CONSTRUCTI +\n        MANUFACTUR +\n        TRANSP_COM +\n        WHOLESALE +\n        RETAIL +\n        FIRE +\n        SERVICES +\n        PUBLIC_ADM)\n  ) |&gt;\n  nrow()\n\n[1] 0\n\n\n\n\n\n# check the sum of employment by categories\nlscog_se_base[\n    lscog_se_base['TOTAL_EMP'] != (\n        lscog_se_base['AGR_FOR_FI'] +\n        lscog_se_base['MINING'] +\n        lscog_se_base['CONSTRUCTI'] +\n        lscog_se_base['MANUFACTUR'] +\n        lscog_se_base['TRANSP_COM'] +\n        lscog_se_base['WHOLESALE'] +\n        lscog_se_base['RETAIL'] +\n        lscog_se_base['FIRE'] +\n        lscog_se_base['SERVICES'] +\n        lscog_se_base['PUBLIC_ADM']\n    )\n].shape[0]\n\n0"
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html",
    "href": "posts/population-pyramid-explorer/index.html",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "",
    "text": "Population demographics are fundamental to transportation planning and urban analysis. Understanding age distributions helps inform infrastructure decisions, transit planning, and resource allocation. In this post, this blog will walk through building an interactive Shiny application that combines population pyramids with spatial visualization using {mapgl} package.\nThe app allows users to explore county-level demographic data across all US states, displaying both the geographic context through an interactive map and detailed age-sex distributions through population pyramids."
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#introduction",
    "href": "posts/population-pyramid-explorer/index.html#introduction",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "",
    "text": "Population demographics are fundamental to transportation planning and urban analysis. Understanding age distributions helps inform infrastructure decisions, transit planning, and resource allocation. In this post, this blog will walk through building an interactive Shiny application that combines population pyramids with spatial visualization using {mapgl} package.\nThe app allows users to explore county-level demographic data across all US states, displaying both the geographic context through an interactive map and detailed age-sex distributions through population pyramids."
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#application-overview",
    "href": "posts/population-pyramid-explorer/index.html#application-overview",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "2 Application Overview",
    "text": "2 Application Overview\nThe Population Pyramid Explorer features:\n\nInteractive state selection with smooth map transitions\nCounty-level demographic visualization using Census tract data for context\nDynamic population pyramids showing age-sex distributions\nResponsive design using MapLibreâ€™s story map format\nReal-time data integration from the US Census Bureau"
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#repository-and-code-access",
    "href": "posts/population-pyramid-explorer/index.html#repository-and-code-access",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "3 Repository and Code Access",
    "text": "3 Repository and Code Access\nThe complete source code for this application is available on GitHub: Population Pyramid Explorer Repository\nThe repository includes additional documentation, deployment instructions, and example usage scenarios."
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#key-dependencies-and-setup",
    "href": "posts/population-pyramid-explorer/index.html#key-dependencies-and-setup",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "4 Key Dependencies and Setup",
    "text": "4 Key Dependencies and Setup\nThe application leverages several powerful R packages for data processing, spatial analysis, and visualization:\n# Custom function to load packages, installing them first if not already installed\nload_pkg &lt;- function(pkg) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE)\n  } else {\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# Data manipulation and visualization\nload_pkg(\"dplyr\")\nload_pkg(\"tidyr\") \nload_pkg(\"ggplot2\")\nload_pkg(\"scales\")\n\n# Spatial data and mapping\nload_pkg(\"sf\")\nload_pkg(\"mapgl\")\nload_pkg(\"tigris\")\n\n# Census data integration\nload_pkg(\"tidycensus\")\n\n# UI framework\nload_pkg(\"shiny\")\nThe load_pkg() function ensures packages are installed and loaded, making deployment more robust across different environments."
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#application-architecture",
    "href": "posts/population-pyramid-explorer/index.html#application-architecture",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "5 Application Architecture",
    "text": "5 Application Architecture\n\nUser Interface Design\nThe UI implements {mapgl}â€™s story map concept, creating a scrollable narrative experience:\nmapgl::story_maplibre(\n  map_id = \"map\",\n  font_family = \"Poppins\",\n  sections = list(\n    \"intro\" = mapgl::story_section(...),\n    \"state_detail\" = mapgl::story_section(...)\n  )\n)\nThis approach provides intuitive navigation between the overview (state selection) and detailed analysis (county demographics).\n\n\nReactive Data Architecture\nThe server logic employs several reactive components that efficiently manage data flow:\nState Geometry: Loads and transforms state boundaries using the Web Mercator projection (EPSG:3857) for optimal web mapping performance.\nCensus Tract Data: Dynamically fetches median age data at the tract level, providing geographic context for county selection. The data pipeline includes coordinate reference system transformation and popup content preparation.\nPopulation Estimates: Retrieves detailed age-sex breakdowns from the Census Bureauâ€™s Population Estimates API, specifically formatted for pyramid visualization."
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#data-processing-pipeline",
    "href": "posts/population-pyramid-explorer/index.html#data-processing-pipeline",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "6 Data Processing Pipeline",
    "text": "6 Data Processing Pipeline\n\nGeographic Data Preparation\nThe application processes spatial data through several key transformations:\nstates_sf &lt;- tigris::states(cb = TRUE) |&gt;\n  sf::st_transform(crs = 3857) |&gt;\n  sf::st_zm(drop = TRUE, what = \"ZM\")\nUsing cartographic boundary files (cb = TRUE) provides simplified geometries that render faster while maintaining visual accuracy at the national scale.\n\n\nCensus Data Integration\nThe demographic data pipeline combines two Census Bureau APIs:\n\nAmerican Community Survey (ACS) for tract-level median age visualization\nPopulation Estimates Program for detailed county-level age-sex distributions\n\nThe tract-level data provides spatial context with median age choropleth mapping, while county estimates power the detailed population pyramids.\n\n\nData Transformation for Visualization\nPopulation pyramid data requires specific transformations:\ndplyr::mutate(\n  value = ifelse(SEX == \"Male\", -value, value),\n  age_min = ifelse(\n    stringr::str_detect(AGEGROUP, \"under\"), 0,\n    as.numeric(stringr::str_extract(AGEGROUP, \"\\\\d+\"))\n  )\n)\nMale populations receive negative values for the traditional pyramid layout, and age groups are parsed to enable proper ordering."
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#interactive-mapping-features",
    "href": "posts/population-pyramid-explorer/index.html#interactive-mapping-features",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "7 Interactive Mapping Features",
    "text": "7 Interactive Mapping Features\n\nMap Initialization and Styling\nThe base map uses Cartoâ€™s Voyager style for clean, readable cartography:\nmapgl::maplibre(\n  mapgl::carto_style(\"voyager\"),\n  center = c(-98.5, 39.5),\n  zoom = 3,\n  scrollZoom = FALSE\n) |&gt;\nmapgl::set_projection(projection = \"globe\")\nThe globe projection adds visual appeal while the disabled scroll zoom prevents navigation conflicts with the story map interface.\n\n\nDynamic Layer Management\nMap layers update responsively based on user selections:\n\nState boundaries provide national context with selective highlighting\nCensus tract fills show median age patterns using a continuous color scale\nInteractive popups display detailed demographic information\n\nThe color scheme uses ColorBrewerâ€™s spectral palette, providing intuitive age visualization from young (red) to old (purple).\n\n\nSmooth Transitions\nSection-based navigation triggers smooth map animations:\nmapgl::fly_to(\n  center = c(-98.5, 39.5),\n  zoom = 3,\n  bearing = 0,\n  pitch = 0,\n  duration = 1500\n)\nThese transitions maintain user orientation while providing engaging visual feedback."
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#population-pyramid-visualization",
    "href": "posts/population-pyramid-explorer/index.html#population-pyramid-visualization",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "8 Population Pyramid Visualization",
    "text": "8 Population Pyramid Visualization\n\nChart Design Philosophy\nThe population pyramid uses a horizontal bar chart approach with careful attention to readability:\n\nDual-color scheme: Navy for males, dark red for females\nAbsolute value labeling: Clear population counts despite negative male values\nMinimal grid lines: Focus attention on data patterns\n\n\n\nAge Group Processing\nAge categories from the Census API require standardization for consistent display:\nscale_y_discrete(labels = ~ stringr::str_remove_all(.x, \"Age\\\\s|\\\\syears\"))\nThis removes redundant text while preserving essential age range information.\n\n\nDynamic Scaling\nThe x-axis automatically adjusts to population magnitudes using thousands formatting, ensuring readability across counties of varying sizes."
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#technical-implementation-notes",
    "href": "posts/population-pyramid-explorer/index.html#technical-implementation-notes",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "9 Technical Implementation Notes",
    "text": "9 Technical Implementation Notes\n\nPerformance Optimization\nSeveral design decisions optimize application performance:\n\n{tigris} caching reduces repeated API calls for geographic data\nselectize server-side processing handles large county lists efficiently\n\nReactive dependency management prevents unnecessary data updates\n\n\n\nError Handling and Validation\nThe application includes robust input validation:\n\nshiny::req() ensures required inputs before processing\n{tidycensus} validation functions prevent invalid state/county combinations\nGraceful handling of missing or incomplete demographic data"
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#future-enhancements",
    "href": "posts/population-pyramid-explorer/index.html#future-enhancements",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "10 Future Enhancements",
    "text": "10 Future Enhancements\nPotential extensions to this application include:\n\nMulti-county comparison capabilities\nTime series analysis showing demographic changes over years\n\nAdditional demographic variables (income, education, housing)\nExport functionality for charts and data\nIntegration with transportation metrics for comprehensive planning analysis"
  },
  {
    "objectID": "posts/population-pyramid-explorer/index.html#conclusion",
    "href": "posts/population-pyramid-explorer/index.html#conclusion",
    "title": "Building an Interactive Population Pyramid Explorer",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nThis Population Pyramid Explorer demonstrates the power of combining modern web mapping with demographic visualization. By leveraging Râ€™s robust ecosystem for spatial data and statistical graphics, we can create compelling tools for understanding population patterns that inform transportation planning and policy decisions.\nThe modular architecture makes it straightforward to extend the application with additional demographic variables or geographic scales, while the story map format provides an intuitive user experience that encourages exploration and discovery.\nWhether youâ€™re a transportation planner analyzing ridership demographics, an urban researcher studying neighborhood change, or a policy analyst examining service delivery patterns, this application framework provides a foundation for demographic-geographic analysis that can be adapted to various planning contexts.\n\nWant to contribute or suggest improvements? Visit the project repository at: https://github.com/ar-puuk/population-pyramid-explorer"
  },
  {
    "objectID": "posts/nepal-population-density/index.html",
    "href": "posts/nepal-population-density/index.html",
    "title": "3D Population Density Mapping of Nepal",
    "section": "",
    "text": "This analysis demonstrates advanced geospatial visualization techniques using Râ€™s rayshader package to create a stunning 3D population density map of Nepal. The project showcases the integration of high-resolution population data with topographic visualization methods, resulting in an interactive and informative representation of demographic patterns across Nepalâ€™s diverse terrain.\nKey Technologies: R, rayshader, sf, stars, magickData Source: Kontur Population Dataset (400m H3 hexagon resolution)Projection: ESRI:102306 (Nepal Nagarkot TM)"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#project-overview",
    "href": "posts/nepal-population-density/index.html#project-overview",
    "title": "3D Population Density Mapping of Nepal",
    "section": "",
    "text": "This analysis demonstrates advanced geospatial visualization techniques using Râ€™s rayshader package to create a stunning 3D population density map of Nepal. The project showcases the integration of high-resolution population data with topographic visualization methods, resulting in an interactive and informative representation of demographic patterns across Nepalâ€™s diverse terrain.\nKey Technologies: R, rayshader, sf, stars, magickData Source: Kontur Population Dataset (400m H3 hexagon resolution)Projection: ESRI:102306 (Nepal Nagarkot TM)"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#environment-setup",
    "href": "posts/nepal-population-density/index.html#environment-setup",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n2 Environment Setup",
    "text": "2 Environment Setup\nPackage Installation and Loading\n\nPackage Installation (Run Once)# Core packages for geospatial analysis\ninstall.packages(c(\"tidyverse\", \"sf\", \"tmap\", \"ggplot2\", \"mapview\", \"stars\"))\n\n# Visualization packages\ninstall.packages(c(\"rayshader\", \"MetBrewer\", \"colorspace\", \"rayrender\", \"magick\"))\n\n# Font handling\ninstall.packages(\"extrafont\")\n\n\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(ggplot2)\nlibrary(mapview)\nlibrary(stars)\nlibrary(rayshader)\nlibrary(MetBrewer)\nlibrary(colorspace)\nlibrary(rayrender)\nlibrary(magick)\nlibrary(extrafont)\n\n\n# Configure 3D rendering\noptions(rgl.useNULL = FALSE)\n\nproject_crs &lt;- \"ESRI:102306\""
  },
  {
    "objectID": "posts/nepal-population-density/index.html#data-acquisition-and-loading",
    "href": "posts/nepal-population-density/index.html#data-acquisition-and-loading",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n3 Data Acquisition and Loading",
    "text": "3 Data Acquisition and Loading\nPopulation Hexagon Data\nThe analysis utilizes high-resolution population data from Kontur, provided in H3 hexagonal grid format at 400-meter resolution. This dataset offers superior granularity compared to traditional administrative boundary-based population estimates.\n\n# Load population data (H3 hexagons at 400m resolution)\nnep_hex &lt;- sf::st_read(\n  paste0(\n    \"/vsizip/\",\n    \"data/kontur_population_NP_20220630.gpkg.gz/kontur_population_NP_20220630.gpkg\"\n  )\n) |&gt;\n  sf::st_transform(project_crs)\n\n# Load administrative boundaries\nnep_admin &lt;- sf::st_read(\n  paste0(\n    \"/vsizip/\",\n    \"data/kontur_boundaries_NP_20220407.gpkg.gz/kontur_boundaries_NP_20220407.gpkg\"\n  )\n) |&gt;\n  sf::st_transform(project_crs)\n\n# Create unified Nepal boundary\nnepal_boundary &lt;- nep_admin |&gt;\n  sf::st_geometry() |&gt;\n  sf::st_union() |&gt;\n  sf::st_sf() |&gt;\n  sf::st_make_valid()\n\n\n\n\n\n\n\nNote\n\n\n\nData Sources:\n- Population Data: Kontur Population Dataset\n- Administrative Boundaries: Kontur Boundaries Dataset\n- Coordinate Reference System: EPSG:6207 for accurate representation of Nepalâ€™s geography"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#data-processing-and-visualization-setup",
    "href": "posts/nepal-population-density/index.html#data-processing-and-visualization-setup",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n4 Data Processing and Visualization Setup",
    "text": "4 Data Processing and Visualization Setup\nInitial Data Exploration\n\n# Preliminary visualization to verify data quality\nggplot(nep_hex) + \n  geom_sf(aes(fill = population), color = \"gray66\", linewidth = 0) +\n  geom_sf(data = nepal_boundary, fill = NA, color = \"black\", \n          linetype = \"dashed\", linewidth = 1) +\n  theme_minimal() +\n  labs(title = \"Nepal Population Distribution\",\n       fill = \"Population\")\n\nRaster Conversion and Matrix Preparation\nThe conversion from vector hexagon data to raster format is essential for rayshaderâ€™s 3D rendering capabilities. This process involves calculating optimal dimensions while preserving the geographic aspect ratio.\n\n# Calculate bounding box and aspect ratio\nbbox &lt;- st_bbox(nepal_boundary)\n\n# Define corner points for aspect ratio calculation\nbottom_left &lt;- st_point(c(bbox[[\"xmin\"]], bbox[[\"ymin\"]])) |&gt;\n  st_sfc(crs = 6207)\nbottom_right &lt;- st_point(c(bbox[[\"xmax\"]], bbox[[\"ymin\"]])) |&gt;\n  st_sfc(crs = 6207)\ntop_left &lt;- st_point(c(bbox[[\"xmin\"]], bbox[[\"ymax\"]])) |&gt;\n  st_sfc(crs = 6207)\n\n# Calculate dimensions\nwidth &lt;- st_distance(bottom_left, bottom_right)\nheight &lt;- st_distance(bottom_left, top_left)\n\n# Determine aspect ratios\nif(width &gt; height) {\n  w_ratio &lt;- 1\n  h_ratio &lt;- height / width\n} else {\n  h_ratio &lt;- 1\n  w_ratio &lt;- width / height\n}\n\n# Convert to raster with optimal resolution\nsize &lt;- 1000 * 2.5\n\npop_raster &lt;- st_rasterize(nep_hex,\n                          nx = floor(size * w_ratio),\n                          ny = floor(size * h_ratio))\n\n# Create matrix for 3D rendering\npop_matrix &lt;- matrix(pop_raster$population,\n                    nrow = floor(size * w_ratio),\n                    ncol = floor(size * h_ratio))"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#color-scheme-development",
    "href": "posts/nepal-population-density/index.html#color-scheme-development",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n5 Color Scheme Development",
    "text": "5 Color Scheme Development\nMetBrewer Color Palette Selection\nThe visualization employs the â€œOKeeffe2â€ palette from MetBrewer, chosen for its warm earth tones that effectively represent population density while maintaining visual appeal and accessibility.\n\n# Generate color palette\ncolor &lt;- met.brewer(\"OKeeffe2\")\n\n# Create texture gradient with bias toward higher values\ntexture &lt;- grDevices::colorRampPalette(color, bias = 3)(256)\n\n# Preview color scheme\nswatchplot(texture)"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#d-visualization-rendering",
    "href": "posts/nepal-population-density/index.html#d-visualization-rendering",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n6 3D Visualization Rendering",
    "text": "6 3D Visualization Rendering\nInteractive 3D Display\n\n# Close any existing RGL windows\nrgl::rgl.close()\n\n# Generate 3D visualization\npop_matrix |&gt;\n  height_shade(texture = texture) |&gt;\n  plot_3d(heightmap = pop_matrix,\n          zscale = 250 / 2.5,\n          solid = FALSE,\n          shadowdepth = 0)\n\n# Set optimal camera position\nrender_camera(theta = 0,\n              phi = 30,\n              zoom = 0.4,\n              fov = 90)\n\nHigh-Quality Rendering\nThe final rendering process utilizes rayshaderâ€™s advanced lighting system to create publication-ready visualizations with professional-grade quality and resolution.\n\noutfile &lt;- \"Plots/final_plot.png\"\n\n# Render high-quality image with timing\n{\n  start_time &lt;- Sys.time()\n  cat(crayon::cyan(\"Rendering started:\", start_time), \"\\n\")\n  \n  # Ensure output directory exists\n  if(!file.exists(outfile)) {\n    png::writePNG(matrix(1), target = outfile)\n  }\n\n  render_highquality(\n    filename = outfile,\n    interactive = FALSE,\n    lightdirection = 225,\n    lightaltitude = c(20, 80),\n    lightcolor = c(color[2], \"white\"),\n    lightintensity = c(600, 100),\n    width = 1980,\n    height = 1080,\n    samples = 300\n  )\n\n  end_time &lt;- Sys.time()\n  diff &lt;- end_time - start_time\n  cat(crayon::cyan(\"Rendering completed in:\", diff), \"\\n\")\n}"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#post-processing-and-annotation",
    "href": "posts/nepal-population-density/index.html#post-processing-and-annotation",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n7 Post-Processing and Annotation",
    "text": "7 Post-Processing and Annotation\nGeographic Labeling\nThe final step enhances the visualization with strategic city labels and professional annotation using the magick package for image manipulation.\n\n# Load rendered image\npop_raster &lt;- image_read(\"Plots/final_plot.png\")\n\n# Define text styling\ntext_color &lt;- darken(color[7], .5)\n\n# Add comprehensive annotations\npop_raster |&gt;\n  image_annotate(\"NEPAL\",\n                 gravity = \"northeast\",\n                 location = \"+50+50\",\n                 color = text_color,\n                 size = 150,\n                 font = \"Ananda Namaste\",\n                 weight = 700) |&gt;\n  image_annotate(\"POPULATION DENSITY MAP\",\n                 gravity = \"northeast\",\n                 location = \"+50+200\",\n                 color = text_color,\n                 size = 36.5,\n                 font = \"FuturaBT-Medium\",\n                 weight = 500) |&gt;\n  # Major urban centers\n  image_annotate(\"Kathmandu\",\n                 gravity = \"center\",\n                 location = \"+250-50\",\n                 color = alpha(text_color, .8),\n                 size = 30,\n                 font = \"FuturaBT-Medium\") |&gt;\n  image_annotate(\"Pokhara\",\n                 gravity = \"center\",\n                 location = \"-30+35\",\n                 color = alpha(text_color, .8),\n                 size = 25,\n                 font = \"FuturaBT-Medium\") |&gt;\n  image_annotate(\"Biratnagar\",\n                 gravity = \"east\",\n                 location = \"+125+100\",\n                 color = alpha(text_color, .8),\n                 size = 28,\n                 font = \"FuturaBT-Medium\") |&gt;\n  image_annotate(\"Birgunj\",\n                 gravity = \"center\",\n                 location = \"+130+100\",\n                 color = alpha(text_color, .8),\n                 size = 25,\n                 font = \"FuturaBT-Medium\") |&gt;\n  # Regional centers\n  image_annotate(\"Nepalgunj\",\n                 gravity = \"center\",\n                 location = \"-450+0\",\n                 color = alpha(text_color, .8),\n                 size = 24,\n                 font = \"FuturaBT-Medium\") |&gt;\n  image_annotate(\"Janakpur\",\n                 gravity = \"east\",\n                 location = \"+500+140\",\n                 color = alpha(text_color, .8),\n                 size = 22,\n                 font = \"FuturaBT-Medium\") |&gt;\n  # Additional urban areas\n  image_annotate(\"Itahari\\nSurkhet\\nTikapur\\nDhangadhi\\nBharatpur\\nButwal\\nGhorahi\\nTulsipur\",\n                 gravity = \"southwest\",\n                 location = \"+20+100\",\n                 color = alpha(text_color, .6),\n                 size = 16,\n                 font = \"FuturaBT-Medium\") |&gt;\n  # Professional attribution\n  image_annotate(\"Visualization by: [Your Name] with Rayshader(@tylermorganwall) | Data: Kontur Population (Released 2022-06-30)\",\n                 gravity = \"southwest\",\n                 location = \"+20+20\",\n                 color = alpha(text_color, .6),\n                 font = \"FuturaBT-Medium\",\n                 size = 20) |&gt;\n  image_write(\"Plots/final_plot_edited.png\", format = \"png\", quality = 100)"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#technical-methodology",
    "href": "posts/nepal-population-density/index.html#technical-methodology",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n8 Technical Methodology",
    "text": "8 Technical Methodology\nSpatial Data Processing Pipeline\n\n\nData Transformation: Vector hexagon data converted to standardized coordinate reference system (Nepal Nagarkot TM - ESRI:102306)\n\nRasterization: Optimal grid resolution calculated to balance detail with computational efficiency\n\nMatrix Conversion: Raster data transformed into height matrix for 3D rendering\n\nTexture Mapping: Population values mapped to color gradients using perceptually uniform color spaces\nRendering Specifications\n\n\nResolution: 1980Ã—1080 pixels for high-definition output\n\nSampling: 300 samples for anti-aliasing and smooth gradients\n\nLighting: Dual-source lighting system (directional + ambient)\n\nZ-Scale: Optimized vertical exaggeration (250/2.5) for clear population peaks"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#results-and-applications",
    "href": "posts/nepal-population-density/index.html#results-and-applications",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n9 Results and Applications",
    "text": "9 Results and Applications\nThis methodology produces publication-quality 3D population density visualizations suitable for:\n\n\nUrban Planning: Identifying population clusters and growth patterns\n\nInfrastructure Development: Targeting high-density areas for transportation networks\n\nEmergency Response: Understanding population distribution for resource allocation\n\nAcademic Research: Visual communication of demographic data"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#environment-management",
    "href": "posts/nepal-population-density/index.html#environment-management",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n10 Environment Management",
    "text": "10 Environment Management\n\n# Save complete workspace for future analysis\nsave.image(file.path(path, \"nep_pop.RData\"))"
  },
  {
    "objectID": "posts/nepal-population-density/index.html#conclusion",
    "href": "posts/nepal-population-density/index.html#conclusion",
    "title": "3D Population Density Mapping of Nepal",
    "section": "\n11 Conclusion",
    "text": "11 Conclusion\nThis workflow demonstrates the power of combining modern R packages for geospatial analysis with advanced 3D visualization techniques. The resulting maps provide intuitive understanding of Nepalâ€™s population distribution patterns, highlighting the concentration of people in the Terai region and major urban centers while showcasing the relatively sparse population in mountainous areas.\nThe methodology is transferable to other geographic contexts and can be adapted for various demographic or infrastructure planning applications in transportation and urban development projects.\n\nWant to contribute or suggest improvements? Visit the project repository at: https://github.com/ar-puuk/Population-Density-Maps"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi! I am Pukar Bhandari.",
    "section": "",
    "text": "I am an aspiring Transportation Planner with a Masterâ€™s in City & Metropolitan Planning and a background in architecture, specializing in data-driven, multimodal strategies that advance equity, sustainability, and community well-being. I have over three years of hands-on experience in multimodal system evaluation, GIS modeling, benefit-cost and economic impact analysis for state and regional projects. I am skilled in leading interagency programs, managing complex contracts, and engaging diverse stakeholders to deliver innovative, resilient, and accessible transportation solutions grounded in inclusive, community-driven planning."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Pukar Bhandari",
    "section": "",
    "text": "I am an aspiring Associate Transportation Planner with a Masterâ€™s in City & Metropolitan Planning and a background in architecture, specializing in data-driven, multimodal strategies that advance equity, sustainability, and community well-being. I have over three years of hands-on experience in multimodal system evaluation, GIS modeling, benefit-cost and economic impact analysis for state and regional projects. I am skilled in leading interagency programs, managing complex contracts, and engaging diverse stakeholders to deliver innovative, resilient, and accessible transportation solutions grounded in inclusive, community-driven planning.\n\n\n\n\n\nAssociate Transportation Planner | Metro Analytics | June 2023 - Present\nGraduate Teaching & Research Assistant | University of Utah | August 2021 - May 2023\nGIS Data Analyst | Energy & Geoscience Institute | June 2022 - August 2022\nAssociate Architect | Prabal Thapa Architects | June 2020 - July 2021\nArchitect & Planner | Urban Park Nepal Pvt. Ltd.Â | January 2019 - June 2020\n\n\n\n\n\n\nMaster of City & Metropolitan Planning | University of Utah | August 2021 - May 2023\nBachelors Degree in Architecture | Tribhuvan University | October 2013 - August 2018"
  },
  {
    "objectID": "about.html#professional-overview",
    "href": "about.html#professional-overview",
    "title": "Pukar Bhandari",
    "section": "",
    "text": "I am an aspiring Associate Transportation Planner with a Masterâ€™s in City & Metropolitan Planning and a background in architecture, specializing in data-driven, multimodal strategies that advance equity, sustainability, and community well-being. I have over three years of hands-on experience in multimodal system evaluation, GIS modeling, benefit-cost and economic impact analysis for state and regional projects. I am skilled in leading interagency programs, managing complex contracts, and engaging diverse stakeholders to deliver innovative, resilient, and accessible transportation solutions grounded in inclusive, community-driven planning."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Pukar Bhandari",
    "section": "",
    "text": "Associate Transportation Planner | Metro Analytics | June 2023 - Present\nGraduate Teaching & Research Assistant | University of Utah | August 2021 - May 2023\nGIS Data Analyst | Energy & Geoscience Institute | June 2022 - August 2022\nAssociate Architect | Prabal Thapa Architects | June 2020 - July 2021\nArchitect & Planner | Urban Park Nepal Pvt. Ltd.Â | January 2019 - June 2020"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Pukar Bhandari",
    "section": "",
    "text": "Master of City & Metropolitan Planning | University of Utah | August 2021 - May 2023\nBachelors Degree in Architecture | Tribhuvan University | October 2013 - August 2018"
  },
  {
    "objectID": "404-gradethis.html",
    "href": "404-gradethis.html",
    "title": " Under Construction! ",
    "section": "",
    "text": "P.S. - If you got this far, youâ€™re definitely not lost anymore. Youâ€™re a certified Transportation Data Scientist! ðŸŽ¯\nBuilt with Quarto Live and WebR"
  },
  {
    "objectID": "404-gradethis.html#knowledge-test",
    "href": "404-gradethis.html#knowledge-test",
    "title": " Under Construction! ",
    "section": "ðŸ§  Knowledge Test",
    "text": "ðŸ§  Knowledge Test\nTest your understanding of transportation planning fundamentals\n\nQuestion 1: Traffic Signal Timing â±ï¸\nIf a traffic light has a 90-second cycle and the green light lasts 60 seconds, how many cars can pass through if each car takes 3 seconds to clear the intersection?\n\n\n\n\n\n\n\n\n\n\n\n# Calculate how many cars can pass in one green phase\ngreen_time &lt;- 60  # seconds\ntime_per_car &lt;- 3  # seconds\n\n# Divide green time by time per car\ncars_per_cycle &lt;- green_time / time_per_car\n\ncars_per_cycle\n# Calculate how many cars can pass in one green phase\ngreen_time &lt;- 60  # seconds\ntime_per_car &lt;- 3  # seconds\n\n# Divide green time by time per car\ncars_per_cycle &lt;- green_time / time_per_car\n\ncars_per_cycle\n\n\n\n\n\n\n\n\nQuestion 2: Level of Service ðŸš¦\nCalculate the volume-to-capacity (V/C) ratio for an intersection with 1,600 vehicles/hour and capacity of 1,800 vehicles/hour. What LOS category does this represent? (A-C: â‰¤0.8, D: 0.8-0.9, E-F: &gt;0.9)\n\n\n\n\n\n\n\n\n\n\n\nvolume &lt;- 1600\ncapacity &lt;- 1800\n\n# Calculate V/C ratio\nvc_ratio &lt;- volume / capacity\n\n# Determine LOS category based on thresholds\nlos_category &lt;- if (vc_ratio &lt;= 0.8) {\n  \"A-C\"\n} else if (vc_ratio &lt;= 0.9) {\n  \"D\" \n} else {\n  \"E-F\"\n}\n\nlist(vc_ratio = vc_ratio, los_category = los_category)\nvolume &lt;- 1600\ncapacity &lt;- 1800\n\n# Calculate V/C ratio\nvc_ratio &lt;- volume / capacity\n\n# Determine LOS category based on thresholds\nlos_category &lt;- if (vc_ratio &lt;= 0.8) {\n  \"A-C\"\n} else if (vc_ratio &lt;= 0.9) {\n  \"D\" \n} else {\n  \"E-F\"\n}\n\nlist(vc_ratio = vc_ratio, los_category = los_category)\n\n\n\n\n\n\n\n\nQuestion 3: Modal Split ðŸšŒðŸš²ðŸš—\nIn a city where 40% of trips are by car, 25% by transit, 20% by walking, and 15% by cycling, what percentage of non-car trips are made by transit?\n\n\n\n\n\n\n\n\n\n\n\ncar_percent &lt;- 40\ntransit_percent &lt;- 25\nwalking_percent &lt;- 20\ncycling_percent &lt;- 15\n\n# Calculate percentage of non-car trips that are transit\nnon_car_total &lt;- 100 - car_percent  # 60%\ntransit_share_of_non_car &lt;- (transit_percent / non_car_total) * 100\n\ntransit_share_of_non_car\ncar_percent &lt;- 40\ntransit_percent &lt;- 25\nwalking_percent &lt;- 20\ncycling_percent &lt;- 15\n\n# Calculate percentage of non-car trips that are transit\nnon_car_total &lt;- 100 - car_percent  # 60%\ntransit_share_of_non_car &lt;- (transit_percent / non_car_total) * 100\n\ntransit_share_of_non_car\n\n\n\n\n\n\n\n\nQuestion 4: Trip Generation ðŸ \nA residential development has 200 housing units. Using ITE trip generation rate of 9.5 trips per dwelling unit per day, calculate total daily trips. If 15% occur during PM peak hour, how many PM peak trips?\n\n\n\n\n\n\n\n\n\n\n\nhousing_units &lt;- 200\ntrip_rate &lt;- 9.5\npm_peak_percent &lt;- 15\n\n# Calculate total daily trips\ntotal_daily_trips &lt;- housing_units * trip_rate\n\n# Calculate PM peak hour trips\npm_peak_trips &lt;- total_daily_trips * (pm_peak_percent / 100)\n\nlist(daily = total_daily_trips, pm_peak = pm_peak_trips)\nhousing_units &lt;- 200\ntrip_rate &lt;- 9.5\npm_peak_percent &lt;- 15\n\n# Calculate total daily trips\ntotal_daily_trips &lt;- housing_units * trip_rate\n\n# Calculate PM peak hour trips\npm_peak_trips &lt;- total_daily_trips * (pm_peak_percent / 100)\n\nlist(daily = total_daily_trips, pm_peak = pm_peak_trips)\n\n\n\n\n\n\n\n\nQuestion 5: Transit Frequency ðŸš\nA bus route operates every 12 minutes during peak hours. How many buses pass a given stop in one hour? If each bus has 40-passenger capacity and average load is 75%, whatâ€™s the hourly passenger capacity past that stop?\n\n\n\n\n\n\n\n\n\n\n\nfrequency_minutes &lt;- 12\nbus_capacity &lt;- 40\nload_factor &lt;- 0.75\n\n# Calculate buses per hour\nbuses_per_hour &lt;- 60 / frequency_minutes\n\n# Calculate hourly passenger capacity\nhourly_passenger_capacity &lt;- buses_per_hour * bus_capacity * load_factor\n\nlist(buses_per_hour = buses_per_hour, hourly_capacity = hourly_passenger_capacity)\nfrequency_minutes &lt;- 12\nbus_capacity &lt;- 40\nload_factor &lt;- 0.75\n\n# Calculate buses per hour\nbuses_per_hour &lt;- 60 / frequency_minutes\n\n# Calculate hourly passenger capacity\nhourly_passenger_capacity &lt;- buses_per_hour * bus_capacity * load_factor\n\nlist(buses_per_hour = buses_per_hour, hourly_capacity = hourly_passenger_capacity)"
  },
  {
    "objectID": "404-gradethis.html#skills-test",
    "href": "404-gradethis.html#skills-test",
    "title": " Under Construction! ",
    "section": "ðŸš€ Skills Test",
    "text": "ðŸš€ Skills Test\nPut your R and dplyr skills to work with real transportation datasets\n\nQuestion 1: High-Traffic Location Analysis ðŸ“Š\nUsing the traffic_data dataset, filter for locations with AADT &gt; 15,000 and count how many locations meet this criteria.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Filter for high-traffic locations and count them\nhigh_traffic &lt;- traffic_data |&gt;\n  filter(aadt &gt; 15000)\n\n# Count the results\ncount_high_traffic &lt;- nrow(high_traffic)\n\ncount_high_traffic\n# Filter for high-traffic locations and count them\nhigh_traffic &lt;- traffic_data |&gt;\n  filter(aadt &gt; 15000)\n\n# Count the results\ncount_high_traffic &lt;- nrow(high_traffic)\n\ncount_high_traffic\n\n\n\n\n\n\n\n\nQuestion 2: Safety Rate Calculation ðŸ”\nCalculate the accident rate per 1,000 AADT for each location. Create a new column called accident_rate and arrange by highest rate.\n\n\n\n\n\n\n\n\n\n\n\n# Calculate accident rates and arrange\nsafety_analysis &lt;- traffic_data |&gt;\n  mutate(accident_rate = (accidents_2023 / aadt) * 1000) |&gt;\n  arrange(desc(accident_rate))\n\n# Show the results\nsafety_analysis\n# Calculate accident rates and arrange\nsafety_analysis &lt;- traffic_data |&gt;\n  mutate(accident_rate = (accidents_2023 / aadt) * 1000) |&gt;\n  arrange(desc(accident_rate))\n\n# Show the results\nsafety_analysis\n\n\n\n\n\n\n\n\nQuestion 3: Transit Efficiency Analysis ðŸšŒ\nCalculate riders per mile for each transit route and identify the most efficient route (highest riders per mile).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Calculate efficiency and find the best route\ntransit_efficiency &lt;- transit_data |&gt;\n  mutate(riders_per_mile = daily_riders / route_length_miles) |&gt;\n  arrange(desc(riders_per_mile))\n\n# Get the most efficient route name\nmost_efficient &lt;- transit_efficiency |&gt;\n  slice(1) |&gt;\n  pull(route)\n\nmost_efficient\n# Calculate efficiency and find the best route\ntransit_efficiency &lt;- transit_data |&gt;\n  mutate(riders_per_mile = daily_riders / route_length_miles) |&gt;\n  arrange(desc(riders_per_mile))\n\n# Get the most efficient route name\nmost_efficient &lt;- transit_efficiency |&gt;\n  slice(1) |&gt;\n  pull(route)\n\nmost_efficient\n\n\n\n\n\n\n\n\nQuestion 4: Bike Share Visualization ðŸ“ˆ\nCreate a scatter plot showing the relationship between distance from downtown and daily trips for bike share stations. Add a smooth trend line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Create the bike share analysis plot\nbike_plot &lt;- ggplot(bike_data, aes(x = distance_downtown_km, y = daily_trips)) +\n  geom_point(size = 3, alpha = 0.7, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Bike Share Usage vs Distance from Downtown\",\n    x = \"Distance from Downtown (km)\",\n    y = \"Daily Trips\"\n  ) +\n  theme_minimal()\n\nbike_plot\n# Create the bike share analysis plot\nbike_plot &lt;- ggplot(bike_data, aes(x = distance_downtown_km, y = daily_trips)) +\n  geom_point(size = 3, alpha = 0.7, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Bike Share Usage vs Distance from Downtown\",\n    x = \"Distance from Downtown (km)\",\n    y = \"Daily Trips\"\n  ) +\n  theme_minimal()\n\nbike_plot\n\n\n\n\n\n\n\n\nQuestion 5: Comprehensive Performance Summary ðŸ†\nCreate a summary table showing for each location: total daily volume, accidents per 1000 vehicles, and congestion level (Low: &lt;3hrs, Medium: 3-5hrs, High: &gt;5hrs).\n\n\n\n\n\n\n\n\n\n\n\n# Create comprehensive performance summary\nperformance_summary &lt;- traffic_data |&gt;\n  mutate(\n    daily_volume = aadt,\n    accidents_per_1000 = (accidents_2023 / aadt) * 1000,\n    congestion_level = case_when(\n      congestion_hours &lt; 3 ~ \"Low\",\n      congestion_hours &lt;= 5 ~ \"Medium\", \n      TRUE ~ \"High\"\n    )\n  ) |&gt;\n  select(location, daily_volume, accidents_per_1000, congestion_level)\n\nperformance_summary\n# Create comprehensive performance summary\nperformance_summary &lt;- traffic_data |&gt;\n  mutate(\n    daily_volume = aadt,\n    accidents_per_1000 = (accidents_2023 / aadt) * 1000,\n    congestion_level = case_when(\n      congestion_hours &lt; 3 ~ \"Low\",\n      congestion_hours &lt;= 5 ~ \"Medium\", \n      TRUE ~ \"High\"\n    )\n  ) |&gt;\n  select(location, daily_volume, accidents_per_1000, congestion_level)\n\nperformance_summary"
  },
  {
    "objectID": "404-gradethis.html#quiz-complete",
    "href": "404-gradethis.html#quiz-complete",
    "title": " Under Construction! ",
    "section": "ðŸŽ¯ Quiz Complete!",
    "text": "ðŸŽ¯ Quiz Complete!\nCongratulations, You are now a Transportation Data Scientist! ðŸ†"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Iâ€™m always excited to connect with fellow transportation professionals, urban planners, data scientists, and anyone passionate about improving mobility through data-driven insights. Whether you want to discuss the latest developments in transportation modeling, share experiences with R and geospatial analysis, or debate the merits of different traffic flow algorithms, Iâ€™m here for it.\nIf you have questions about transportation planning methodologies, want to discuss R techniques for mobility data analysis, share interesting datasets, or just want to chat about anything related to transportation and data science, please use this contact form.\n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\n\nSend message"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "The blog posts on this website are all released under a Creative Commons Attribution-ShareAlike 4.0 International License. Please feel free to use and share anything you find valuable in these posts, but please cite me too!"
  },
  {
    "objectID": "posts/overture-data-download/index.html",
    "href": "posts/overture-data-download/index.html",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "Overture Maps Foundation provides a collaborative, open-source initiative to create the worldâ€™s most comprehensive and interoperable geospatial dataset. As transportation planners and data analysts, we often need access to high-quality geospatial data for buildings, transportation networks, places, and administrative boundaries. This post demonstrates how to efficiently download Overture Maps data using both R and Python with DuckDBâ€™s powerful spatial capabilities.\n\nOverture Maps is an open-source mapping initiative that provides global-scale geospatial data across four main themes:\n\n\nBuildings: Footprints and building parts\n\nTransportation: Road segments and connectors\n\nPlaces: Points of interest and place data\n\nAdmins: Administrative boundaries and localities\n\nBase: Infrastructure, land use, land cover, and water features\n\nThe data is stored in cloud-optimized Parquet format on AWS S3, making it ideal for efficient querying and analysis.\n\nBefore diving into the code, ensure you have the following dependencies installed:\n\n\n R\n Python\n\n\n\n\n# Install required packages\ninstall.packages(c(\"tidyverse\", \"sf\", \"tmap\", \"DBI\", \"duckdb\", \"arrow\"))\n\n\n\n\n# Install required packages\npip install duckdb matplotlib geopandas pandas shapely folium pathlib\n\n\n\n\n\nFirst, we need to load our libraries and configure the environment for spatial data processing.\n\n\n R\n Python\n\n\n\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(arrow)\n\n# Set global options\noptions(scipen = 999) # avoiding scientific notation\ntmap_mode(mode = \"view\")\n\n\n\n\nimport duckdb\nimport geopandas as gpd\nimport pandas as pd\nimport shapely.wkb\nimport matplotlib.pyplot as plt\nimport folium\nfrom pathlib import Path\n\n\n\n\n\nOverture data is organized by themes, and we need to map specific data types to their corresponding themes for proper S3 path construction.\n\n\n R\n Python\n\n\n\n\n# Define the theme map\nmap_themes &lt;- list(\n  \"locality\" = \"admins\",\n  \"locality_area\" = \"admins\",\n  \"administrative_boundary\" = \"admins\",\n  \"building\" = \"buildings\",\n  \"building_part\" = \"buildings\",\n  \"place\" = \"places\",\n  \"segment\" = \"transportation\",\n  \"connector\" = \"transportation\",\n  \"infrastructure\" = \"base\",\n  \"land\" = \"base\",\n  \"land_use\" = \"base\",\n  \"water\" = \"base\"\n)\n\n\n\n\n# Define theme mapping\nTHEME_MAP = {\n    \"locality\": \"admins\",\n    \"locality_area\": \"admins\",\n    \"administrative_boundary\": \"admins\",\n    \"building\": \"buildings\",\n    \"building_part\": \"buildings\",\n    \"place\": \"places\",\n    \"segment\": \"transportation\",\n    \"connector\": \"transportation\",\n    \"infrastructure\": \"base\",\n    \"land\": \"base\",\n    \"land_use\": \"base\",\n    \"water\": \"base\",\n}\n\n\n\n\n\nThis function handles the DuckDB connection, S3 configuration, and spatial filtering to download only the data within your specified bounding box.\n\n\n R\n Python\n\n\n\n\noverture_data &lt;- function(bbox, overture_type, dst_parquet) {\n\n  # Validate overture_type\n  if (!overture_type %in% names(map_themes)) {\n    stop(paste(\"Valid Overture types are:\", paste(names(map_themes), collapse = \", \")))\n  }\n\n  # Configure S3 path\n  s3_region &lt;- \"us-west-2\"\n  base_url &lt;- sprintf(\"s3://overturemaps-%s/release\", s3_region)\n  version &lt;- \"2024-04-16-beta.0\"\n  theme &lt;- map_themes[[overture_type]]\n  remote_path &lt;- sprintf(\"%s/%s/theme=%s/type=%s/*\", base_url, version, theme, overture_type)\n\n  # Connect to DuckDB and install extensions\n  conn &lt;- dbConnect(duckdb::duckdb())\n  dbExecute(conn, \"INSTALL httpfs;\")\n  dbExecute(conn, \"INSTALL spatial;\")\n  dbExecute(conn, \"LOAD httpfs;\")\n  dbExecute(conn, \"LOAD spatial;\")\n  dbExecute(conn, sprintf(\"SET s3_region='%s';\", s3_region))\n\n  # Create view and execute spatial query\n  read_parquet &lt;- sprintf(\"read_parquet('%s', filename=TRUE, hive_partitioning=1);\", remote_path)\n  dbExecute(conn, sprintf(\"CREATE OR REPLACE VIEW data_view AS SELECT * FROM %s\", read_parquet))\n\n  query &lt;- sprintf(\"\n    SELECT data.*\n    FROM data_view AS data\n    WHERE data.bbox.xmin &lt;= %f AND data.bbox.xmax &gt;= %f\n    AND data.bbox.ymin &lt;= %f AND data.bbox.ymax &gt;= %f\n  \", bbox[3], bbox[1], bbox[4], bbox[2])\n\n  # Save results to Parquet file\n  file &lt;- normalizePath(dst_parquet, mustWork = FALSE)\n  dbExecute(conn, sprintf(\"COPY (%s) TO '%s' WITH (FORMAT 'parquet');\", query, file))\n  dbDisconnect(conn, shutdown = TRUE)\n}\n\n\n\n\ndef overture_data(bbox, overture_type, dst_parquet):\n    \"\"\"Query a subset of Overture's data and save it as a GeoParquet file.\n\n    Parameters\n    ----------\n    bbox : tuple\n        A tuple of floats representing the bounding box (xmin, ymin, xmax, ymax)\n        in EPSG:4326 coordinate reference system.\n    overture_type : str\n        The type of Overture data to query\n    dst_parquet : str or Path\n        The path to the output GeoParquet file.\n    \"\"\"\n    if overture_type not in THEME_MAP:\n        raise ValueError(f\"Valid Overture types are: {list(THEME_MAP)}\")\n\n    # Configure S3 connection\n    s3_region = \"us-west-2\"\n    base_url = f\"s3://overturemaps-{s3_region}/release\"\n    version = \"2024-04-16-beta.0\"\n    theme = THEME_MAP[overture_type]\n    remote_path = f\"{base_url}/{version}/theme={theme}/type={overture_type}/*\"\n\n    # Setup DuckDB with spatial extensions\n    conn = duckdb.connect()\n    conn.execute(\"INSTALL httpfs;\")\n    conn.execute(\"INSTALL spatial;\")\n    conn.execute(\"LOAD httpfs;\")\n    conn.execute(\"LOAD spatial;\")\n    conn.execute(f\"SET s3_region='{s3_region}';\")\n\n    # Execute spatial query\n    read_parquet = f\"read_parquet('{remote_path}', filename=true, hive_partitioning=1);\"\n    conn.execute(f\"CREATE OR REPLACE VIEW data_view AS SELECT * FROM {read_parquet}\")\n\n    query = f\"\"\"\n    SELECT data.*\n    FROM data_view AS data\n    WHERE data.bbox.xmin &lt;= {bbox[2]} AND data.bbox.xmax &gt;= {bbox[0]}\n    AND data.bbox.ymin &lt;= {bbox[3]} AND data.bbox.ymax &gt;= {bbox[1]}\n    \"\"\"\n\n    file = str(Path(dst_parquet).resolve())\n    conn.execute(f\"COPY ({query}) TO '{file}' WITH (FORMAT PARQUET);\")\n    conn.close()\n\n\n\n\n\nFor spatial analysis, you need to define a bounding box for your area of interest. This can come from existing boundary data or manual coordinates.\n\n\n R\n Python\n\n\n\n\n# Read existing boundary data (example: Albany, NY)\nalbany_boundary &lt;- read_sf(file.path(wd, project), layer = \"City_of_Albany\") |&gt;\n  st_transform(4326)\n\n# Extract bounding box coordinates (xmin, ymin, xmax, ymax)\nalbany_bbox &lt;- albany_boundary |&gt;\n  st_bbox() |&gt;\n  as.vector()\n\nprint(albany_bbox)\n\n\n\n\n# Define study area bounding box manually\n# Manhattan example (xmin, ymin, xmax, ymax) in EPSG:4326\nbbox_example = (-74.02169, 40.696423, -73.891338, 40.831263)\n\n# Alternative: extract from existing boundary data\n# boundary_gdf = gpd.read_file(\"your_boundary.shp\")\n# bbox_example = boundary_gdf.total_bounds\n\n\n\n\n\nNow we can download specific data types for our study area. The function handles all the cloud connectivity and spatial filtering automatically.\n\n\n R\n Python\n\n\n\n\n# Download places data for Albany\noverture_data(albany_bbox, \"place\", \"albany_places_subset.parquet\")\n\n# Download buildings data\noverture_data(albany_bbox, \"building\", \"albany_buildings_subset.parquet\")\n\n# Download transportation segments\noverture_data(albany_bbox, \"segment\", \"albany_roads_subset.parquet\")\n\n\n\n\n# Download buildings data for Manhattan\noverture_data(bbox_example, \"building\", \"nyc_buildings_subset.parquet\")\n\n# Download places data\noverture_data(bbox_example, \"place\", \"nyc_places_subset.parquet\")\n\n# Download transportation network\noverture_data(bbox_example, \"segment\", \"nyc_roads_subset.parquet\")\n\n\n\n\n\nAfter downloading, convert the Parquet files to spatial data formats for analysis and visualization.\n\n\n R\n Python\n\n\n\n\n# Read the downloaded Parquet file\nalbany_places &lt;- read_parquet(\"albany_places_subset.parquet\")\n\n# Convert to sf object for spatial operations\nalbany_places_sf &lt;- st_as_sf(\n  albany_places |&gt; select(-sources),\n  geometry = albany_places$geometry,\n  crs = 4326\n)\n\n# Basic data exploration\nprint(paste(\"Downloaded\", nrow(albany_places_sf), \"places\"))\nprint(colnames(albany_places_sf))\n\n\n\n\n# Read the downloaded data\nmanhattan = pd.read_parquet(\"nyc_buildings_subset.parquet\")\n\n# Convert to GeoDataFrame\nmanhattan_gdf = gpd.GeoDataFrame(\n    manhattan.drop(columns=\"geometry\"),\n    geometry=shapely.wkb.loads(manhattan[\"geometry\"]),\n    crs=4326,\n)\n\n# Basic exploration\nprint(f\"Downloaded {len(manhattan_gdf)} buildings\")\nprint(manhattan_gdf.columns.tolist())\n\n\n\n\n\nCreate quick visualizations to explore your downloaded data and verify the results.\n\n\n R\n Python\n\n\n\n\n# Interactive map using tmap (equivalent to folium)\nalbany_places_sf |&gt;\n  select(names$primary, categories$main, confidence) |&gt;\n  tm_shape() +\n  tm_dots(col = \"categories$main\", size = 0.5, alpha = 0.8) +\n  tm_view(view.legend.position = c(\"left\", \"bottom\"))\n\n# Simple quick visualization\nalbany_places_sf |&gt;\n  select(names$primary, categories$main, confidence) |&gt;\n  qtm(dots.col = \"categories$main\")\n\n\n\n\n# Static plot using GeoPandas\nmanhattan_gdf.plot(figsize=(10, 10), alpha=0.7, column='categories', legend=True)\nplt.title(\"Manhattan Buildings from Overture Maps\")\nplt.show()\n\n# Interactive map with folium (equivalent to tmap interactive)\nm = folium.Map(location=[40.7589, -73.9851], zoom_start=12)\nfolium.GeoJson(\n    manhattan_gdf.head(100),\n    popup=folium.GeoJsonPopup(fields=['names', 'categories'])\n).add_to(m)\nm\n\n\n\n\n\nOverture Maps provides the following data types organized by theme:\n\n\n\n\n\n\n\nTheme\nData Types\nDescription\n\n\n\nAdmins\n\nlocality, locality_area, administrative_boundary\n\nAdministrative boundaries and place hierarchies\n\n\nBuildings\n\nbuilding, building_part\n\nBuilding footprints and structural components\n\n\nPlaces\nplace\nPoints of interest, businesses, and landmarks\n\n\nTransportation\n\nsegment, connector\n\nRoad networks and transportation infrastructure\n\n\nBase\n\ninfrastructure, land, land_use, water\n\nBase map features and land cover\n\n\n\nThis approach is particularly valuable for transportation planning workflows where you need to integrate multiple data sources for comprehensive analysis. The standardized schema and efficient spatial querying make it ideal for network analysis, land use integration, and multi-modal planning across different jurisdictions and scales.\n\nThe complete code and examples are available in the Overture Data Download repository on GitHub.\nFor more information about Overture Maps:\n\nOfficial Documentation\nData Schema Reference\nCommunity Forum\n\n\nWant to contribute or suggest improvements? Visit the project repository at: https://github.com/ar-puuk/overture-data-download"
  },
  {
    "objectID": "posts/overture-data-download/index.html#what-is-overture-maps",
    "href": "posts/overture-data-download/index.html#what-is-overture-maps",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "Overture Maps is an open-source mapping initiative that provides global-scale geospatial data across four main themes:\n\n\nBuildings: Footprints and building parts\n\nTransportation: Road segments and connectors\n\nPlaces: Points of interest and place data\n\nAdmins: Administrative boundaries and localities\n\nBase: Infrastructure, land use, land cover, and water features\n\nThe data is stored in cloud-optimized Parquet format on AWS S3, making it ideal for efficient querying and analysis."
  },
  {
    "objectID": "posts/overture-data-download/index.html#prerequisites",
    "href": "posts/overture-data-download/index.html#prerequisites",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "Before diving into the code, ensure you have the following dependencies installed:\n\n\n R\n Python\n\n\n\n\n# Install required packages\ninstall.packages(c(\"tidyverse\", \"sf\", \"tmap\", \"DBI\", \"duckdb\", \"arrow\"))\n\n\n\n\n# Install required packages\npip install duckdb matplotlib geopandas pandas shapely folium pathlib"
  },
  {
    "objectID": "posts/overture-data-download/index.html#setting-up-the-environment",
    "href": "posts/overture-data-download/index.html#setting-up-the-environment",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "First, we need to load our libraries and configure the environment for spatial data processing.\n\n\n R\n Python\n\n\n\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(arrow)\n\n# Set global options\noptions(scipen = 999) # avoiding scientific notation\ntmap_mode(mode = \"view\")\n\n\n\n\nimport duckdb\nimport geopandas as gpd\nimport pandas as pd\nimport shapely.wkb\nimport matplotlib.pyplot as plt\nimport folium\nfrom pathlib import Path"
  },
  {
    "objectID": "posts/overture-data-download/index.html#data-type-mapping",
    "href": "posts/overture-data-download/index.html#data-type-mapping",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "Overture data is organized by themes, and we need to map specific data types to their corresponding themes for proper S3 path construction.\n\n\n R\n Python\n\n\n\n\n# Define the theme map\nmap_themes &lt;- list(\n  \"locality\" = \"admins\",\n  \"locality_area\" = \"admins\",\n  \"administrative_boundary\" = \"admins\",\n  \"building\" = \"buildings\",\n  \"building_part\" = \"buildings\",\n  \"place\" = \"places\",\n  \"segment\" = \"transportation\",\n  \"connector\" = \"transportation\",\n  \"infrastructure\" = \"base\",\n  \"land\" = \"base\",\n  \"land_use\" = \"base\",\n  \"water\" = \"base\"\n)\n\n\n\n\n# Define theme mapping\nTHEME_MAP = {\n    \"locality\": \"admins\",\n    \"locality_area\": \"admins\",\n    \"administrative_boundary\": \"admins\",\n    \"building\": \"buildings\",\n    \"building_part\": \"buildings\",\n    \"place\": \"places\",\n    \"segment\": \"transportation\",\n    \"connector\": \"transportation\",\n    \"infrastructure\": \"base\",\n    \"land\": \"base\",\n    \"land_use\": \"base\",\n    \"water\": \"base\",\n}"
  },
  {
    "objectID": "posts/overture-data-download/index.html#core-download-function",
    "href": "posts/overture-data-download/index.html#core-download-function",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "This function handles the DuckDB connection, S3 configuration, and spatial filtering to download only the data within your specified bounding box.\n\n\n R\n Python\n\n\n\n\noverture_data &lt;- function(bbox, overture_type, dst_parquet) {\n\n  # Validate overture_type\n  if (!overture_type %in% names(map_themes)) {\n    stop(paste(\"Valid Overture types are:\", paste(names(map_themes), collapse = \", \")))\n  }\n\n  # Configure S3 path\n  s3_region &lt;- \"us-west-2\"\n  base_url &lt;- sprintf(\"s3://overturemaps-%s/release\", s3_region)\n  version &lt;- \"2024-04-16-beta.0\"\n  theme &lt;- map_themes[[overture_type]]\n  remote_path &lt;- sprintf(\"%s/%s/theme=%s/type=%s/*\", base_url, version, theme, overture_type)\n\n  # Connect to DuckDB and install extensions\n  conn &lt;- dbConnect(duckdb::duckdb())\n  dbExecute(conn, \"INSTALL httpfs;\")\n  dbExecute(conn, \"INSTALL spatial;\")\n  dbExecute(conn, \"LOAD httpfs;\")\n  dbExecute(conn, \"LOAD spatial;\")\n  dbExecute(conn, sprintf(\"SET s3_region='%s';\", s3_region))\n\n  # Create view and execute spatial query\n  read_parquet &lt;- sprintf(\"read_parquet('%s', filename=TRUE, hive_partitioning=1);\", remote_path)\n  dbExecute(conn, sprintf(\"CREATE OR REPLACE VIEW data_view AS SELECT * FROM %s\", read_parquet))\n\n  query &lt;- sprintf(\"\n    SELECT data.*\n    FROM data_view AS data\n    WHERE data.bbox.xmin &lt;= %f AND data.bbox.xmax &gt;= %f\n    AND data.bbox.ymin &lt;= %f AND data.bbox.ymax &gt;= %f\n  \", bbox[3], bbox[1], bbox[4], bbox[2])\n\n  # Save results to Parquet file\n  file &lt;- normalizePath(dst_parquet, mustWork = FALSE)\n  dbExecute(conn, sprintf(\"COPY (%s) TO '%s' WITH (FORMAT 'parquet');\", query, file))\n  dbDisconnect(conn, shutdown = TRUE)\n}\n\n\n\n\ndef overture_data(bbox, overture_type, dst_parquet):\n    \"\"\"Query a subset of Overture's data and save it as a GeoParquet file.\n\n    Parameters\n    ----------\n    bbox : tuple\n        A tuple of floats representing the bounding box (xmin, ymin, xmax, ymax)\n        in EPSG:4326 coordinate reference system.\n    overture_type : str\n        The type of Overture data to query\n    dst_parquet : str or Path\n        The path to the output GeoParquet file.\n    \"\"\"\n    if overture_type not in THEME_MAP:\n        raise ValueError(f\"Valid Overture types are: {list(THEME_MAP)}\")\n\n    # Configure S3 connection\n    s3_region = \"us-west-2\"\n    base_url = f\"s3://overturemaps-{s3_region}/release\"\n    version = \"2024-04-16-beta.0\"\n    theme = THEME_MAP[overture_type]\n    remote_path = f\"{base_url}/{version}/theme={theme}/type={overture_type}/*\"\n\n    # Setup DuckDB with spatial extensions\n    conn = duckdb.connect()\n    conn.execute(\"INSTALL httpfs;\")\n    conn.execute(\"INSTALL spatial;\")\n    conn.execute(\"LOAD httpfs;\")\n    conn.execute(\"LOAD spatial;\")\n    conn.execute(f\"SET s3_region='{s3_region}';\")\n\n    # Execute spatial query\n    read_parquet = f\"read_parquet('{remote_path}', filename=true, hive_partitioning=1);\"\n    conn.execute(f\"CREATE OR REPLACE VIEW data_view AS SELECT * FROM {read_parquet}\")\n\n    query = f\"\"\"\n    SELECT data.*\n    FROM data_view AS data\n    WHERE data.bbox.xmin &lt;= {bbox[2]} AND data.bbox.xmax &gt;= {bbox[0]}\n    AND data.bbox.ymin &lt;= {bbox[3]} AND data.bbox.ymax &gt;= {bbox[1]}\n    \"\"\"\n\n    file = str(Path(dst_parquet).resolve())\n    conn.execute(f\"COPY ({query}) TO '{file}' WITH (FORMAT PARQUET);\")\n    conn.close()"
  },
  {
    "objectID": "posts/overture-data-download/index.html#defining-your-study-area",
    "href": "posts/overture-data-download/index.html#defining-your-study-area",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "For spatial analysis, you need to define a bounding box for your area of interest. This can come from existing boundary data or manual coordinates.\n\n\n R\n Python\n\n\n\n\n# Read existing boundary data (example: Albany, NY)\nalbany_boundary &lt;- read_sf(file.path(wd, project), layer = \"City_of_Albany\") |&gt;\n  st_transform(4326)\n\n# Extract bounding box coordinates (xmin, ymin, xmax, ymax)\nalbany_bbox &lt;- albany_boundary |&gt;\n  st_bbox() |&gt;\n  as.vector()\n\nprint(albany_bbox)\n\n\n\n\n# Define study area bounding box manually\n# Manhattan example (xmin, ymin, xmax, ymax) in EPSG:4326\nbbox_example = (-74.02169, 40.696423, -73.891338, 40.831263)\n\n# Alternative: extract from existing boundary data\n# boundary_gdf = gpd.read_file(\"your_boundary.shp\")\n# bbox_example = boundary_gdf.total_bounds"
  },
  {
    "objectID": "posts/overture-data-download/index.html#downloading-the-data",
    "href": "posts/overture-data-download/index.html#downloading-the-data",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "Now we can download specific data types for our study area. The function handles all the cloud connectivity and spatial filtering automatically.\n\n\n R\n Python\n\n\n\n\n# Download places data for Albany\noverture_data(albany_bbox, \"place\", \"albany_places_subset.parquet\")\n\n# Download buildings data\noverture_data(albany_bbox, \"building\", \"albany_buildings_subset.parquet\")\n\n# Download transportation segments\noverture_data(albany_bbox, \"segment\", \"albany_roads_subset.parquet\")\n\n\n\n\n# Download buildings data for Manhattan\noverture_data(bbox_example, \"building\", \"nyc_buildings_subset.parquet\")\n\n# Download places data\noverture_data(bbox_example, \"place\", \"nyc_places_subset.parquet\")\n\n# Download transportation network\noverture_data(bbox_example, \"segment\", \"nyc_roads_subset.parquet\")"
  },
  {
    "objectID": "posts/overture-data-download/index.html#processing-downloaded-data",
    "href": "posts/overture-data-download/index.html#processing-downloaded-data",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "After downloading, convert the Parquet files to spatial data formats for analysis and visualization.\n\n\n R\n Python\n\n\n\n\n# Read the downloaded Parquet file\nalbany_places &lt;- read_parquet(\"albany_places_subset.parquet\")\n\n# Convert to sf object for spatial operations\nalbany_places_sf &lt;- st_as_sf(\n  albany_places |&gt; select(-sources),\n  geometry = albany_places$geometry,\n  crs = 4326\n)\n\n# Basic data exploration\nprint(paste(\"Downloaded\", nrow(albany_places_sf), \"places\"))\nprint(colnames(albany_places_sf))\n\n\n\n\n# Read the downloaded data\nmanhattan = pd.read_parquet(\"nyc_buildings_subset.parquet\")\n\n# Convert to GeoDataFrame\nmanhattan_gdf = gpd.GeoDataFrame(\n    manhattan.drop(columns=\"geometry\"),\n    geometry=shapely.wkb.loads(manhattan[\"geometry\"]),\n    crs=4326,\n)\n\n# Basic exploration\nprint(f\"Downloaded {len(manhattan_gdf)} buildings\")\nprint(manhattan_gdf.columns.tolist())"
  },
  {
    "objectID": "posts/overture-data-download/index.html#data-visualization",
    "href": "posts/overture-data-download/index.html#data-visualization",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "Create quick visualizations to explore your downloaded data and verify the results.\n\n\n R\n Python\n\n\n\n\n# Interactive map using tmap (equivalent to folium)\nalbany_places_sf |&gt;\n  select(names$primary, categories$main, confidence) |&gt;\n  tm_shape() +\n  tm_dots(col = \"categories$main\", size = 0.5, alpha = 0.8) +\n  tm_view(view.legend.position = c(\"left\", \"bottom\"))\n\n# Simple quick visualization\nalbany_places_sf |&gt;\n  select(names$primary, categories$main, confidence) |&gt;\n  qtm(dots.col = \"categories$main\")\n\n\n\n\n# Static plot using GeoPandas\nmanhattan_gdf.plot(figsize=(10, 10), alpha=0.7, column='categories', legend=True)\nplt.title(\"Manhattan Buildings from Overture Maps\")\nplt.show()\n\n# Interactive map with folium (equivalent to tmap interactive)\nm = folium.Map(location=[40.7589, -73.9851], zoom_start=12)\nfolium.GeoJson(\n    manhattan_gdf.head(100),\n    popup=folium.GeoJsonPopup(fields=['names', 'categories'])\n).add_to(m)\nm"
  },
  {
    "objectID": "posts/overture-data-download/index.html#available-data-types",
    "href": "posts/overture-data-download/index.html#available-data-types",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "Overture Maps provides the following data types organized by theme:\n\n\n\n\n\n\n\nTheme\nData Types\nDescription\n\n\n\nAdmins\n\nlocality, locality_area, administrative_boundary\n\nAdministrative boundaries and place hierarchies\n\n\nBuildings\n\nbuilding, building_part\n\nBuilding footprints and structural components\n\n\nPlaces\nplace\nPoints of interest, businesses, and landmarks\n\n\nTransportation\n\nsegment, connector\n\nRoad networks and transportation infrastructure\n\n\nBase\n\ninfrastructure, land, land_use, water\n\nBase map features and land cover"
  },
  {
    "objectID": "posts/overture-data-download/index.html#transportation-planning-applications",
    "href": "posts/overture-data-download/index.html#transportation-planning-applications",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "This approach is particularly valuable for transportation planning workflows where you need to integrate multiple data sources for comprehensive analysis. The standardized schema and efficient spatial querying make it ideal for network analysis, land use integration, and multi-modal planning across different jurisdictions and scales."
  },
  {
    "objectID": "posts/overture-data-download/index.html#repository-and-additional-resources",
    "href": "posts/overture-data-download/index.html#repository-and-additional-resources",
    "title": "Overture Maps Data Download",
    "section": "",
    "text": "The complete code and examples are available in the Overture Data Download repository on GitHub.\nFor more information about Overture Maps:\n\nOfficial Documentation\nData Schema Reference\nCommunity Forum\n\n\nWant to contribute or suggest improvements? Visit the project repository at: https://github.com/ar-puuk/overture-data-download"
  },
  {
    "objectID": "posts/robocopy-gui/index.html",
    "href": "posts/robocopy-gui/index.html",
    "title": "Interactive Robocopy GUI Tool",
    "section": "",
    "text": "Iâ€™ve developed an interactive web-based tool that simplifies creating robocopy commands through a user-friendly graphical interface. Instead of memorizing complex command-line syntax, users can simply select options and generate commands visually.\n\n\n\nOpen in New Tab View Source Code"
  },
  {
    "objectID": "posts/robocopy-gui/index.html#robocopy-command-generator",
    "href": "posts/robocopy-gui/index.html#robocopy-command-generator",
    "title": "Interactive Robocopy GUI Tool",
    "section": "",
    "text": "Iâ€™ve developed an interactive web-based tool that simplifies creating robocopy commands through a user-friendly graphical interface. Instead of memorizing complex command-line syntax, users can simply select options and generate commands visually.\n\n\n\nOpen in New Tab View Source Code"
  },
  {
    "objectID": "posts/robocopy-gui/index.html#features",
    "href": "posts/robocopy-gui/index.html#features",
    "title": "Interactive Robocopy GUI Tool",
    "section": "2 Features",
    "text": "2 Features\nThe tool provides an intuitive interface for:\n\nSource and Destination Selection: Easy path input with validation\nCopy Options: Checkboxes for common robocopy parameters like /E, /MIR, /R:n, etc.\nReal-time Preview: See your robocopy command as you build it\nCopy to Clipboard: One-click copying of the generated command\nParameter Explanations: Tooltips and descriptions for each option"
  },
  {
    "objectID": "posts/robocopy-gui/index.html#technical-details",
    "href": "posts/robocopy-gui/index.html#technical-details",
    "title": "Interactive Robocopy GUI Tool",
    "section": "3 Technical Details",
    "text": "3 Technical Details\nThis tool is built with vanilla HTML, CSS, and JavaScript, making it lightweight and fast. Itâ€™s hosted on GitHub Pages, which means it automatically updates whenever I push changes to the source repository.\nThe interface translates user selections into proper robocopy syntax, helping both beginners learn the command structure and experienced users save time on complex operations."
  },
  {
    "objectID": "posts/robocopy-gui/index.html#use-cases",
    "href": "posts/robocopy-gui/index.html#use-cases",
    "title": "Interactive Robocopy GUI Tool",
    "section": "4 Use Cases",
    "text": "4 Use Cases\nThis tool is particularly useful for:\n\nSystem administrators managing file synchronization\nIT professionals setting up backup routines\n\nAnyone needing to copy large directory structures with specific parameters\nLearning robocopy syntax through visual exploration\n\n\nWant to contribute or suggest improvements? Visit the project repository at: https://github.com/ar-puuk/robocopy-gui"
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html",
    "href": "posts/uscensus-dashboard/index.html",
    "title": "Building a U.S. Census Data Explorer",
    "section": "",
    "text": "As a transportation planner, accessing and visualizing demographic and employment data is crucial for informed decision-making. The U.S. Census Bureau provides a wealth of information through various datasets, but navigating multiple APIs and data formats can be challenging. To streamline this process, I developed an interactive R Shiny dashboard that provides unified access to three major Census data sources:\n\nAmerican Community Survey (ACS) - Detailed demographic and socioeconomic estimates\nDecennial Census - Complete population counts every 10 years\n\nLongitudinal Employer-Household Dynamics (LEHD) - Employment and commuting patterns\n\nThis post walks through the development process, key technical decisions, and lessons learned while building this data exploration tool."
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#introduction",
    "href": "posts/uscensus-dashboard/index.html#introduction",
    "title": "Building a U.S. Census Data Explorer",
    "section": "",
    "text": "As a transportation planner, accessing and visualizing demographic and employment data is crucial for informed decision-making. The U.S. Census Bureau provides a wealth of information through various datasets, but navigating multiple APIs and data formats can be challenging. To streamline this process, I developed an interactive R Shiny dashboard that provides unified access to three major Census data sources:\n\nAmerican Community Survey (ACS) - Detailed demographic and socioeconomic estimates\nDecennial Census - Complete population counts every 10 years\n\nLongitudinal Employer-Household Dynamics (LEHD) - Employment and commuting patterns\n\nThis post walks through the development process, key technical decisions, and lessons learned while building this data exploration tool."
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#project-overview",
    "href": "posts/uscensus-dashboard/index.html#project-overview",
    "title": "Building a U.S. Census Data Explorer",
    "section": "2 Project Overview",
    "text": "2 Project Overview\nThe dashboard serves as a one-stop interface for Census data retrieval, featuring:\n\nInteractive geographic selection (state/county level)\nVariable selection through searchable tables\nReal-time data visualization on interactive maps\nExport capabilities for both tabular and geospatial data\nResponsive design optimized for different screen sizes\n\nðŸ”— Repository: You can find the complete source code and documentation at: https://github.com/ar-puuk/uscensus-dashboard/"
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#technical-architecture",
    "href": "posts/uscensus-dashboard/index.html#technical-architecture",
    "title": "Building a U.S. Census Data Explorer",
    "section": "3 Technical Architecture",
    "text": "3 Technical Architecture\n\nCore Dependencies\nThe application leverages several specialized R packages for Census data access and geospatial processing:\n# Census data access\nlibrary(tidycensus)   # ACS and Decennial Census API\nlibrary(lehdr)        # LEHD Origin-Destination data\nlibrary(tigris)       # Geographic boundaries\n\n# Geospatial processing  \nlibrary(sf)           # Simple features for spatial data\nlibrary(leaflet)      # Interactive mapping\n\n# Shiny ecosystem\nlibrary(shiny)\nlibrary(shinydashboard)\nlibrary(shinyWidgets)\nlibrary(DT)          # Interactive data tables\n\n\nApplication Structure\nThe app follows a modular design pattern with separate UI and server components for each data source. This approach enhances maintainability and allows for independent feature development.\n# Modular UI components\nacsUI &lt;- fluidPage(...)      # American Community Survey interface\ncensusUI &lt;- fluidPage(...)   # Decennial Census interface  \nlehdUI &lt;- fluidPage(...)     # LEHD interface\n\n# Unified navigation\nui &lt;- navbarPage(...)"
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#data-source-integration",
    "href": "posts/uscensus-dashboard/index.html#data-source-integration",
    "title": "Building a U.S. Census Data Explorer",
    "section": "4 Data Source Integration",
    "text": "4 Data Source Integration\n\nAmerican Community Survey (ACS)\nThe ACS module provides access to detailed demographic estimates from 2009-2022. Key implementation features:\nDynamic Variable Loading: Variables are loaded based on the selected year and geographic level, ensuring users only see relevant options.\nvariables_acs &lt;- reactive({\n  if (!is.null(input$year_acs)) {\n    variables &lt;- load_variables(year = as.numeric(input$year_acs), \n                               dataset = \"acs5\", cache = TRUE)\n    # Filter variables based on geographic level\n    if (input$level_acs == \"tract\") {\n      variables &lt;- variables |&gt;\n        filter(geography %in% c(\"tract\", \"block group\"))\n    }\n    return(variables)\n  }\n})\nGeographic Hierarchy: The interface maintains proper geographic relationships, with county options updating based on state selection.\n\n\nDecennial Census\nThe Decennial Census module focuses on the 2000, 2010, and 2020 complete counts, using the PL 94-171 dataset optimized for redistricting data.\nStreamlined Variable Selection: Since Decennial Census has fewer variables than ACS, the interface emphasizes ease of use while maintaining the same interaction patterns.\n\n\nLongitudinal Employer-Household Dynamics (LEHD)\nThe LEHD module provides employment statistics and origin-destination flows, crucial for transportation planning applications.\nVersion Management: Different LODES versions cover different time periods, requiring dynamic year filtering:\nobserve({\n  version &lt;- if (!is.null(input$version_lehd)) input$version_lehd else \"default\"\n  \n  options &lt;- if (version == \"LODES5\") {\n    2002:2009\n  } else if (version == \"LODES7\") {\n    2002:2019  \n  } else {\n    2002:2021\n  }\n  \n  updateSelectInput(session, \"year_lehd\", choices = options, selected = max(options))\n})"
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#interactive-mapping",
    "href": "posts/uscensus-dashboard/index.html#interactive-mapping",
    "title": "Building a U.S. Census Data Explorer",
    "section": "5 Interactive Mapping",
    "text": "5 Interactive Mapping\n\nBase Map Configuration\nThe application uses Leaflet for interactive mapping, with a carefully chosen base layer that balances aesthetics and functionality:\ndefault_map &lt;- leaflet(options = leafletOptions(crs = leafletCRS())) |&gt;\n  addProviderTiles(\"CartoDB.Voyager\") |&gt;\n  addPolygons(data = sf_states, color = \"#222222\", weight = 1, fillOpacity = 0.15)\nCartoDB.Voyager was selected for its clean design and good contrast with overlay data, though the code structure allows for easy switching between provider tiles.\n\n\nDynamic Geographic Focus\nMaps automatically adjust to show selected geographic areas, providing contextual awareness:\noutput$map_acs &lt;- renderLeaflet({\n  if (!is.null(input$state_acs) && input$state_acs != \"\") {\n    selected_state_acs &lt;- sf_states[sf_states$NAME == input$state_acs, ]\n    selected_bbox_acs &lt;- sf::st_bbox(selected_state_acs)\n    \n    default_map |&gt;\n      addPolygons(data = selected_state_acs, color = \"#222222\", weight = 4) |&gt;\n      fitBounds(selected_bbox_acs[[1]], selected_bbox_acs[[2]], \n               selected_bbox_acs[[3]], selected_bbox_acs[[4]])\n  }\n})"
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#data-export-functionality",
    "href": "posts/uscensus-dashboard/index.html#data-export-functionality",
    "title": "Building a U.S. Census Data Explorer",
    "section": "6 Data Export Functionality",
    "text": "6 Data Export Functionality\n\nFlexible Format Support\nUsers can export data in multiple formats depending on their needs:\n\nCSV files for tabular analysis\nShapefiles (zipped) for GIS applications\n\noutput$download_acs &lt;- downloadHandler(\n  filename = function() {\n    if (input$geometry_acs) {\n      paste0(\"acs_data_\", input$state_acs, \"_\", input$year_acs, \".zip\")\n    } else {\n      paste0(\"acs_data_\", input$state_acs, \"_\", input$year_acs, \".csv\")\n    }\n  },\n  content = function(file) {\n    if (input$geometry_acs) {\n      write_sf_zip(data_acs(), file, overwrite = TRUE)\n    } else {\n      readr::write_csv(data_acs() |&gt; st_drop_geometry(), file)\n    }\n  }\n)\n\n\nCustom Shapefile Export\nSince Râ€™s sf package doesnâ€™t directly export zipped shapefiles, I implemented a custom function to handle the complete shapefile format:\nwrite_sf_zip &lt;- function(obj, zipfile, overwrite = FALSE) {\n  # Create temporary directory for shapefile components\n  tmp &lt;- tempfile()\n  dir.create(tmp)\n  on.exit(unlink(tmp, recursive = TRUE, force = TRUE))\n  \n  # Write shapefile and zip all components\n  sf::write_sf(obj, file.path(tmp, shp_name), delete_layer = TRUE)\n  withr::with_dir(tmp, zip(tmp_zip, list.files()))\n  \n  file.copy(file.path(tmp, tmp_zip), zipfile, overwrite = overwrite)\n}"
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#user-experience-considerations",
    "href": "posts/uscensus-dashboard/index.html#user-experience-considerations",
    "title": "Building a U.S. Census Data Explorer",
    "section": "7 User Experience Considerations",
    "text": "7 User Experience Considerations\n\nVariable Selection Interface\nOne of the biggest UX challenges was making Census variable selection intuitive. The solution uses modal dialogs with searchable data tables:\nvar_modal_acs &lt;- modalDialog(\n  title = h4(\"Select Variable(s) from the List\"),\n  DTOutput(\"var_table_acs\"),\n  size = \"l\",\n  easyClose = TRUE,\n  footer = actionButton(\"selectVarButton_acs\", \"Select Variable(s)\")\n)\nThis approach allows users to browse thousands of variables efficiently while maintaining a clean main interface.\n\n\nResponsive Geographic Selection\nThe cascading geographic selection (State â†’ County â†’ Geographic Level) follows familiar patterns while enforcing data availability constraints.\n\n\nProgress Feedback\nAPI calls can take several seconds, so the interface provides clear feedback through action buttons and conditional panels that appear after data is loaded."
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#performance-optimizations",
    "href": "posts/uscensus-dashboard/index.html#performance-optimizations",
    "title": "Building a U.S. Census Data Explorer",
    "section": "8 Performance Optimizations",
    "text": "8 Performance Optimizations\n\nCaching Strategy\noptions(tigris_use_cache = TRUE)  # Geographic boundaries\nload_variables(..., cache = TRUE) # Variable metadata\nCaching is enabled for static data like geographic boundaries and variable definitions, significantly reducing load times for repeat users.\n\n\nData Processing Efficiency\nFor ACS data, margin of error columns are automatically removed to focus on estimates:\nresult_acs &lt;- result_acs |&gt;\n  select(-matches(\"M$\")) |&gt;    # Remove margin columns\n  rename_all(~ sub(\"E$\", \"\", .)) # Clean estimate column names"
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#deployment-considerations",
    "href": "posts/uscensus-dashboard/index.html#deployment-considerations",
    "title": "Building a U.S. Census Data Explorer",
    "section": "9 Deployment Considerations",
    "text": "9 Deployment Considerations\n\nAPI Key Management\nThe application requires Census API keys but handles them securely through user input rather than hardcoding. This approach ensures:\n\nNo sensitive credentials in source code\nUsers maintain control over their API usage\nEasy deployment across different environments\n\n\n\nError Handling\nRobust error handling prevents crashes when API calls fail or users make invalid selections:\nreq(input$api_key_acs, input$year_acs, input$state_acs, input$county_acs, input$level_acs)\nThe req() function ensures all required inputs are available before processing."
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#future-enhancements",
    "href": "posts/uscensus-dashboard/index.html#future-enhancements",
    "title": "Building a U.S. Census Data Explorer",
    "section": "10 Future Enhancements",
    "text": "10 Future Enhancements\nSeveral features are planned for future releases:\n\nData Visualization: Built-in charts and maps with Census data overlays\nComparison Tools: Side-by-side analysis across years or geographies\n\nCustom Geography: Support for user-uploaded boundary files\nBatch Processing: Multiple state/year combinations in single requests\nAPI Integration: Direct connection to external GIS platforms"
  },
  {
    "objectID": "posts/uscensus-dashboard/index.html#conclusion",
    "href": "posts/uscensus-dashboard/index.html#conclusion",
    "title": "Building a U.S. Census Data Explorer",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nThis Census data explorer demonstrates the power of R Shiny for creating specialized data access tools. By combining multiple Census APIs into a single interface, it significantly reduces the technical barrier for accessing demographic and employment data.\nThe modular architecture and attention to user experience make it a valuable tool for researchers, planners, and analysts who regularly work with Census data. The open-source approach ensures continued development and community contributions.\nFor transportation planners specifically, having easy access to demographic characteristics, employment patterns, and commuting flows in a single application streamlines the data gathering phase of project development, allowing more time for analysis and decision-making.\n\nWant to contribute or suggest improvements? Visit the project repository at: https://github.com/ar-puuk/uscensus-dashboard"
  },
  {
    "objectID": "research/articles/salt-lake-transit-equity-2023/index.html",
    "href": "research/articles/salt-lake-transit-equity-2023/index.html",
    "title": "Evaluating Social Equity of Transit Accessibility: A Case of Salt Lake County, U.S.",
    "section": "",
    "text": "Addressing social equity in public transportation remains a key challenge for many cities and planning organizations. In this study, we examined social equity dimensions of accessibility to light rail transit (LRT) stations in Salt Lake County, U.S., by employing two novel methods. First, we used the two-step floating catchment area (2SFCA) method to examine the interactions between the demand and supply of the public transit service. Second, we developed geospatial models to account for spatial bias in transit equity analysis. Results showed little evidence of inequitable access to LRT stations in Salt Lake County. The accessibility to LRT stations appeared to be generally higher in the downtown and transit catchment areas with a higher concentration of low-income and ethnic minority populations. Furthermore, we found statistically significant associations between higher transit accessibility and households which are not homeowners, and/or do not own a private motor vehicle. Our findings suggest that transit investments in Salt Lake County could leverage substantial transportation accessibility opportunities to achieve an equitable and sustainable future."
  },
  {
    "objectID": "research/articles/salt-lake-transit-equity-2023/index.html#abstract",
    "href": "research/articles/salt-lake-transit-equity-2023/index.html#abstract",
    "title": "Evaluating Social Equity of Transit Accessibility: A Case of Salt Lake County, U.S.",
    "section": "",
    "text": "Addressing social equity in public transportation remains a key challenge for many cities and planning organizations. In this study, we examined social equity dimensions of accessibility to light rail transit (LRT) stations in Salt Lake County, U.S., by employing two novel methods. First, we used the two-step floating catchment area (2SFCA) method to examine the interactions between the demand and supply of the public transit service. Second, we developed geospatial models to account for spatial bias in transit equity analysis. Results showed little evidence of inequitable access to LRT stations in Salt Lake County. The accessibility to LRT stations appeared to be generally higher in the downtown and transit catchment areas with a higher concentration of low-income and ethnic minority populations. Furthermore, we found statistically significant associations between higher transit accessibility and households which are not homeowners, and/or do not own a private motor vehicle. Our findings suggest that transit investments in Salt Lake County could leverage substantial transportation accessibility opportunities to achieve an equitable and sustainable future."
  },
  {
    "objectID": "research/articles/salt-lake-transit-equity-2023/index.html#key-findings",
    "href": "research/articles/salt-lake-transit-equity-2023/index.html#key-findings",
    "title": "Evaluating Social Equity of Transit Accessibility: A Case of Salt Lake County, U.S.",
    "section": "Key Findings",
    "text": "Key Findings\n\nEquitable Transit Access: Found little evidence of inequitable access to light rail stations in Salt Lake County, with accessibility generally higher in areas with higher concentrations of low-income and ethnic minority populations\nVehicle Ownership Correlation: Discovered statistically significant positive associations between higher transit accessibility and households without private motor vehicles, suggesting the system serves those most dependent on public transit\nSpatial Clustering: Identified spatial clustering of accessibility around downtown Salt Lake City, University of Utah area, and southbound towards Murray and Sandy areas\nMethodological Innovation: Successfully applied the 2SFCA method and spatial regression models to transportation equity analysis, providing a more comprehensive approach than traditional methods"
  },
  {
    "objectID": "research/articles/salt-lake-transit-equity-2023/index.html#research-context",
    "href": "research/articles/salt-lake-transit-equity-2023/index.html#research-context",
    "title": "Evaluating Social Equity of Transit Accessibility: A Case of Salt Lake County, U.S.",
    "section": "Research Context",
    "text": "Research Context\nThis study addresses a critical gap in transit equity research by examining the UTA TRAX light rail system in Salt Lake County. With 12% of renter households and 2% of owner-occupied households having no car access, understanding the equitable distribution of transit infrastructure is crucial for transportation planning in the region."
  },
  {
    "objectID": "research/articles/salt-lake-transit-equity-2023/index.html#methods-and-data",
    "href": "research/articles/salt-lake-transit-equity-2023/index.html#methods-and-data",
    "title": "Evaluating Social Equity of Transit Accessibility: A Case of Salt Lake County, U.S.",
    "section": "Methods and Data",
    "text": "Methods and Data\nAnalytical Approach:\n\nTwo-Step Floating Catchment Area (2SFCA): Novel application to transportation planning to capture both supply and demand sides of accessibility\nSpatial Regression Models: Used Spatial Lag and Spatial Error models to account for spatial autocorrelation, addressing limitations of traditional OLS regression\n15-minute Walking Catchments: Defined service areas using network-based buffers around TRAX stations\n\nData Sources:\n\n2019 5-year American Community Survey (demographic data)\nUTA General Transit Feed Specification (GTFS) for service frequency\nUtah Geospatial Research Center for spatial transit data\nOpenRouteServices API for isochron buffer creation\n\nSocial Equity Indicators:\nEight comprehensive indicators including household income, race, ethnicity, age, employment, education, vehicle ownership, and homeownership status.\nTools Used:\n\nR (v4.2.1) with packages including tidyverse, tidycensus, and spatial analysis libraries\nQGIS (3.22.2) for geospatial analysis"
  },
  {
    "objectID": "research/articles/salt-lake-transit-equity-2023/index.html#policy-implications",
    "href": "research/articles/salt-lake-transit-equity-2023/index.html#policy-implications",
    "title": "Evaluating Social Equity of Transit Accessibility: A Case of Salt Lake County, U.S.",
    "section": "Policy Implications",
    "text": "Policy Implications\nThe findings suggest that Salt Lake Countyâ€™s transit investments have achieved a relatively equitable distribution compared to similar studies in Chicago, Brisbane, Perth, and Auckland. This may be attributed to the long history of coordination among government agencies and the influence of historical streetcar networks on development patterns.\nThe research provides evidence that can inform future transit planning decisions and demonstrates the importance of using robust spatial analytical methods for equity assessments."
  },
  {
    "objectID": "research/articles/salt-lake-transit-equity-2023/index.html#citation",
    "href": "research/articles/salt-lake-transit-equity-2023/index.html#citation",
    "title": "Evaluating Social Equity of Transit Accessibility: A Case of Salt Lake County, U.S.",
    "section": "Citation",
    "text": "Citation\n@article{Zinia2023TransitEquity,\n    title = {Evaluating Social Equity of Transit Accessibility: A Case of Salt Lake County, U.S.},\n    author = {Faria Afrin Zinia and Pukar Bhandari and Justice Prosper Tuffour and Andy Hong},\n    journal = {Transportation Research Record},\n    volume = {2677},\n    number = {12},\n    pages = {806--814},\n    year = {2023},\n    doi = {10.1177/03611981231170005},\n    url = {https://doi.org/10.1177/03611981231170005},\n    abstract = {Addressing social equity in public transportation remains a key challenge for many cities and planning organizations. In this study, we examined social equity dimensions of accessibility to light rail transit (LRT) stations in Salt Lake County, U.S., by employing two novel methods. First, we used the two-step floating catchment area (2SFCA) method to examine the interactions between the demand and supply of the public transit service. Second, we developed geospatial models to account for spatial bias in transit equity analysis. Results showed little evidence of inequitable access to LRT stations in Salt Lake County. The accessibility to LRT stations appeared to be generally higher in the downtown and transit catchment areas with a higher concentration of low-income and ethnic minority populations. Furthermore, we found statistically significant associations between higher transit accessibility and households which are not homeowners, and/or do not own a private motor vehicle. Our findings suggest that transit investments in Salt Lake County could leverage substantial transportation accessibility opportunities to achieve an equitable and sustainable future.}\n}"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download Latest Resume"
  }
]